---
title: "Chapter4 Nonparametric Estimation of Time-Varying Models"
date: 2021-08-19T21:17:53+08:00
draft: true

description: ""
upd: ""

tags: ['Notes']
categories: ['Nonparametric Analysis in Time Series']
---

<!--more-->

In this section, we consider smoothed nonparametric estimation of time-varying models, where model parameters are deterministic functions of time. For simplicity, we focus on estimating a known time trend function in a time series process.

## 4.1 Slow-Varying Time Trend Estimation

Suppose we observe a bivariate time series random sample $ \left\{Y_{t}, X_{t}\right\}_{t=1}^{T} $ of size $ T, $ where

$$
Y_{t}=m(t / T)+X_{t}, \quad t=1, \ldots, T
$$

where $ m(\cdot) $ is a smooth but unknown time-trend function and $ \left\{X_{t}\right\} $ is a strictly stationary process with $ E\left(X_{t}\right)=0 $ and auto-covariance function $ \gamma(j)=\operatorname{cov}\left(X_{t}, X_{t-j}\right) . $ Thus the mean of $ Y_{t} $ is changing over time, $ \left\{Y_{t}\right\} $ is non-stationary.

**Question**: Why is the trend function $ m(\cdot) $ assumed to be a function of normalized time $ t / T $ rather than time $ t $ only?

This is a crucial device for consistent estimation of the trend function $ m(\cdot) $ as the sample size $ T \rightarrow \infty $. *Suppose $ m(\cdot) $ is a function of time $ t $ only, then when sample size $ T $ increases, new information about future times becomes available, but the information about the function $ m(\cdot) $ around a given time point, say, $ t_{0}, $ does not increase. Therefore, it is impossible to obtain a consistent estimation of $ m\left(t_{0}\right) $. In contrast, with $ m\left(t_{0} / T\right) $ as a function of $ t_{0} / T, $ more and more information about $ m(\cdot) $ in a neighborhood of $ t_{0} / T $ ($ t \in [t_0-Th,t_0+Th ] $) will become available when $ T $ increases, which ensures consistent estimation of $ m\left(t_{0} / T\right) $.*

**Question**: How to estimate the time trend function $ m(t / T) ? $

We can separate the smooth trend from the stochastic component with smoothed nonparametric estimation.

Suppose $ m(\cdot) $ is continuously differentiable on [0,1] up to order $ p, $ and we are interested in estimating the function $ m\left(t_{0} / T\right) $ at a time point $ t_{0} $ such that $ t_{0} / T \rightarrow \tau_{0} \in[0,1] $, where $ \tau_{0} $ is a fixed point. Then by a Taylor series expansion, we have, for all $ t $ such that $ t / T $ lies in a neighborhood of $ \tau_{0}=t_{0} / T $, i.e. $ |t/T-\tau_0| \leq h$, 

$$
m(t / T)=\sum_{j=0}^{p} \frac{1}{\nu !} m^{(\nu)}\left(\tau_{0}\right)\left(\frac{t-t_{0}}{T}\right)^{j}+\frac{1}{(p+1) !} m^{(p+1)}\left(\bar{\tau}_{t}\right)\left(\frac{t-t_{0}}{T}\right)^{p+1}
$$

where $ \bar{\tau}_{t} $ lies in the segment between $ \tau_{0} $ and $ t / T . $ Thus, we consider local polynomial smoothing by solving the local weighted sum of squared residuals minimization problem

$$
\min _{\alpha} \sum_{t=1}^{T}\left[Y_{t}-\sum_{j=0}^{p} \alpha_{j}\left(\frac{t-t_{0}}{T}\right)^{j}\right]^{2} K_{h}\left(\frac{t-t_{0}}{T}\right)=\sum_{t=1}^{T}\left(Y_{t}-\alpha^{\prime} Z_{t}\right)^{2} W_{t}
$$

where $ \alpha=\left(\alpha_{0}, \alpha_{1}, \ldots, \alpha_{p}\right)^{\prime}, $

$$
Z_{t}=\left[1,\left(\frac{t-t_{0}}{T}\right), \ldots,\left(\frac{t-t_{0}}{T}\right)^{p}\right]^{\prime}
$$

and

$$
W_{t} \equiv K_{h}\left(\frac{t-t_{0}}{T}\right)=\frac{1}{h} K\left(\frac{t-t_{0}}{T h}\right)
$$

Then the solution for $ \alpha $ is

$$
\begin{aligned}
\hat{\alpha} &=\left(\sum_{t=1}^{T} Z_{t} W_{t} Z_{t}^{\prime}\right)^{-1} \sum_{t=1}^{T} Z_{t} W_{t} Y_{t} \\
&=\left(Z^{\prime} W Z\right)^{-1} Z^{\prime} W Y
\end{aligned}
$$

In particular, we have

$$
\hat{\alpha}_{\nu}=e_{\nu+1}^{\prime} \hat{\alpha}, \quad 0 \leq \nu \leq p
$$

where $ e_{\nu+1} $ is a $ (p+1) \times 1 $ unit vector with the $ \nu+1 $ element being unity and all others being zero.

Question: What are the asymptotic properties of $ \hat{\alpha}_{\nu} $ for $ 0 \leq \nu \leq p ? $

We first derive the asymptotic MSE of $ \hat{\alpha}_{\nu} . $ Put

$$
\hat{S}=Z^{\prime} W Z^{\prime}
$$

a nonstochastic $ (p+1) \times(p+1) $ matrix, whose $ (i, j) $ -element is

$$
\hat{S}_{(i, j)}=\sum_{t=1}^{T}\left(\frac{t-t_{0}}{T}\right)^{(i-1)+(j-1)} K_{h}\left(t-t_{0}\right)
$$

We first decompose

$$
\begin{aligned}
\hat{\alpha}_{\nu}-\alpha_{\nu}=& e_{\nu+1}^{\prime} \hat{S}^{-1} \sum_{t=1}^{T} Z_{t} W_{t} X_{t} \\
&+e_{\nu+1}^{\prime} \hat{S}^{-1} \sum_{t=1}^{T} Z_{t} W_{t} m\left(\frac{t}{T}\right)-\alpha_{\nu} \\
=& \hat{V}+\hat{B}, \text { say. }
\end{aligned}
$$

For the first term, we have $ E(\hat{V})=0 $ given $ E\left(X_{t}\right)=0, $ and

$$
\begin{aligned}
\operatorname{var}(\hat{V}) &=E\left(e_{\nu+1}^{\prime} \hat{S}^{-1} Z^{\prime} W X X^{\prime} W Z \hat{S}^{-1} e_{\nu+1}\right) \\
&=e_{\nu+1}^{\prime} \hat{S}^{-1} Z^{\prime} W E\left(X^{\prime} X\right) W Z \hat{S}^{-1} e_{\nu+1} \\
&=e_{\nu+1}^{\prime} \hat{S}^{-1}\left[\sum_{t=1}^{T} \sum_{s=1}^{T} Z_{t} W_{t} \gamma(t-s) Z_{0}^{\prime} W_{0}\right] \hat{S}^{-1} e_{\nu+1} \\
&=e_{\nu+1}^{\prime} \hat{S}^{-1}\left[\sum_{j=1-T}^{T-1}\left(1-\frac{|j|}{T}\right) \gamma(j) \sum_{t=1}^{T} Z_{t} W_{t} Z_{t-j}^{\prime} W_{t-j}\right] \hat{S}^{-1} e_{v+1}
\end{aligned}
$$

By approximating the discrete sum with a continuous integral, we have

$$
\frac{1}{T h} \sum_{t=1}^{T}\left(\frac{t-t_{0}}{T h}\right)^{j} K\left(\frac{t-t_{0}}{T h}\right) \rightarrow \int_{-1}^{1} u^{j} K(u) d u \text { for } 0 \leq j \leq 2 p-1
$$

as $ h \rightarrow 0, T h \rightarrow \infty . $ It follows that

$$
\frac{1}{T} H^{-1} \hat{S} H^{-1}=S[1+o(1)]
$$

where, as before, $ S $ is a $ (p+1) \times(p+1) $ matrix with its $ (i, j) $ -th element being $ \int_{-1}^{1} u^{(i-1)+(j-1)} K(u) d u . $ Also, for each given $ j, $ by approximating the discrete sum with
a continuous integral, we have

$$
\frac{1}{T h} \sum_{t=1}^{T}\left(\frac{t-t_{0}}{T h}\right)^{m}\left(\frac{t-t_{0}-j}{T h}\right)^{l} K\left(\frac{t-t_{0}}{T h}\right) K\left(\frac{t-t_{0}-j}{T h}\right) \rightarrow \int_{-1}^{1} u^{m+l} K^{2}(u) d u
$$

as $ h \rightarrow 0, T h \rightarrow \infty . $ Therefore, for any given lah order $ j, $ we obtain

$$
\frac{1}{T} H^{-1}\left(\sum_{t=1}^{T} Z_{t} W_{t} Z_{t-j}^{\prime} W_{t-j}\right) H^{-1}=h^{-1} S^{*}[1+o(1)]
$$

where $ S^{*} $ is a $ (p+1) \times(p+1) $ matrix with its $ (i, j) $ element being $ \int_{-1}^{1} u^{(i-1)+(j-1)} K^{2}(u) d u $
It follows that

$$
\begin{aligned}
\operatorname{var}(\hat{V}) &=\frac{1}{T h} H^{-1} S^{-1} S^{*} S^{-1} H^{-1}\left[\sum_{j=-\infty}^{\infty} \gamma(j)\right][1+o(1)] \\
&=\frac{1}{T h^{2 \nu+1}} e_{\nu+1}^{\prime} S^{-1} S^{*} S^{-1} e_{\nu+1}\left[\sum_{j=-\infty}^{\infty} \gamma(j)\right][1+o(1)]
\end{aligned}
$$
Unlike the estimator for the regression function $ r\left(X_{t}\right), $ the asymptotic variance of the local polynomial estimator $ \hat{m}\left(t_{0} / T\right)=\hat{\alpha}_{0} $ depends on the long-run variance of $ \left\{X_{t}\right\} . $ In other words, the serial dependence in $ \left\{X_{t}\right\} $ has impact on the asymptotic variance of the local polynomial estimator $ \hat{\alpha}_{0} . $ More specifically, whether $ \left\{X_{t}\right\} $ is $ \mathrm{IID} $ or serially dependent has an important impact on the asymptotic variance of $ \hat{\alpha}_{\nu}=\frac{1}{\nu !} \hat{m}^{(\nu)}\left(t_{0} / T\right) $.
Question: Why?

The local polynomial estimator for $ m\left(t_{0} / T\right) $ is based on the observations in the local interval $ \left[\frac{t_{0}}{T}-h, \frac{t_{0}}{T}+h\right] . $ These observations are conservative observations over time in an interval $ \left[t_{0}-T h, t_{0}+T h\right], $ whose width, equal to $ 2 T h, $ is increasing but at a slower
rate than sample size $ T $. Thus, it maintains the same pattern of serial dependence as
the original time series $ \left\{X_{t}\right\} . $ As a result, the asymptotic variance of $ \hat{\alpha}_{0} $ will depend on the long-run variance of $ \left\{X_{t}\right\} . $ In contrast, the local polynomial estimator $ \hat{r}(x) $ for
the regression function $ r(x)=E\left(Y_{t} \mid X_{t}=x\right) $ is based on the observations in a small interval $ [x-h, x+h] . $ The observations that fall into this small interval are generally
not conservative over time, so their serial dependence structure has been destroyed, and
they appear like an IID sequence.

Next, to obtain the bias of $ \hat{m}\left(t_{0} / T\right), $ where $ t_{0} \in[T h, T-T h], $ using the Taylor series
expansion,
$$
m(t / T)=\sum_{j=0}^{p} \frac{1}{j !} m^{(j)}\left(\frac{t_{0}}{T}\right)\left(\frac{t-t_{0}}{T}\right)^{j}+\frac{1}{(p+1) !} m^{(p+1)}\left(\frac{\bar{t}}{T}\right)\left(\frac{t-t_{0}}{T}\right)^{p+1}
$$
where $ \bar{t}=\lambda t+(1-\lambda) t_{0}, $ we have
$$
\begin{aligned}
\hat{B} &=e_{\nu+1}^{\prime} \hat{S}^{-1} \sum_{t=1}^{T} Z_{t} W_{t} m\left(\frac{t}{T}\right)-\alpha_{\nu} \\
&=\frac{1}{(p+1) !} e_{\nu+1}^{\prime} \hat{S}^{-1} \sum_{t=1}^{T} Z_{t} W_{h}\left(\frac{t-t_{0}}{T}\right)^{p+1} m^{(p+1)}\left(\frac{\bar{t}}{T}\right) \\
&=\frac{h^{p+1}}{(p+1) !} e_{\nu+1}^{\prime} H^{-1}\left(H^{-1} \hat{S} H^{-1}\right)^{-1} H^{-1} \sum_{t=1}^{T} Z_{t} W_{h}\left(\frac{t-t_{0}}{T h}\right)^{p+1} m^{(p+1)}\left(\frac{\bar{t}}{T}\right) \\
&=\frac{h^{p+1-\nu} m^{(p+1)}\left(\frac{t_{0}}{T}\right)}{(p+1) !} e_{\nu+1}^{\prime} S^{-1} C[1+o(1)]
\end{aligned}
$$
where $ C $ is a $ (p+1) \times 1 $ vector with the $ i $ -th element being $ \int_{-1}^{1} u^{(p+1)-(i-1)} K(u) d u . $ Here,
we have used a continuous integral to approximate a discrete sum:
$$
\frac{1}{T h} \sum_{t=1}^{T}\left(\frac{t-t_{0}}{T h}\right)^{j} K\left(\frac{t-t_{0}}{T h}\right) \rightarrow \int_{-1}^{1} u^{j} K(u) d u
$$
as $ T \rightarrow \infty . $ It follows that the asymptotic MSE of $ \hat{\alpha}_{\nu} $ is
$$
\begin{aligned}
\operatorname{MSE}\left(\hat{\alpha}_{\nu}\right)=& \frac{1}{T h^{2 \nu+1}} e_{\nu+1}^{\prime} S^{-1} S^{*} S^{-1} e_{\nu+1} \sum_{j=-\infty}^{\infty} \gamma(j) \\
&+h^{2(p+1-\nu)}\left[\frac{m^{(p+1)}\left(\frac{t_{0}}{T}\right)}{(p+1) !}\right]^{2}\left(e_{\nu+1}^{\prime} S^{-1} C\right)^{2} \\
&+o\left(T^{-1} h^{-2 \nu-1}+h^{2(p+1-\nu)}\right)
\end{aligned}
$$
Next, we derive the asymptotic distribution of $ \hat{m}\left(t_{0} / T\right) $.
Next, we use the central limit theorem to derive the asymptotic distribution of
$ \hat{m}\left(t_{0} / T\right) $
Theorem [Asymptotic Normality of $ \left.\hat{m}\left(t_{0} / T\right)\right]: $ If $ h=O\left(T^{-1 /(2 p+3)}\right) $ and $ m^{(p+1)}\left(t_{0} / T\right) $
is continuous, where $ t_{0} \in[T h, T-T h] $, then as $ T \rightarrow \infty $
$$
\begin{array}{l}
\sqrt{T h}\left[H\left[\hat{\alpha}\left(t_{0} / T\right)-\alpha\left(t_{0} / T\right)\right]-\frac{h^{p+1} m^{(p+1)}\left(t_{0} / T\right)}{(p+1) !} S^{-1} C\right] \\
\rightarrow{d}^{\prime} N\left(0, S^{-1} S^{*} S^{-1} \sum_{j=-\infty}^{\infty} \gamma(j)\right)
\end{array}
$$

where
$$
\alpha(0)=\left[r(0), r^{(1)}(0), \ldots, r^{(p)}(0) / p !\right]^{\prime}
$$
Therefore,
$$
\begin{array}{l}
\qquad \sqrt{T h^{2 \nu+1}}\left[\hat{m}^{(\nu)}\left(t_{0} / T\right)-m^{(\nu)}\left(t_{0} / T\right)-\frac{h^{p+1-\nu} m^{(p+1)}\left(t_{0} / T\right)}{(p+1) !} \int_{-1}^{1} u^{p+1} K_{\nu}^{*}(u) d u\right] \\
\rightarrow^{d} N\left(0,(\nu !)^{2} \int_{-1}^{1} \tilde{K}^{2}(u) d u \sum_{j=-\infty}^{\infty} \gamma(j)\right)
\end{array}
$$
Similar results for the asymptotic MSE and asymptotic normality of the local poly-
nomial trend estimator for $ t_{0} $ in the boundary region $ [1, T h) $ or $ (T-T h, T] $ could also
be obtained. We omit them for space.
$ 4.2 \quad $ Locally Linear Time-Varying Regression Estimation
Question: How to model smooth time changes in economics?
We consider a locally linear time-varying regression model
$$
Y_{t}=X_{t}^{\prime} \alpha\left(\frac{t}{T}\right)+\varepsilon_{t}, \quad t=1, \ldots, T
$$
where $ Y_{t} $ is a scalar, $ X_{t} $ is a $ d \times 1 $ random vector, and $ \alpha(t / T) $ is a $ d \times 1 $ smooth function
of normalized time $ t / T $. The time-trend time series model can be viewed as a special
case of this locally linear time-varying regression model with $ X_{t}=1 $.
Question: Why to consider smooth changes in a regression model?
Structural changes are rather a rule than an exception, due to advances in technology,
changes in preferences, policy shifts, and institutional changes in the economic system. It takes time for economic agents to react to sudden shocks, because it takes time
for economic agents to collect information needed for making decisions, it takes time for
markets to reach some consensus due to heterogenous beliefs, it takes time for economic
agents to change their habits, etc. Even if individual agents can respond immediately
to sudden changes, the aggregated economic variables (such as consumption) over many
individuals will become smooth. Indeed, as Alfred Marshall points out, economic changes
are evolutionary.

The locally linear time-varying regression model is potentially useful for macroeco-nomic applications and for long time series data.

Question: How to estimate the $ d \times 1 $ time-varying parameter vector $ \alpha(t / T) ? $
Suppose $ \alpha(t / T) $ is continuously differentiable up to order $ p+1, $ and $ t_{0} $ is a specified time point such that $ t_{0} / T $ converges $ \tau_{0} $ in $ [0,1] . $ Then by a Taylor series expansion, we
have that for all $ t $ such that $ t / T $ is in a small neighborhoo of $ t_{0} / T $,
$$
\begin{aligned}
\alpha\left(\frac{t}{T}\right)=& \sum_{j=0}^{p} \frac{1}{j !} \alpha^{(j)}\left(\frac{t_{0}}{T}\right)\left(\frac{t-t_{0}}{T}\right)^{j} \\
&+\frac{1}{(p+1) !} \alpha^{(p+1)}\left(\frac{\bar{t}}{T}\right)\left(\frac{t-t_{0}}{T}\right)^{p+1}
\end{aligned}
$$
where $ \bar{t} $ lies in the segment between $ t $ and $ t_{0} $. Precisely, $ \alpha^{(p+1)}(\bar{t} / T) $ should be understood as being evaluated at a different $ \bar{t} $ for a different component of the $ d \times 1 $ vector $ \alpha^{(p+1)}(\cdot) $.
Therefore, we can use a $ p $ -th order polynomial to approximate the unknown vector
function $ \alpha(t / T) $ in the neighbood of $ t_{0} / T . $ Put
$$
Z_{t}=\left[1, \frac{t-t_{0}}{T}, \ldots,\left(\frac{t-t_{0}}{T}\right)^{p}\right]^{\prime}
$$
and
$$
Q_{t}=Z_{t} \otimes X_{t}
$$
is a $ d(p+1) \times 1 $ vector of augmented regressors, where $ \otimes $ is the Konecker product.
Then we consider the following locally weighted sum of squared residuals minimization
problem
$$
\begin{aligned}
& \sum_{t=1}^{T}\left(Y_{t}-\sum_{l=1}^{d} \alpha_{l t}^{\prime} X_{l t}\right)^{2} K_{h}\left(t-t_{0}\right) \\
=& \sum_{t=1}^{T}\left(Y_{t}-\sum_{l=1}^{d} \alpha_{l}^{\prime} Z_{t} X_{l t}\right)^{2} K_{h}\left(t-t_{0}\right) \\
=& \sum_{t=1}^{T}\left[Y_{t}-\alpha^{\prime}\left(Z_{t} \otimes X_{t}\right)\right]^{2} K_{h}\left(t-t_{0}\right) \\
=& \sum_{t=1}^{T}\left(Y_{t}-\alpha^{\prime} Q_{t}\right)^{2} K_{h}\left(t-t_{0}\right)
\end{aligned}
$$
where $ \alpha=\left(\alpha_{1}^{\prime}, \ldots, \alpha_{d}^{\prime}\right)^{\prime} $ is a $ d(p+1) \times 1 $ vector, with $ \alpha_{l} $ being a $ (p+1) \times 1 $ coefficient
vector for the product regressor vector $ Z_{t} X_{l t} $

The local polynomial estimator
$$
\begin{aligned}
\hat{\alpha} &=\left(\sum_{t=1}^{T} Q_{t} W_{t} Q_{t}^{\prime}\right)^{-1} \sum_{t=1}^{T} Q_{t} W_{t} Y_{t} \\
&=\left[\sum_{t=1}^{T}\left(Z_{t} \otimes X_{t}\right) W_{t}\left(Z_{t} \otimes X_{t}\right)^{\prime}\right]^{-1} \sum_{t=1}^{T}\left(Z_{t} \otimes X_{t}\right) W_{t} Y_{t}
\end{aligned}
$$
The estimator for the $ d \times 1 $ vector $ \alpha\left(t_{0} / T\right) $ is then given by
$$
\hat{\alpha}\left(t_{0} / T\right)=\left(I_{d} \otimes e_{1}\right)^{\prime} \hat{\alpha}
$$
where $ I_{d} $ is a $ d \times d $ identity matrix and $ e_{1} $ is a $ (p+1) \times 1 $ vector with unity for the first
component equal to 1 and all the other components equal to zero. Intuitively, $ \hat{\alpha}\left(t_{0} / T\right) $
is a $ d \times 1 $ vector consisting of the $ d $ estimated intercepts from $ \left\{\hat{\alpha}_{l}\right\}_{l=1}^{d}, $ where $ \hat{\alpha}_{l} $ is a
$ (p+1) \times 1 $ estimated coefficient vector for $ Z_{t} X_{l t} $

We could derive the asymptotic MSE formula for $ \hat{\alpha}\left(t_{0} / T\right), $ and their asymptotic
normal distributions.

Theorem [Asymptotic MSE]: Suppose $ \left\{X_{t}^{\prime}, \varepsilon_{t}\right\}^{\prime} $ is a strictly stationary $ \alpha $ -mixing process with $ \gamma(j)=\operatorname{cov}\left(\varepsilon_{t}, \varepsilon_{t-j}\right) $ and $ Q=E\left(X_{t} X_{t}^{\prime}\right), $ and $ \left\{X_{t}\right\} $ and $ \left\{\varepsilon_{t}\right\} $ are mutually
independent. Then for any given $ t_{0} / T $ in the interior region of $ [0,1], $ the asymptotic MSE
of each component of the $ d \times 1 $ vector $ \hat{\alpha}\left(t_{0} / T\right)=\left(I_{d} \otimes e_{1}\right)^{\prime} \hat{\alpha}, $ where $ \hat{\alpha}=\left(\hat{\alpha}_{1}^{\prime}, \ldots, \hat{\alpha}_{d}^{\prime}\right)^{\prime} $ is
the local weighted least squares estimator, is given by
$$
\begin{aligned}
\operatorname{MSE}\left[\hat{\alpha}_{l}\left(t_{0} / T\right), \alpha_{l}\left(t_{0} / T\right)\right]=& \frac{1}{T h}\left[\sum_{j=-\infty}^{\infty} \gamma(j)\right]\left(I_{d} \otimes e_{1}\right)^{\prime}(Q \otimes S)^{-1}\left(Q \otimes S^{*}\right)(Q \otimes S)^{-1}\left(I_{d} \otimes e_{1}\right) \\
&+\left[\frac{h^{p+1} \alpha_{l}^{(p+1)}\left(t_{0} / T\right)}{(p+1) !}\right]^{2}\left(I_{d} \otimes e_{1}\right)^{\prime}(Q \otimes S)^{-1}(Q \otimes C)(Q \otimes C)^{\prime}(Q \otimes S)^{-1} \\
&+o\left(T^{-1} h^{-1}+h^{2(p+1)}\right) \\
=& \frac{1}{T h}\left[\sum_{j=-\infty}^{\infty} \gamma(j)\right] \int_{-1}^{1} \tilde{K}_{0}^{2}(u) d u \\
&+h^{2(p+1)}\left[\frac{\alpha_{l}^{(p+1)}\left(t_{0} / T\right)}{(p+1) !}\right]^{2}\left[\int_{-1}^{1} u^{p+1} \tilde{K}_{0}(u) d u\right]^{2} \\
&+o\left(T^{-1} h^{-1}+h^{2(p+1)}\right)
\end{aligned}
$$

where the equivalent kernel is defined as
$$
\tilde{K}_{0}(u)=\left(I_{d} \otimes e_{1}\right)^{\prime}(Q \otimes S)^{-1}[Q \otimes P(u)] K(u)
$$
By minimizing the asymptotic MSE, the optimal convergence rate of each component
of $ \hat{\alpha}\left(t_{0} / T\right) $ can be obtained by choosing the bandwidth
$$
h^{*} \propto T^{-\frac{1}{2 p+3}}
$$
Theorem [Asymptotic Normality of $ \left.\hat{\alpha}\left(t_{0} / T\right)\right]: $ If $ h=O\left(T^{-1 /(2 p+3)}\right) $ and $ \alpha^{(p+1)}\left(t_{0} / T\right) $ is continuous, where $ t_{0} \in[T h, T-T h] $, then for $ l=1, \ldots, d, $ as $ T \rightarrow \infty $
$$
\begin{array}{c}
\sqrt{T h}\left[\hat{\alpha}_{l}\left(t_{0} / T\right)-\alpha_{l}\left(t_{0} / T\right)\right]-\frac{h^{p+1} \alpha_{l}^{(p+1)}\left(t_{0} / T\right)}{(p+1) !}(Q \otimes S)^{-1}(Q \otimes C) \\
\rightarrow d N\left(0,(Q \otimes S)^{-1}\left(Q \otimes S^{*}\right)(Q \otimes S)^{-1} \sum_{j=-\infty}^{\infty} \gamma(j)\right)
\end{array}
$$
Similar results of the asymptotic MSE and asymptotic normality for $ \hat{\alpha}\left(t_{0} / T\right) $ can be
obtained when the normalized time time $ \frac{t_{0}}{T} $ is in the left boundary region $ [0, h] $ or the right boundary region $ [1-h, 1] $.

By plotting the estimator $ \hat{\alpha}\left(t_{0} / T\right) $ as a function of $ t_{0} / T $ in the interval $ [0,1], $ we can
examine whether the coefficient vector $ \alpha $ is time-varying. Formally, Chen and Hong
(2012) propose some consistent tests for smooth structural changes as well as a finite number of multiple breaks in a linear regression model by using a local linear estimator
for the time-varying coefficient $ \alpha(\cdot) . $ Specifically, they propose a generalized Chow (1960)
test, which is a generalized $ F $ -test by comparing the sum of squared residuals of a local
linear regression model with that of a constant parameter regression model. They also
propose a generalized Hausman (1978) type test by comparing the fitted values of a
local linear estimator with those of a constant parameter regression model. Chen and
Hong (2012) derive the asymptotic null distribution of these test statistics under the
null hypothesis of no structural change and establish consistency of these tests under the alternative hypothesis. See Chen and Hong ( 2012 ) for more discussion, including a a
simulation study and an empirical application.

Chen and Hong (2016) also propose a local smoothing quasi-likelihood based test for parameter constancy of a GARCH model.

