---
title: "Chapter2 Linear Time Series Models"
date: 2021-06-01T17:38:51+08:00
draft: false

description: "Autoregressive (AR) models, Moving average (MA) models and ARMA(p,q) model."
upd: "Autoregressive (AR) models, Moving average (MA) models and ARMA(p,q) model."

tags: ['Notes']
categories: ['Financial Econometrics']
---

<!--more-->

## 2.1 Introduction

A **time series** is a sequence of data points, measured typically at successive points in time spaced at uniform time intervals.

Examples: tick data of stock prices, daily exchange rates, monthly interest rates, annual GDP growth rate etc.;

Time series analysis provides econometrics tools for those time series data;

Famous time series models/tools: ARIMA model, random walk, cointegration, error correction model, vector autoregressive model and ARCH/GARCH etc..

The Importance of Time Series to Economics

- Time and uncertainty are two most important factors when economic agents make a decision;
- Time series econometrics provides statistical methods and tools to investigate dynamic relations in economics/finance;
- Time series data are attractive to researchers as they show many interesting phenomena: asymmetry(牛短熊长), time irreversibility, regime-shifts, volatility clustering, and jumps or outliers;
- In past two decades, a fast development of Time series from linear to nonlinear, stationary to nonstationary, univariate to multivariate.

The objective of time series econometrics

- Examine how well economic and financial theory/models can explain the stylized facts of economic/financial phenomena (e.g., volatility clustering, seasonal effects)

- Test the validity of economic and financial hypotheses (e.g., market Efficient Hypothesis)

- Predict the future evolution of economic systems and financial markets using historical data (e.g., the prediction of business cycle turning points).

- Make policy recommendations (e.g., program evaluations)

General **Methodology** of Time Series Analysis

- Survey/data collection and summary of stylized facts;
- Model specification: propose an economic/financial model using economic theory, empirical experience, and etc;
- Model estimation: estimate the proposed model using historical data.
- Model evaluation (in-sample and out-of-sample evaluation);
- Explain the stylized facts, test economic hypotheses/theories, and forecast futures.

## 2.2 Linear time series models

### 2.2.0 Basic

【strictly stationary】A time series $ \left\{X_{t}\right\} $ is said to be strictly stationary (or strongly stationary) if the $ k $ -dimensional distribution of $ \left(X_{1}, \ldots, X_{k}\right) $ is the same as that of $ \left(X_{t+1}, X_{t+2}, \ldots, X_{t+k}\right) $ for any integer $ k \geq 1 $ and $ t $.

【weakly stationary】A time series $ \left\{X_{t}\right\} $ is said to be weakly stationary (or second order stationary or covariance stationary) if $ E\left[X_{t}^{2}\right]<\infty $ and both $ E X_{t} $ and $ \operatorname{cov}\left(X_{t}, X_{t-k}\right), $ for any integer $ k, $ do not depend on $ t $.

#### ACVFs and ACFs

For weakly stationary time series $ \left\{X_{t}\right\}, $ let $ \mu=E X_{t} $ denote its common mean. We define the **autocovariance function** (ACVF) as

$$
\gamma(k)=\operatorname{cov}\left(X_{t}, X_{t+k}\right)=E\left\{\left(X_{t}-\mu\right)\left(X_{t+k}-\mu\right)\right\}
$$

and the **autocorrelation function** (ACF) as

$$
\rho(k)=\operatorname{Corr}\left(X_{t}, X_{t+k}\right)=\gamma(k) / \gamma(0)
$$

for $ k=0,\pm 1,\pm 2, \cdots . $ Note that $ \gamma(0) $ is the variance of $ X_{t}, $ i.e.
$ \gamma(0)=\operatorname{var}\left(X_{t}\right), $ and $ \rho(k)=\rho(-k) $

【white noise】$ \left\{X_{t}\right\} $ is called a white noise process when $ \rho(k)=0 $ for any $ k \neq 0, $ and is denoted by $ X_{t} \sim W N\left(\mu, \sigma^{2}\right), $ where $ \sigma^{2}=\gamma(0)=\operatorname{var}\left(X_{t}\right) $

如果时间序列$ x_t $是一个具有有限均值和有限方差的独立同分布随机变量序列，则$ \{x_t\} $称为一个白噪声序列(white noise).

特别地，若$ x_t $，还服从均值为0、方差为$ \sigma^2 $的正态分布，则称这个序列为高斯白噪声(Gaussian white noise)。

对于白噪声序列，所有自相关函数为零。在实际应用中，如果所有样本自相关函数接近于零，则认为该序列是白噪声序列。

In practice, we observe $ X_{1}, \ldots, X_{T} $ and estimate ACVF and ACF by the sample ACVF and sample ACF.

$$
\hat{\gamma}(k)=\frac{1}{T} \sum_{t=k+1}^{T}\left(X_{t}-\bar{X}\right)\left(X_{t-k}-\bar{X}\right), \quad \hat{\rho}(k)=\hat{\gamma}(k) / \widehat{\gamma}(0)
$$

where $ \bar{X}=T^{-1} \sum_{t=1}^{T} X_{t} $.

In time series analysis, we use **stationary autoregressive moving average** (ARMA) models to reveal the autocorrelation structures in the data.

在时间序列分析中，单变量是否显著不重要，重要的是残差项接近白噪声(不包含有效信息)。

### 2.2.1 Autoregressive (AR) models

An autoregressive model of order $ p(\mathrm{AR}(\mathrm{p})) $ is defined as

$$
X_{t}=c+\rho_{1} X_{t-1}+\cdots+\rho_{p} X_{t-p}+\epsilon_{t}
$$

where $ \epsilon_{t} \sim W N\left(0, \sigma^{2}\right), $ and $ c, \rho_{1}, \ldots, \rho_{p} $ are parameters. 

In particular, an **AR(1)** model is written as

$$
X_{t}=c+\rho_{1} X_{t-1}+\epsilon_{t}
$$

where $ \epsilon_{t} \sim W N\left(0, \sigma^{2}\right) $. Then $ \left\{X_{t}\right\} $ is stationary if and only if $ |\rho_1|<1 $.

Question: Compute the mean, variance, ACVFs and ACFs of $ \left\{X_{t}\right\} $.

#### AR(1) model

Taking expectations on both sides of the $ \mathrm{AR}(1) $ equation, we obtain

$$
\begin{aligned}
E\left[X_{t}\right] &=c+E\left[\rho X_{t-1}\right]+E\left[\epsilon_{t}\right] \\
&=c+\rho E\left[X_{t-1}\right]+0 \\
&=c+\rho E\left[X_{t}\right] \quad (\text{by stationary})
\end{aligned}
$$

since $ |\rho|<1, $ we have

$$
E\left[X_{t}\right]=\frac{c}{1-\rho}
$$

Note that $ E\left[X_{t}\right]=0 $ if and only if $ c=0 $.

We compute the variances of both sides of the AR(1) equation, and obtain

$$
\begin{aligned}
\operatorname{var}\left[X_{t}\right] &=\operatorname{var}\left[c+\rho X_{t-1}+\epsilon_{t}\right] \\
&=\operatorname{var}\left[\rho X_{t-1}+\epsilon_{t}\right] \\
&=\operatorname{var}\left[\rho X_{t-1}\right]+\operatorname{var}\left[\epsilon_{t}\right]+2 \operatorname{cov}\left(\rho X_{t-1}, \epsilon_{t}\right) \\
&=\rho^{2} \operatorname{var}\left[X_{t-1}\right]+\sigma^{2}+0 \\
&=\rho^{2} \operatorname{var}\left[X_{t}\right]+\sigma^{2}
\end{aligned}
$$

since $ |\rho|<1, $ we have

$$
\operatorname{var}\left[X_{t}\right]=\frac{\sigma^{2}}{1-\rho^{2}}
$$

Finally, we compute the autocovariance of $ X_{t} $ for any integer $ k . $ First, when $ k=1 $

$$
\begin{aligned}
\gamma(1) &=\operatorname{cov}\left(X_{t}, X_{t-1}\right)=\operatorname{cov}\left(c+\rho X_{t-1}+\epsilon_{t}, X_{t-1}\right) \\
&=\operatorname{cov}\left(c, X_{t-1}\right)+\operatorname{cov}\left(\rho X_{t-1}, X_{t-1}\right)+\operatorname{cov}\left(\epsilon_{t}, X_{t-1}\right) \\
&=0+\rho \operatorname{cov}\left(X_{t-1}, X_{t-1}\right)+0=\rho \operatorname{var}\left(X_{t-1}\right)=\rho \gamma(0)
\end{aligned}
$$

i.e., $ \gamma(1)=\rho \sigma^{2} /\left(1-\rho^{2}\right) $. Using the same method, we can show that $ \gamma(k)=\operatorname{cov}\left(X_{t}, X_{t-k}\right)=\rho^{k} \gamma(0) $ for all $ k>1 $ when $ X_{t} \sim \operatorname{AR}(1) $.

Therefore, $ \rho(1)=\gamma(1) / \gamma(0)=\rho $ and $ \rho(k)=\rho^{k} $ for $ k>1 $

To summarize, if $ X_{t}=c+\rho X_{t-1}+\epsilon_{t} $ is a stationary process, we have

$$
\begin{array}{c}
E\left[X_{t}\right]=\frac{c}{1-\rho} \\
\operatorname{var}\left[X_{t}\right]=\frac{\sigma^{2}}{1-\rho^{2}} \\
\gamma(k)=\left\{\begin{array}{ll}
\rho \sigma^{2} /\left(1-\rho^{2}\right) & \text { for } k=1 \\
\rho^{k} \sigma^{2} /\left(1-\rho^{2}\right) & \text { for } k>1
\end{array}\right. \\
\rho(k)=\left\{\begin{array}{cc}
\rho & \text { for } k=1 \\
\rho^{k} & \text { for } k>1
\end{array}\right.
\end{array}
$$

#### Autoregressive Process with Order p

$$
Y_{t}=c+\phi_{1} Y_{t-1}+\phi_{2} Y_{t-2}+\ldots+\phi_{p} Y_{t-p}+\varepsilon_{t}, \text { where } \varepsilon_{t} \sim W N\left(0, \sigma^{2}\right)
$$

Provided that the roots of $ 1-\phi_{1} z-\phi_{2} z^{2}-\ldots, \phi_{p} z^{p}=0 $ are outside of the unit circle, then $ Y_{t} $ is stationary.

$$
\mu=\frac{c}{1-\phi_{1}-\phi_{2}-\ldots-\phi_{p}}
$$

After some adjustment:

$$
Y_{t}-\mu=\phi_{1}\left(Y_{t-1}-\mu\right)+\phi_{2}\left(Y_{t-2}-\mu\right)+\ldots+\phi_{p}\left(Y_{t-p}-\mu\right)+\varepsilon_{t}
$$

Multiple $ Y_{t-j}-\mu $ on both sides and take expectation, we have

$$
\gamma_{j}=\left\{\begin{array}{r}
\phi_{1} \gamma_{j-1}+\phi_{2} \gamma_{j-2}+\ldots+\phi_{p} \gamma_{j-p}, \text { for all } j=1,2,3, \ldots \\
\phi_{1} \gamma_{1}+\phi_{2} \gamma_{2}+\ldots+\phi_{p} \gamma_{p}+\sigma^{2}, \text { for } j=0
\end{array}\right\}
$$

### 2.2.2 Moving average (MA) models

A moving average model of order $ q(\mathrm{MA}(\mathrm{q})) $ is defined as

$$
X_{t}=\mu+\epsilon_{t}+\theta_{1} \epsilon_{t-1}+\cdots+\theta_{q} \epsilon_{t-q}
$$

where $ \epsilon_{t} \sim W N\left(0, \sigma^{2}\right), $ and $ \mu, \theta_{1}, \ldots, \theta_{q} $ are parameters. Note that a MA(q) process is always stationary if the coefficients do not vary over time. 

In particular, a MA(1) models is written as

$$
X_{t}=\mu+\epsilon_{t}+\theta_{1} \epsilon_{t-1}
$$
where $ \epsilon_{t} \sim W N\left(0, \sigma^{2}\right) $

Question: Compute the mean, variance, ACVFs and ACFs of $ \left\{X_{t}\right\} $.

Suppose that $ X_{t}=\mu+\epsilon_{t}+\theta \epsilon_{t-1}, $ where $ \epsilon_{t} \sim W N\left(0, \sigma^{2}\right), $ we have

$$
\begin{aligned}
E\left[X_{t}\right]&=E\left[\mu+\epsilon_{t}+\theta \epsilon_{t-1}\right] \\
&=\mu+E\left[\epsilon_{t}\right]+E\left[\theta \epsilon_{t-1}\right] \\
&=\mu \\
\operatorname{var}\left[X_{t}\right]&=\operatorname{var}\left[\mu+\epsilon_{t}+\theta \epsilon_{t-1}\right]\\
&=\operatorname{var}\left[\epsilon_{t}+\theta \epsilon_{t-1}\right] \\
&=\operatorname{var}\left[\epsilon_{t}\right]+\operatorname{var}\left[\theta \epsilon_{t-1}\right]+2 \operatorname{cov}\left[\epsilon_{t}, \theta \epsilon_{t-1}\right] \\
&=\sigma^{2}+\theta^{2} \sigma^{2}+0\\
&=\left(1+\theta^{2}\right) \sigma^{2}
\end{aligned}
$$

For the $ k- $th order autocovariance, when $ k=1 $,

$$
\begin{aligned}
\gamma(1)=& \operatorname{cov}\left(X_{t}, X_{t-1}\right)=\operatorname{cov}\left(\mu+\epsilon_{t}+\theta \epsilon_{t-1}, \mu+\epsilon_{t-1}+\theta \epsilon_{t-2}\right) \\
=& \operatorname{cov}\left(\epsilon_{t}+\theta \epsilon_{t-1}, \epsilon_{t-1}+\theta \epsilon_{t-2}\right) \\
=& \operatorname{cov}\left(\epsilon_{t}, \epsilon_{t-1}\right)+\theta \operatorname{cov}\left(\epsilon_{t}, \epsilon_{t-2}\right)+\theta \operatorname{cov}\left(\epsilon_{t-1}, \epsilon_{t-1}\right) \\
&+\theta^{2} \operatorname{cov}\left(\epsilon_{t-1}, \epsilon_{t-2}\right)=\theta \sigma^{2}
\end{aligned}
$$

It is easy to show that $ \gamma(k)=0 $ for $ k>1 $.

Therefore, $ \rho(1)=\frac{\theta}{1+\theta^{2}} $ and $ \rho(k)=0 $ for $ k>1 $.

To summarize, if $ X_{t}=\mu+\epsilon_{t}+\theta \epsilon_{t-1} $ is a stationary process, we have

$$
\begin{array}{c}
E\left[X_{t}\right]=\mu \\
\operatorname{var}\left[X_{t}\right]=\left(1+\theta^{2}\right) \sigma^{2} \\
\gamma(k)=\left\{\begin{array}{cl}
\theta \sigma^{2} & \text { for } k=1 \\
0 & \text { for } k>1
\end{array}\right. \\
\rho(k)=\left\{\begin{array}{cc}
\frac{\theta}{1+\theta^{2}} & \text { for } k=1 \\
0 & \text { for } k>1
\end{array}\right.
\end{array}
$$

MA(q) process:

$$
Y_{t}=c+\varepsilon_{t}+\theta_{1} \varepsilon_{t-1}+\theta_{2} \varepsilon_{t-2}+\ldots+\theta_{q} \varepsilon_{t-q} \text{, where } \varepsilon_{t} \text{ is WN}\left(0, \sigma^{2}\right)
$$

Mean and autocovariance functions:

$$
\begin{aligned}
\mu &=E\left(Y_{t}\right)=E\left(c+\varepsilon_{t}+\theta_{1} \varepsilon_{t-1}+\theta_{2} \varepsilon_{t-2}+\ldots+\theta_{q} \varepsilon_{t-q}\right)=c \\
\gamma_{0} &=E\left[\left(Y_{t}-\mu\right)\left(Y_{t}-\mu\right)\right] \\
&=E\left(\varepsilon_{t}^{2}+\theta_{1}^{2} \varepsilon_{t-1}^{2}+\ldots+\theta_{q}^{2} \varepsilon_{t-q}^{2}\right)=\left(1+\theta_{1}^{2}+, \ldots+\theta_{q}^{2}\right) \sigma^{2} \\
\gamma_{j} &=E\left[\left(Y_{t}-\mu\right)\left(Y_{t-j}-\mu\right)\right] \\
&=\left(\theta_{j}+\theta_{j+1} \theta_{1}+\theta_{j+2} \theta_{2}+\ldots+\theta_{q} \theta_{q-j}\right) \sigma^{2}, \text { for } j=1,2, \ldots, q \\
\gamma_{j} &=0 \text { for } j>q
\end{aligned}
$$

$ \mathrm{MA}(\infty) $ process:

$$
Y_{t}=\mu+\psi_{0} \varepsilon_{t}+\psi_{1} \varepsilon_{t-1}+\psi_{2} \varepsilon_{t-2}+\ldots, \text { where } \varepsilon_{t} \text { is } W N\left(0, \sigma^{2}\right)
$$

$ Y_{t} $ is weakly stationary if $ \left\{\psi_{j}\right\}_{j=0}^{\infty} $ is square summable:

$$
\sum_{j=0}^{\infty} \psi_{j}^{2}<\infty
$$

a slightly stronger condition is absolutely summable

$$
\sum_{j=0}^{\infty}\left|\psi_{j}\right|<\infty
$$

Suppose that $ X_{t} $ follows a stationary $ \operatorname{AR}(1) $ model

$$
X_{t}=\rho X_{t-1}+\epsilon_{t}
$$

where $ |\rho|<1 $ and $ \epsilon_{t} \sim W N\left(0, \sigma^{2}\right) . $ Then, $ X_{t} $ can be written as an $ \operatorname{MA}(\infty) $ model as

$$
X_{t}=\epsilon_{t}+\rho \epsilon_{t-1}+\rho^{2} \epsilon_{t-2}+\ldots=\sum_{s=0}^{\infty} \rho^{s} \epsilon_{t-s}
$$

All AR model can be written in form of MA model.

## 2.3 ARMA(p,q) model

In time series analysis, we use stationary autoregressive moving average (ARMA) models to reveal the autocorrelation structures in the data.

An typical ARMA(p,q) model could be established as

$$
X_{t}=c+\underbrace{\rho_{1} X_{t-1}+\cdots+\rho_{p} X_{t-p}}_{\text {AR component }}+\epsilon_{t}+\underbrace{\theta_{1} \epsilon_{t-1}+\cdots+\theta_{q} \epsilon_{t-q}}_{\text {MA component }}
$$

Stationary Condition: the roots of $ 1-\rho_{1} z-\rho_{2} z^{2}-\ldots, \rho_{p} z^{p}=0 $ are outside of the unit circle. 

While in practice, we usually use **ARMA (1,1)** model to capture the dynamics in empirical time series data.

### ARMA model construction

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20201110164255.png)

#### Box-Jenkins Modeling Philosophy

Four-steps procedure:

- Transform the data to be stationary (taking log or difference);
- Make an initial guess of small values for $ p $ and $ q $ for an $ A R M A(p, q) $;
- Estimate the parameters in $ \Phi(L) $ (AR parameters) and $ \theta(L) $ (MA parameters);
- Perform a diagnostic analysis to confirm the model;

#### Identify the degree of MA process

Sample Autocorrelation is given by

$$
\widehat{\rho}_{j}=\frac{\widehat{\gamma}_{j}}{\hat{\gamma}_{0}}
$$

For a pure $ M A(q) $ process, for $ j>q, $ we have

$$
E\left(\widehat{\rho}_{j}\right)=0, \quad \operatorname{Var}\left(\widehat{\rho}_{j}\right)=\frac{1}{T}\left(1+2 \sum_{i=1}^{q} \rho_{i}^{2}\right)
$$

One can identify the degree of MA process by checking for the smallest value of $ h $ such as that all $ \hat{\rho}_{j} $ for any $ j>h, $ stays within

$$
\pm \frac{1.96}{\sqrt{T}} \sqrt{\left(1+2 \sum_{i=1}^{h-1} \widehat{\rho}_{i}^{2}\right)}
$$

It means that its ACF cuts off at lag q.

#### Identify the degree of AR process

Consider the regression

$$
y_{t+1}=\widehat{c}+\widehat{\alpha}_{1}^{(m)} y_{t}++\widehat{\alpha}_{2}^{(m)} y_{t-1}+\ldots++\widehat{\alpha}_{m}^{(m)} y_{t-m+1}+\widehat{e}_{t}
$$

where $ \hat{\alpha}_{m}^{(m)} $ is called $ m- $ th **partial autocorrelation (PACF)**.

The estimation formula is

$$
\widehat{\alpha}^{(m)}=\left(\begin{array}{r}
\gamma_{0}, \gamma_{1}, \gamma_{2}, \ldots \ldots \gamma_{m-1} \\
\gamma_{1}, \gamma_{0}, \gamma_{1}, \ldots \ldots \gamma_{m-2} \\
\ldots \\
\gamma_{m-1}, \gamma_{m-2}, \gamma_{m-3}, \ldots \ldots \gamma_{0}
\end{array}\right)^{-1}\left(\begin{array}{c}
\gamma_{1} \\
\gamma_{2} \\
\ldots \\
\gamma_{m}
\end{array}\right)
$$

If the data were really generated by an $ A R(p) $ process, for $ m>p, $ we have

$$
E\left(\widehat{\alpha}_{m}^{(m)}\right)=0, \quad \operatorname{Var}\left(\widehat{\alpha}_{m}^{(m)}\right)=\frac{1}{T}
$$

One can use this result to identify the degree of AR process by checking for which value of $ h $ such as that $ \widehat{\alpha}_{h}^{(h)} $ stays within

$$
\pm \frac{1.96}{\sqrt{T}}
$$

It means that its PACF cuts off at lag q.

Summary: In practice, we use Box-Jenkins method to determine the orders in AR(p) and $ \mathrm{MA}(\mathrm{q}) $

- For a stationary $ \mathrm{AR}(\mathrm{p}) $ model, its ACF decays, while its PACF cuts off at lag $ p $.
- For a stationary MA(q) model, its PACF decays, while its ACF cuts off at lag $ q $

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20201110164709.png)

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20201110164642.png)

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20201110164739.png)

#### Ljung-Box test for white noise

- $ \mathrm{H}_0 : r_{t} $ is a white noise process;
- $ \mathrm{H}_1: r_{t} $ is not a white noise process.

The **Ljung-Box test** statistic is defined as

$$
Q_{m}=T(T+2) \sum_{j=1}^{m} \frac{1}{T-j} \widehat{\rho}_{j}^{2}
$$

where $ m \geq 1 $ is a prescribed integer.

We reject the null hypothesis at significant level $ \alpha $ if $ Q_{m}>\chi_{\alpha, m}^{2}, $ where $ \chi_{\alpha, m}^{2} $ is the top $ \alpha $ -th percentile of the $ \chi^{2} $ distribution with $ m $ degrees of freedom.

Residual diagnostics - An adequate model

$$
X_{t}=\alpha+\rho_{1} X_{t-1}+\epsilon_{t}+\theta_{1} \epsilon_{t-1}
$$

An ARMA $ (p, q) $ model is said to be **adequate** if there is <u>no serial correlation in the residuals</u>. In other words, if you can still find significant autocorrelation in the residuals, the model is not adequate. Then, you should extend your model to capture remaining structures of autocorrelations.

Note that when we test for white noise in the residuals of an estimate ARMA(p,q) model, we should adjust the distribution in the Ljung-Box test from $ \chi_{m}^{2} $ to $ \chi_{m-p-q}^{2} $.

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20201110164904.png)

#### Model identification based on information criteria

The previous method is powerless in detecting overfitting, which leads to an unnecessarily complicated model with some redundant parameters. 

Therefore, they increase errors in the estimated parameters. To solve this problem, we use AlC and BIC to <u>combine the consideration on both the goodness of the fit and the simplicity of the model by penalizing extra terms in the model</u>.

【AIC】Model identification based on information criteria The Akaike's information criterion (AIC) is defined as

$$
A I C=-2 \log L+2 k
$$

where log $ L $ is the log likelihood, $ k $ is the number of parameters.

【BIC】The Bayesian information criterion $ (\mathrm{BIC}) $ is defined as

$$
B I C=-2 \log L+2 k \log n
$$

where $ n $ is the number of observations.

We choose a better model with a smaller value of AIC or BIC.

### Forecast based on ARMA models

Suppose that stock market return follows a stationary AR(1) process

$$
r_{t}=c+\rho r_{t-1}+\epsilon_{t}
$$

where $ \epsilon_{t} \sim W N\left(0, \sigma^{2}\right) . $ Let $ F_{t}=\left\{r_{1}, r_{2}, \ldots, r_{t}\right\} $ denote the information set up to time $ t $.

1. Compute the 1-step ahead forecast of $ r_{T} $ based on $ F_{T}, $ i.e., $ E\left[r_{T+1} \mid F_{T}\right] $
2. Compute the 2 -step ahead forecast of $ r_{T} $ based on $ F_{T}, $ i.e., $ E\left[r_{T+2} \mid F_{T}\right] $
3. Compute the k-step ahead forecast of $ r_{T} $ based on $ F_{T}, $ i.e., $ E\left[r_{T+k} \mid F_{T}\right] $
4. What happens if $ k \rightarrow \infty ? $

## 2.4 Unit root process

### 2.4.1 Random walks

Suppose that $ X_{t} $ follows a random walk process

$$
X_{t}=c+X_{t-1}+e_{t}
$$

for $ t=1,2, \ldots, T, $ where $ c \neq 0, e_{t} \sim W N\left(0, \sigma^{2}\right), $ and $ X_{0}=0 $.

We can show that $ X_{t}=c t+\sum_{s=1}^{t} e_{s} $ for $ t=1,2, \ldots, T $.

Then we can show that 

- $ E\left[X_{t}\right]=c t, \operatorname{Var}\left[X_{t}\right]=t \sigma^{2} $.
- $ \operatorname{Cov}\left(X_{t}, X_{t-k}\right)=(t-k) \sigma^{2} $

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20201110164957.png)

Predicting R.W. processes

Suppose that stock price follows a random walk process

$$
P_{t}=c+P_{t-1}+\epsilon_{t}
$$

where $ \epsilon_{t} \sim W N\left(0, \sigma^{2}\right) $ and $ P_{0} $ is a constant. Let $ F_{t}=\left\{P_{0}, P_{1}, \ldots, P_{t}\right\} $ be the information set up to time $ t $.

For some integer $ k>1 $, compute the forecast value and variance of $ P_{t+k} $ based on $ F_{T}, $ i.e., $ E\left[P_{T+k} \mid F_{T}\right] $ and $ \operatorname{Var}\left[P_{T+k} \mid F_{T}\right] . $ What happens if $ k \rightarrow \infty $

### 2.4.2 Trend-stationary process

A trend-stationary process is a deterministic time trend plus a stationary process. For example,

$$
Y_{t}=\alpha+\delta t+e_{t}
$$

where $ \alpha+\delta t $ is a linear time trend, $ e_{t} $ is a stationary process, e.g.
stationary process like $ e_{t}=\theta(L) \epsilon_{t}, $ in which $ \epsilon_{t} \sim W N\left(0, \sigma^{2}\right) . $ 

Then, $ \epsilon_{t} $ only causes transitory shocks to $ Y_{t}, $ i.e., the effect caused by $ \epsilon_{t} $ to $ Y_{t+k} $ decays with $ k $.

Question: Suppose that $ Y_{t}=\alpha+\delta t+e_{t}, e_{t}=0.5 e_{t-1}+\epsilon_{t}, $ where
$ \epsilon_{t} \sim W N\left(0, \sigma^{2}\right) . $ Compute the effect caused by $ \epsilon_{t} $ to $ Y_{t+k}, $ i.e., the change of $ Y_{t+k} $ for one unit change in $ \epsilon_{t} $.

Suppose that $ \Phi(L) X_{t}=\theta(L) \epsilon_{t}, $ then $ X_{t} $ is called a unit root process if $ \Phi(z)=0 $ has a root of unity $ (z=1) . $ e.g., random walk

$$
X_{t}=X_{t-1}+e_{t}
$$

is a unit root process, where $ e_{t} $ is a stationary process.

Question: Suppose that $ X_{t}=X_{t-1}+e_{t}, e_{t}=0.5 e_{t-1}+\epsilon_{t}, $ where $ \epsilon_{t} \sim W N\left(0, \sigma^{2}\right), $ compute the effect caused by $ \epsilon_{t} $ to $ X_{t+k} $

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20201110165101.png)

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20201110165124.png)

The Dickey-Fuller (DF) unit root test

- $ \mathbb{H}_{0}: X_{t} $ has a unit root

- $ \mathbb{H}_{1}: X_{t} $ is (trend)-stationary

Test equations:

- (None) $ \Delta X_{t}=\rho X_{t-1}+e_{t} $
- (Constant) $ \Delta X_{t}=\alpha+\rho X_{t-1}+e_{t} $
- (Constant+Trend) $ \Delta X_{t}=\alpha+\delta t+\rho X_{t-1}+e_{t} $

Equivalently, we need to test

$$
\mathbb{H}_{0}: \rho=0 \quad \text { v.s. } \quad \mathbb{H}_{1}: \rho<0
$$

### 2.4.2 Stationary linear process

【Linear I(0) processes】A linear I(0) process can be written as a constant plus a zero-mean linear process $ \left\{u_{t}\right\} $ such that

$$
u_{t}=\psi(L) \varepsilon_{t}, \psi(L) \equiv \psi_{0}+\psi_{1} L+\psi_{2} L^{2}+\cdots \text { for } t=0,\pm 1,\pm 2, \ldots
$$

$ \left\{\varepsilon_{t}\right\} $ is independent white noise (i.i.d. with mean 0 and $ \mathrm{E}\left(\varepsilon_{t}^{2}\right) \equiv \sigma^{2}>0 $）

$$
\begin{array}{c}
\sum_{j=0}^{\infty} j\left|\psi_{j}\right|<\infty \\
\psi(1) \neq 0
\end{array}
$$

#### Beveridge-Nelson decomposition

Approximating I(1) by a Random Walk: Let $ \left\{\xi_{t}\right\} $ be I(1) so that $ \Delta \xi_{t}=\delta+u_{t} $ where $ u_{t} \equiv \psi(L) \varepsilon_{t} $ is a zero-mean $ \mathrm{I}(0) $ process with $ \mathrm{E}\left(\xi_{0}^{2}\right)<\infty . $ Using the following identity:

$$
\begin{array}{c}
\psi(L)=\psi(1)+\Delta \alpha(L), \quad \Delta \equiv 1-L \\
\alpha(L) \equiv \sum_{j=0}^{\infty} \alpha_{j} L^{j}, \quad \alpha_{j}=-\left(\psi_{j+1}+\psi_{j+2}+\cdots\right) \quad(j=0,1,2, \ldots)
\end{array}
$$

we can write $ u_{t} $ as

$$
u_{t} \equiv \psi(L) \varepsilon_{t}=\psi(1) \cdot \varepsilon_{t}+\eta_{t}-\eta_{t-1} \text { with } \eta_{t} \equiv \alpha(L) \varepsilon_{t}
$$

It can be shown that $ \alpha(L) $ is absolutely summable. So, by Proposition $ 6.1(\mathrm{a}),\left\{\eta_{t}\right\} $ is a well-defined zero-mean covariance-stationary process (it is actually ergodic stationary by Proposition $ 6.1(\mathrm{d}) $ ) . Substituting $ (9.2 .6) $ into $ (9.1 .4), $ we obtain (what is known in econometrics as) the Beveridge-Nelson decomposition:

$$
\begin{aligned}
\xi_{t} &=\delta \cdot t+\sum_{s=1}^{t}\left[\psi(1) \cdot \varepsilon_{s}+\eta_{s}-\eta_{s-1}\right]+\xi_{0} \\
&=\delta \cdot t+\psi(1) \sum_{s=1}^{t} \varepsilon_{s}+\eta_{t}+\left(\xi_{0}-\eta_{0}\right)\left(\text { since } \sum_{s=1}^{t}\left(\eta_{s}-\eta_{s-1}\right)=\eta_{t}-\eta_{0}\right)
\end{aligned}
$$

#### Wiener process

【The Wiener Process】The next two sections will present a variety of unit-root tests. The limiting distributions of their test statistics will be written in terms of Wiener processes (also
called Brownian motion processes). Some of you may already be familiar with this from continuous-time finance, but to refresh your memory,

【Standard Wiener processes】A standard Wiener (**Brownian motion**) process $ W(\cdot) $ is a continuous-time stochastic process, associating each date $ t \in[0,1] $ with the scalar random variable $ W(t), $ such that

1. $ W(0)=0 $
2. for any dates $ 0 \leq t_{1}<t_{2}<\cdots<t_{k} \leq 1 $, the changes

$$
W\left(t_{2}\right)-W\left(t_{1}\right), W\left(t_{3}\right)-W\left(t_{2}\right), \ldots, W\left(t_{k}\right)-W\left(t_{k-1}\right)
$$

are independent multivariate normal with $ W(s)-W(t) \sim N(0,(s-t)) $ (so in $ \text { particular } W(1) \sim N(0,1)$).
3. for any realization, $ W(t) $ is continuous in $ t $ with probability 1.

### 2.4.3 Dickey-Fuller unit root test

In the first case with no intercept and no trend, we construct two test statistics as

$$
T \cdot \widehat{\rho} \rightarrow_{d} \frac{\frac{1}{2}\left(W(1)^{2}-1\right)}{\int_{0}^{1} W(r)^{2} d r} \triangleq D F_{\rho}
$$

and

$$
t=\frac{\widehat{\rho}}{\operatorname{se}(\widehat{\rho})} \rightarrow_{d} \frac{\frac{1}{2}\left(W(1)^{2}-1\right)}{\sqrt{\int_{0}^{1} W(r)^{2} d r}} \triangleq D F_{t}
$$

in which $ D F_{\rho} $ and $ D F_{t} $ are not the usual $ t $ -distribution.

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20201110165146.png)

The Augmented Dickey-Fuller (ADF) unit root test

- $ \mathbb{H}_{0}: X_{t} $ has a unit root
- $ \mathbb{H}_{1}: X_{t} $ is (trend)-stationary

Test equations:

- (None) $ \Delta X_{t}=\rho X_{t-1}+\theta_{1} \Delta y_{t-1}+\ldots+\theta_{p} \Delta y_{t-p}+e_{t} $
- (Constant)$ \Delta X_{t}=\alpha+\rho X_{t-1}+\theta_{1} \Delta y_{t-1}+\ldots+\theta_{p} \Delta y_{t-p}+e_{t} $
- (Constant+Trend) $ \Delta X_{t}=\alpha+\delta t+\rho X_{t-1}+\theta_{1} \Delta y_{t-1}+\ldots+\theta_{p} \Delta y_{t-p}+e_{t} $

Equivalently, we need to test

$$
\mathbb{H}_{0}: \rho=0 \quad \text { v.s. } \quad \mathbb{H}_{1}: \rho<0
$$

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20201110165305.png)

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20201110165345.png)

## 2.5 Order of integration

【integration】If time series $ X_{t} $ is a stationary process, then $ X_{t} \sim I(0) $; If time series $ X_{t} $ is an $ I(\mathrm{d}) $ process, if and only if $ \Delta^{j} X_{t} $ for $ 1 \leq j<d $ contain unit roots, and $ \Delta^{d} X_{t} \sim I(0) $ (stationary).

【Cointegration】Suppose that $ X_{t} $ and $ Y_{t} $ are $ I(\mathrm{d}) $ time series, and there exists some linear combination such that

$$
Y_{t}-X_{t}^{\prime} \beta \sim 1(0)
$$

then, $ X_{t} $ and $ Y_{t} $ are said to be **cointegrated**, and $ \beta $ is called the cointegrating vector (coefficient).

2003 Nobel Prize winner: Clive W.J. Granger and Robert F. Engle for their research on 'cointegration'.

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20201110165430.png)

Relationship between 1 year and 3 month interest rates.

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20201110165451.png)

The spread (difference) between 1 year and 3 month interest rates.

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20201110165518.png)

#### Trend removal methods

In classical time series models, the data are required to be stationary. If in practice, the data is nonstaitonary with trends, then one needs to remove the trend first to stationarize the data. Common trend-removal methods include

- Taking difference;
- Trend estimation and subtraction;
- HP filter;
- ...

##### HodrickPrescott filter

Suppose that

$$
y_{t}=\tau_{t}+c_{t}+\epsilon_{t}
$$

where $ \tau_{t} $ is trend, $ c_{t} $ is cycles, and $ \epsilon_{t} $ is disturbance. Given an adequately chosen, positive value of $ \lambda $, there is a trend component that will solve

$$
\min _{\tau}\left(\sum_{t=1}^{T}\left(y_{t}-\tau_{t}\right)^{2}+\lambda \sum_{t=2}^{T-1}\left[\left(\tau_{t+1}-\tau_{t}\right)-\left(\tau_{t}-\tau_{t-1}\right)\right]^{2}\right)
$$

ii.e. 拟合程度+惩罚系数*二阶导的大小(平滑程度)

$ \lambda $ should equal 6.25 for annual data, 1,600 for quarterly data, and 129,600 for monthly data.

An example: Global temperature anomalies

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20201110165556.png)

![](https://cdn.jsdelivr.net/gh/henrywu97/FigBed@master/2021/20220602233221.png)

![](https://cdn.jsdelivr.net/gh/henrywu97/FigBed@master/2021/20220602233244.png)

```R
library(mFilter)
f1gta=hpfilter(GTA,freq=120,type=c("lambda"))
plot(f1gta)
f2gta=hpfilter(GTA,freq=1200,type=c("lambda"))
plot(f2gta)
f3gta=hpfilter(GTA,freq=12000,type=c("lambda"))
plot(f3gta)

f1gta
## Title:
## Hodrick-Prescott Filter
## Call:
## hpfilter(x = GTA, freq = 120, type = c("lambda"))
## Method:
## hpfilter
## Filter Type:
## lambda
## Series:
## GTA
## GTA Trend Cycle
## 1 -0.255273 -0.285225 0.029952
## 2 -0.325364 -0.285402 -0.039962
## 3 -0.239818 -0.285330 0.045511
## 4 -0.270364 -0.285091 0.014727
## 5 -0.280818 -0.284390 0.003572
## 6 -0.337727 -0.282809 -0.054918
## 7 -0.233364 -0.279899 0.046535
```

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20201110165744.png)

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20201110165804.png)

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20201110165823.png)

![](https://cdn.jsdelivr.net/gh/henrywu97/FigBed@master/2021/20220602233302.png)

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20201110165920.png)