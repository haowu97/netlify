---
title: "Chapter3 Random Variable and Univariate Distributions"
date: 2021-05-04T17:38:51+08:00
draft: false

description: "Basic Concepts of Probability, Set Theory, Fundamental Probability Laws and Bayes' Formula."
upd: "Basic Concepts of Probability, Set Theory, Fundamental Probability Laws and Bayes' Formula."

tags: ['Notes']
categories: ['Probability and Statistic']
---

<!--more-->

## 3.1 Random Variables

**Definition (Random Variable)**: A random variable $ X(\cdot) $ is a $ \mathcal{B} $ -measurable mapping from the sample space $ S $ to $ \mathbb{R} $ such that to each $ s \in S, $ there exists a corresponding unique real number $ X(s) $ (i.e. point function).

**Remarks**: For each outcome $ s \in S, X(s) $ is a real number. The collection of all possible values that the random variable $ X $ can take, also called the range of $ X(\cdot), $ constitutes a new sample space, denoted as $ \Omega $.

**Example**: Toss a coin three times.

$$
S=\{\mathrm{HHH}, \mathrm{HHT}, \mathrm{HTH}, \mathrm{THH}, \mathrm{HTT}, \mathrm{THT}, \mathrm{TTH}, \mathrm{TTT}\}
$$

The associated $ \sigma $-algebra is $ \mathcal{B}=\{\text { all subsets of } S\} . $ Define $ X(s) $ as "number of heads in $ s " $, then

$$
\begin{array}{l}
X(\mathrm{T} \mathrm{T} \mathrm{T})=0, \quad X(\mathrm{T} \mathrm{TH})=X(\mathrm{THT})=X(\mathrm{HTT})=1 \\
X(\mathrm{HHT})=X(\mathrm{HTH})=X(\mathrm{THH})=2, \quad X(\mathrm{HHH})=3
\end{array}
$$

For any set $ A \in \mathcal{B}_{\Omega}, $ where $ \mathcal{B}_{\Omega} $ is a $ \sigma $ -field associated with $ \Omega, $ we can define a probability function $ P_{X}: \mathcal{B}_{\Omega} \rightarrow \mathbb{R} $ in terms of the original probability function $ P(\cdot) $ as

$$
P_{X}(A)=P(\{s \in S: X(s) \in A\})
$$

- $ P_{X}(\cdot), $ so-called the induced probability function, is indeed a probability function.
- $ P_{X}(\Omega)=P(S)=1 $
- Require additional condition to ensure that

$$
\{s \in S: X(s) \in A\} \in \mathcal{B}
$$

**Definition (Measurable Function)**: A function $ X: S \rightarrow \mathbb{R} $ is $ \mathcal{B} $ -measurable if for every real number $ x $, the set $ \{s \in S: X(s) \leqslant x\} \in \mathcal{B} $

- This is a regulation condition to guarantee that $ P(\{s \in S: X(s) \leqslant x\}) $ exists.
- Equivalently, $ X $ is $ \mathcal{B} $ -measurable if $ \{s \in S: X(s) \in B\} \in \mathcal{B} $ for every Borel set $ B $.

- Let $ X(\cdot) $ and $ Y(\cdot) $ be $ \mathcal{B} $ -measurable functions and $ c \in \mathbb{R} $. 
    - Then $ c X(\cdot), X(\cdot)+Y(\cdot), X(\cdot) Y(\cdot), $ and $ |X(\cdot)| $ are $ \mathcal{B} $ -measurable;
    - $ X(\cdot) / Y(\cdot) $ is $ \mathcal{B} $ -measurable if $ Y(s) \neq 0 $ for all $ s $
- Let $ X_{1}(\cdot), X_{2}(\cdot), \ldots $ be a sequence of $ \mathcal{B} $ -measurable functions. Then
    - $ \min \left\{X_{1}(\cdot), \ldots, X_{n}(\cdot)\right\} $ and $ \max \left\{X_{1}(\cdot), \ldots, X_{n}(\cdot)\right\} $ are
        $ \mathcal{B} $-measurable;
    - if $ X(\cdot)=\lim _{n \rightarrow \infty} X_{n}(\cdot) $ exists then $ X(\cdot) $ is $ \mathcal{B} $ -measurable.

- Let $ X(\cdot) $ be a $ \mathcal{B} $ -measurable function and $ h(\cdot) $ be a Borel-measurable real valued function, then $ h(X(\cdot)) $ is also $ \mathcal{B} $ -measurable.

## 3.2 Cumulative Distribution Function

**Definition (Cumulative Distribution Function)**: The cumulative distribution function $ (\mathrm{CDF}) $ of a random variable $ X $ is defined as

$$
F_{X}(x)=P(X \leqslant x) \text { for all } x \in \mathbb{R}
$$

**Example**: Toss a coin three times. Define $ X(s) $ as "number of heads in $ s^{\prime \prime} $ Then,

$$
P(X=0)=\frac{1}{8}, P(X=1)=P(X=2)=\frac{3}{8}, P(X=3)=\frac{1}{8}
$$

and therefore,

$$
F_{X}(x)=\left\{\begin{array}{ll}
0 & \text { if } x<0 \\
1 / 8 & \text { if } 0 \leqslant x<1 \\
1 / 2 & \text { if } 1 \leqslant x<2 \\
7 / 8 & \text { if } 2 \leqslant x<3 \\
1 & \text { if } x \geqslant 3
\end{array}\right.
$$

**Example**: The experiment consists of shooting once at a circular target $ T $ of radius $ r . $ Assume that it is certain that the target will be hit, and the probability of hitting a particular section $ A $ is the ratio of the area of $ A $ to the area of $ T $. Define random variable $ X(s) $ as the distance between the hitting point $ s $ and the center.

- Clearly, $ 0 \leqslant X \leqslant r . $ So $ F_{X}(x)=0 $ for $ x<0 $ and $ F_{X}(x)=1 $ for $ x>r $
- For $ 0 \leqslant x \leqslant r $

$$
F_{X}(x)=P(X \leqslant x)=\left(\pi x^{2}\right) /\left(\pi r^{2}\right)=x^{2} / r^{2}
$$

Properties of CDF:

1. $ F x(x) $ is non-decreasing, i.e. $ F_X \left(x_{1}\right) \leqslant F_X \left(x_{2}\right) $ for any $ x_{1}<x_{2} $
2. $ \lim _{x \rightarrow-\infty} F_{X}(x)=0, \lim _{x \rightarrow+\infty} F_{X}(x)=1 $
3. $ F_{X}(x) $ is right-continuous, i.e. for all $ x $

$$
\lim _{\delta \rightarrow 0^{+}} F_{X}(x+\delta)=F_{X}(x)
$$

**Remark**: 

- If a function $ F(\cdot) $ satisfies properties (1),(2) and (3), then there is a random variable $ X^{*} $ such that $ P\left(X^{*} \leqslant x\right)=F(x) $
- In some textbooks, the CDF is defined as $ F_{X}(x)=P(X<x) $. Then $ F_{X}(x) $ is left-continuous under this definition.

Properties of CDF:

- $ P(a<X \leqslant b)=F_{X}(b)-F_{X}(a) $ for $ a<b $
- $ P(X>b)=1-F_{X}(b) $
- Suppose $ F_{1}(x) $ and $ F_{2}(x) $ are two CDF's, then for $ 0<p<1 $ $ F(x)=p F_{1}(x)+(1-p) F_{2}(x) $ is also a $ C D F $

**Definition (Identical Distribution)**: Two random variables $ X $ and $ Y $ are identically distributed if for every Borel set $ B $, $ P(X \in B)=P(Y \in B) $

**Theorem**: Two random variables $ X $ and $ Y $ are identically distributed if and only if $ F_{X}(x)=F_{Y}(x) $ for all $ x \in \mathbb{R} $

## 3.3 Discrete Random Variable

**Definition (Discrete Random Variable)**: If a random variable $ X $ can only take a countable number of values, then $ X $ is called a discrete random variable (DRV).

**Definition (Probability Mass Function)**: The probability mass function $ (\mathrm{PMF}) $ of a DRV $ X $ is defined as

$$
f_{X}(x)=P(X=x) \text { for all } x \in \mathbb{R}
$$

**Definition (Support of DRV)**: The collection of the points at which a DRV $ X $ has a positive probability is called the support of $ X $, denoted as

$$
\text { Support }(X)=\left\{x \in \mathbb{R}: f_{X}(x)>0\right\}
$$

Properties of PMF:

- $ 0 \leqslant f_{X}(x) \leqslant 1 $ for all $ x \in \mathbb{R} $
- $ F_{X}(x)=\sum_{y \in \text { Support }(X), y \leqslant x} f_{X}(y) $
- $ \sum_{y \in \text { Support }(X)} f_{X}(y)=1 $
- $ f_{X}(x)=F_{X}(x)-\lim _{\delta \rightarrow 0^{+}} F_{X}(x-\delta) $

**Example (Discrete Uniform Distribution)**: A DRV $ X $ follows uniform distribution $ U(n) $ if its PMF

$$
f_{X}(x)=\left\{\begin{array}{ll}
1 / n & \text { if } x=1,2, \ldots, n \\
0 & \text { otherwise }
\end{array}\right.
$$

The CDF of uniform distributed DRV $ X $ is

$$
F_{X}(x)=\left\{\begin{array}{ll}
0 & \text { if } x<1 \\
i / n & \text { if } i \leqslant x<i+1, i=1, \ldots, n-1 \\
1 & \text { if } x \geqslant n
\end{array}\right.
$$

**Example (Bernoulli Distribution)**: A DRV $ X $ follows a Bernoulli(p) $ (0<p<1) $ distribution if its PMF

$$
f_{X}(x)=\left\{\begin{array}{ll}
p & \text { if } x=1 \\
1-p & \text { if } x=0
\end{array}\right.
$$

**Example (Binomial Distribution)**: A DRV $ X $ follows a binomial $ B(n, p) $ distribution if its PMF

$$
f_{X}(x)=C_{n}^{x} p^{x}(1-p)^{n-x}, \quad x=0,1, \ldots, n
$$

Remark: Toss a coin $ n $ times independently. Each time the head has probability $ p . $ The total number of heads follows $ B(n, p) $ distribution.

**Example (Poisson Distribution)**: A DRV $ X $ follows a Poisson$ (\lambda)(\lambda>0) $ distribution if its PMF

$$
f_{X}(x)=\frac{e^{-\lambda} \lambda^{x}}{x !}, \quad x=0,1,2, \ldots
$$

**Remarks**:

- Support of Poisson distribution is an infinite countable set.
- $ \sum_{x=0}^{\infty} f_{X}(x)=e^{-\lambda} \sum_{x=0}^{\infty} \lambda^{x} / x !=e^{-\lambda} e^{\lambda}=1 $
- In a Poisson process with intensity $ \lambda, $ the total number of occurrences over $ (0, t] $ follows a Poisson $ (\lambda t) $ distribution.
- **Poisson distribution can be used to describe the number of jumps in financial markets in a certain period**.

**Theorem (Poisson Limit Theorem)**: Denote $ p_{n} $ as the probability that the event $ A $ occurs in a random experiment and it is related to the number of total experiments $ n $. $ X $ is the number that $ A $ occurs in $ n $ experiments. If $ n p_{n} \rightarrow \lambda $ as $ n \rightarrow \infty, $ then we have

$$
P(X=k) \rightarrow \frac{\lambda^{k}}{k !} e^{-\lambda}
$$

**Example (Geometric Distribution)**: The geometric distribution is the probability distribution of the number of Bernoulli trials required to obtain the first success. The PMF of a geometric distributed DRV $ X $ is

$$
f_{X}(x)=p(1-p)^{x-1}
$$

**Remarks**:

- The geometric distribution is the simplest of **the waiting time distributions**.
- The geometric distribution has the so-called "memoryless" property in the sense that for integers $ s>t, $ we have

$$
P(X>s \mid X>t)=P(X>s-t)
$$

## 3.4 Continuous Random Variables

**Definition (Continuous Random Variable)**: A random variable is called continuous (CRV) if its cumulative distribution function $ F_{X}(x) $ is continuous for all $ x \in \mathbb{R} $

**Definition (Absolute Continuity & Probability Density Function)**: The cumulative distribution function $ F_{X}(x) $ of a random variable $ X $ is called **absolutely continuous** with respect to Lebesgue measure if there exists a Borel-measurable function $ f_{X}(\cdot) $ such that

$$
F_{X}(x)=\int_{-\infty}^{x} f_{X}(u) \mathrm{d} u \quad \text { for all } x \in \mathbb{R}
$$

The function $ f_{X}: \mathbb{R} \rightarrow \mathbb{R} $ is called a probability density function (PDF) of $ X $.

- An absolutely continuous CDF is continuous; For some continuous CDF, absolute continuity may not hold.
- For those $ x $ 's where $ F_X(x) $ is differentiable, $ f_{X}(x)=F_{X}^{\prime}(x) $.

**Theorem (Properties of PDF)**: A function $ f_{X}(x) $ is a PDF of a CRV $ X $ if and only if

1. $ f_{X}(x) \geqslant 0 $ for almost everywhere $ x, $ and
2. $ \int_{-\infty}^{\infty} f_{X}(x) d x=1 $

Remark:

- For any nonnegative function $ g(x) $ with finite integral, i.e. $ 0<\int_{-\infty}^{\infty} g(x) \mathrm{d} x<\infty, f(x)=g(x) / \int_{-\infty}^{\infty} g(u) \mathrm{d} u $ is a $ \mathrm{PDF} $, where $ \int_{-\infty}^{\infty} g(u) \mathrm{d} u $ is called the normalizing constant.
- For a given continuous random variable, CDF is unique but
    $ \mathrm{PDF} $ is not.
- The probability density functions of a continuous random variable can be different on a set of measure $ 0 . $ The value of
    **the PDF can be changed arbitrarily on a sequence of countable points without altering the distribution** of $ X . $ For example,
    $ f_{X}(x)=\left\{\begin{array}{ll}e^{-x} & \text { if } x>0, \\ 0 & \text { otherwise },\end{array} \text { and } f_{X}(x)=\left\{\begin{array}{ll}e^{-x} & \text { if } x \geqslant 0 \\ 0 & \text { otherwise }\end{array}\right.\right. $
    represent the same distribution.

**Definition (Support of CRV)**: The support of a CRV $ X $ with PDF $ f_{X}(x) $ is defined as

$$
\text { Support }(X)=\left\{x \in \mathbb{R}: f_{X}(x)>0\right\}
$$

**Example (Continuous Uniform Distribution)**: A CRV $ X $ follows a uniform distribution on $ [a, b] $ if its PDF

$$
f_{X}(x)=\left\{\begin{array}{ll}
\frac{1}{b-a} & \text { if } a \leqslant x \leqslant b \\
0 & \text { otherwise }
\end{array}\right.
$$

**Example (Normal Distribution)**: A normally distributed random variable $ X \sim N\left(\mu, \sigma^{2}\right) $ has the PDF

$$
f_{X}(x)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left[-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right]
$$

where $ -\infty<\mu<\infty $ and $ \sigma>0 $.

| PDF of Normal Distributions                                  | CDF of Normal Distributions                                  |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20200730203436.png) | ![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20200730203456.png) |

**Example (Log-normal Distribution)**: $ X $ follows a log-normal $ \left(\mu, \sigma^{2}\right) $ distribution if its PDF

$$
f_{X}(x)=\left\{\begin{array}{ll}
\frac{1}{\sqrt{2 \pi} \sigma x} \exp \left[-\frac{(\ln x-\mu)^{2}}{2 \sigma^{2}}\right] & \text { if } x>0 \\
0 & \text { otherwise }
\end{array}\right.
$$

If $ X \sim $ log-normal $ \left(\mu, \sigma^{2}\right), $ then $ \ln X \sim N\left(\mu, \sigma^{2}\right) $.

**Example (Exponential Distribution)**: CRV $ X $ follows Exponential $ (\beta) $ distribution if its PDF

$$
f_{X}(x)=\left\{\begin{array}{ll}
\frac{1}{\beta} e^{-x / \beta} & \text { if } x>0 \\
0 & \text { otherwise }
\end{array}\right.
$$

where $ \beta>0 $.

**Remark**:

- The PDF can be written in expression with $ \lambda=1 / \beta $
- Exponential distribution is popular in modeling **duration** between financial events or economic events because of its
    "memoryless" property:

$$
P(X>t+s \mid X>t)=P(X>s), \quad s, t>0
$$

**Example (Gamma Distribution)**: A CRV $ X $ follows a Gamma $ (\alpha, \beta) $ distribution if its PDF

$$
f_{X}(x)=\left\{\begin{array}{ll}
\frac{1}{\Gamma(\alpha) \beta^{\alpha}} x^{\alpha-1} e^{-x / \beta} & \text { if } x>0 \\
0 & \text { otherwise }
\end{array}\right.
$$

where $ \alpha, \beta>0 $ and $ \Gamma(\alpha)=\int_{0}^{\infty} t^{\alpha-1} e^{-t} \mathrm{d} t $ is the gamma function.
- Two parameters: shape $ (\alpha) $ and scale $ (\beta) $
- Gamma $ (1, \beta)= $ Exponential $ (\beta) $
- Gamma $ (v / 2,2)=\chi^{2}(v) $

**Example (Beta Distribution)**: A CRV $ X $ follows a Beta $ (\alpha, \beta) $ distribution if its PDF

$$
f_{X}(x)=\left\{\begin{array}{ll}
\frac{1}{\mathrm{B}(\alpha, \beta)} x^{\alpha-1}(1-x)^{\beta-1} & \text { if } 0<x<1 \\
0 & \text { otherwise }
\end{array}\right.
$$

where $ \alpha, \beta>0 $ and $ \mathrm{B}(\alpha, \beta)=\int_{0}^{1} t^{\alpha-1}(1-t)^{\beta-1} \mathrm{d} t $ is the beta
function.

- The support of beta distribution is [0,1]
- $ \operatorname{Beta}(1,1)=U[0,1] $

| PDF of gamma distribution                                    | PDF of beta distribution                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20200730203555.png) | ![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20200730203804.png) |

## 3.5 Functions of Random Variable

Suppose $ g: \mathbb{R} \rightarrow \mathbb{R} $ is a Borel-measurable function (i.e. the preimage of any Borel set under $ g $ is a Borel set), then $ Y=g(X) $ is also a random variable. What is the distribution function of the new
random variable $ Y $?

### 3.5.1 Discrete Case

If $ X $ is a discrete random variable with PMF $ f_{X}(x), $ then the PMF of $ Y $ can by obtained by using

$$
f_{Y}(y)=\sum_{x \in \Omega_{X}(y)} f_{X}(x)
$$

where $ \Omega_{X}(y)=\left\{x \in \Omega_{X}: g(x)=y\right\} $ is the set of all possible values of $ x $ in the sample space $ \Omega x $ of $ X $ such that $ g(x)=y $.

Moreover, if function $ g: \mathbb{R} \rightarrow \mathbb{R} $ be strictly **monotonic**

$$
f_{Y}(y)=\sum_{x \in \Omega_{X}(y)} f_{X}(x)= \sum_{x \in g^{-1}(y)} f_{X}(x) 
$$
for y in support.

**Example**: Suppose random variable $ X $ has the distribution

| $x$        | -2   | -1   | 0    | 1    | 2    | 3    | 4    |
| ---------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| $ f_X(x) $ | 0.1  | 0.2  | 0.1  | 0.1  | 0.1  | 0.2  | 0.2  |

For function $ Y=X^2 $, the distribution is

| $ y $      | 0    | 1       | 4       | 9    | 16   |
| ---------- | ---- | ------- | ------- | ---- | ---- |
| $ f_Y(y) $ | 0.1  | 0.1+0.2 | 0.1+0.1 | 0.2  | 0.2  |

**Example (Binomial transformation)**: a CRV $ X $ has a binomial distribution and its PMF is

$$
f_{X}(x)=P(X=x)=\left(\begin{array}{l}
n \\
x
\end{array}\right) p^{x}(1-p)^{n-x}
$$

Consider the random variable $Y=g(x)=n-X $,

$$
\begin{aligned}
f_{Y}(y) &=\sum_{x \in g^{-1}(y)} f_{X}(x) \\
&=f_{X}(n-y) \\
&=\left(\begin{array}{c}
n \\
n-y
\end{array}\right) p^{n-y}(1-p)^{n-(n-y)} \\
&=\left(\begin{array}{l}
n \\
y
\end{array}\right)(1-p)^{y} p^{n-y}
\end{aligned}
$$

### 3.5.2 Continuous Case

#### Method 1: The CDF approach

The basic idea is first to find the CDF $ F_{Y}(y) $ of $ Y $ and then its $ \mathrm{PDF} $ by differentiation, $ f_{Y}(y)=F_{Y}^{\prime}(y) $

1. Identify the possible values of $ Y $ (i.e. the support of $ Y $ ). For
this purpose, it is useful to plot the function $ g(x) $
2. Find $ F_{Y}(y):  F_{Y}(y)=P(Y \leqslant y)=P(g(X) \leqslant y)  $
3. Differentiate the CDF $ F_{Y}(y) $ with respect to $ y $ :

$$
f_{Y}(y)=F_{Y}^{\prime}(y)
$$

If function $ g: \mathbb{R} \rightarrow \mathbb{R} $ be strictly **monotonic**, let

$$
\mathcal{X}=\left\{x: f_{X}(x)>0\right\} \text{ and } \mathcal{Y}=\{y: y=g(x) \text { for some } x \in \mathcal{X}\} 
$$

Monotone: one-to-one mapping

- Increasing

$$
\{x \in \mathcal{X}: g(x) \leq y\}=\left\{x \in \mathcal{X}: g^{-1}(g(x)) \leq g^{-1}(y)\right\}=\left\{x \in \mathcal{X}: x \leq g^{-1}(y)\right\} 
$$

- Decreasing

$$
 \{x \in \mathcal{X}: g(x) \leq y\}=\left\{x \in \mathcal{X}: g^{-1}(g(x)) \leq g^{-1}(y)\right\}=\left\{x \in \mathcal{X}: x \geq g^{-1}(y)\right\} 
$$

Transformation (Increasing)

$$
F_{Y}(y)=\int_{\left\{x \in \mathcal{X}: x \leq g^{-1}(y)\right\}} f_{X}(x) d x=\int_{-\infty}^{g^{-1}(y)} f_{X}(x) d x=F_{X}\left(g^{-1}(y)\right)
$$

Transformation (decreasing)

$$
F_{Y}(y)=\int_{g^{-1}(y)}^{\infty} f_{X}(x) d x=1-F_{X}\left(g^{-1}(x)\right)
$$

**Theorem**: Let $ X $ have cdf $ F_{X}(x), $ let $ Y=g(X), $ and let $ \mathcal{X} $ and $ \mathcal{Y} $ be defined as above.

1. If $ g $ is an increasing function on $ \mathcal{X}, F_{Y}(y)=F_{X}\left(g^{-1}(y)\right) $ for $ y \in \mathcal{Y} $.
2. If $ g $ is a decreasing function on $ \mathcal{X} $ and $ X $ is a continuous random variable, $ F_{Y}(y)=1-F_{X}\left(g^{-1}(y)\right) $ for $ y \in \mathcal{Y} $

**Example**: Suppose a CRV $ X $ has a PDF

$$
f_{X}(x)=\left\{\begin{array}{ll}
1 & \text { if }-\frac{1}{2}<x<\frac{1}{2} \\
0 & \text { otherwise }
\end{array}\right.
$$

Find the PDF of the following $ Y $:

1. $ Y=a+b X, b \neq 0 $
2. $ Y=X^{2} $
3. $ Y=|X| $

1) $ X \sim U\left(-\frac{1}{2}, \frac{1}{2}\right), Y=a+b X $ with $ b \neq 0 $

If $ b>0, $ since Support $ (X)=\left(-\frac{1}{2}, \frac{1}{2}\right), $ the support of $ Y $ is given by $ a-\frac{b}{2}<y<a+\frac{b}{2} . $ So for $ y \in\left(a-\frac{b}{2}, a+\frac{b}{2}\right) $

$$
\begin{aligned}
F_{Y}(y)&=P(Y\leqslant y)=P(a+b X \leqslant y)=P\left(X \leqslant \frac{y-a}{b}\right) \\
&=F_X \left(\frac{y-a}{b}\right)=\int_{-\frac{1}{2}}^{\frac{y-a}{b}} f_{X}(x) \mathrm{d} x=\frac{y-a}{b}+\frac{1}{2}
\end{aligned}
$$

then $ f_{Y}(y)=F_{Y}^{\prime}(y)=\frac{1}{b} $. Therefore,

$$
f_{Y}(y)=\left\{\begin{array}{ll}
\frac{1}{b} & \text { if } a-\frac{b}{2}<y<a+\frac{b}{2} \\
0 & \text { otherwise }
\end{array}\right.
$$

Similarly, if $ b<0, $ the support of $ Y $ is $ \left(a+\frac{b}{2}, a-\frac{b}{2}\right) . $ So for $ y \in\left(a+\frac{b}{2}, a-\frac{b}{2}\right) $,

$$
\begin{array}{c}
F_{Y}(y)&=P(a+b X \leqslant y)=P\left(X \geqslant \frac{y-a}{b}\right) \\
&=1-F x\left(\frac{y-a}{b}\right)=1-\left(\frac{y-a}{b}+\frac{1}{2}\right)
\end{array}
$$

and then $ f_{Y}(y)=F_{Y}^{\prime}(y)=-\frac{1}{b} $. It follows that

$$
f_{Y}(y)=\left\{\begin{array}{ll}
-\frac{1}{b} & \text { if } a+\frac{b}{2}<y<a-\frac{b}{2} \\
0 & \text { otherwise }
\end{array}\right.
$$

2) $ x \sim U\left(-\frac{1}{2}, \frac{1}{2}\right), Y=X^{2} $

Observe that $ 0 \leqslant x^{2}<1 / 4 $ for $ x $ in Support $ (X), $ then $ F_{Y}(y)=0 $ if $ y<0 $ and $ F_{Y}(y)=1 $ if $ y>1 / 4 $. For $ y \in\left[0, \frac{1}{4}\right] $

$$
\begin{array}{c}
F_{Y}(y)=P(Y \leqslant y)=P\left(X^{2} \leqslant y\right)=P(-\sqrt{y} \leqslant X \leqslant \sqrt{y}) \\
=F_{X}(\sqrt{y})-F_{X}(-\sqrt{y})=\int_{-\sqrt{y}}^{\sqrt{y}} f_{X}(x) \mathrm{d} x=2 \sqrt{y}
\end{array}
$$

By differentiation, $ f_{Y}(y)=\frac{1}{\sqrt{y}} . $ Therefore, we have

$$
f_{Y}(y)=\left\{\begin{array}{ll}
\frac{1}{\sqrt{y}} & \text { if } 0<y<\frac{1}{4} \\
0 & \text { otherwise }
\end{array}\right.
$$

3) $ X \sim U\left(-\frac{1}{2}, \frac{1}{2}\right), Y=|X| $

Since the Support $ (X)=\left(-\frac{1}{2}, \frac{1}{2}\right), $ we have Support $ (Y)=\left[0, \frac{1}{2}\right) $ Thus, for $ 0 \leqslant y<1 / 2 $,

$$
\begin{aligned}
F_{Y}(y)=P(Y \leqslant y) &=P(|X| \leqslant y)=P(-y \leqslant X \leqslant y) \\
&=F_{X}(y)-F_{X}(-y)=\int_{-y}^{y} f_{X}(x) \mathrm{d} x=2 y
\end{aligned}
$$

By differentiation, $ f_{Y}(y)=2 $. It follows that

$$
f_{Y}(y)=\left\{\begin{array}{ll}
2 & \text { if } 0 \leqslant y<1 / 2 \\
0 & \text { otherwise }
\end{array}\right.
$$

**Example**: Random variable $ X $ follows double exponential (or Laplace) distribution if

$$
f_{X}(x)=\frac{1}{2} \alpha e^{-\alpha|x|}, \quad x \in \mathbb{R}
$$

where $ \alpha>0 . $ Find the PDF of the following $ Y: $

1. $ Y=|X| $
2. $ Y=X^{2} $

1) $ x $ has PDF $ f_{X}(x)=\frac{\alpha}{2} e^{-\alpha|x|}, Y=|X| $

Since Support $ (Y)=[0, \infty), F_{Y}(y)=0 $ and therefore $ f_{Y}(y)=0 $ for
$ y \leqslant 0 . $ For $ y>0 $

$$
\begin{array}{c}
F_{Y}(y)=P(|X| \leqslant y)=P(-y \leqslant X \leqslant y)=F_{X}(y)-F_{X}(-y) \\
=\int_{-y}^{y} f_{X}(x) d x=\int_{-y}^{0} \frac{\alpha}{2} e^{\alpha x} d x+\int_{0}^{y} \frac{\alpha}{2} e^{-\alpha x} d x
\end{array}
$$

By differentiation,

$$
f_{Y}(y)=-(-1) \frac{\alpha}{2} e^{-\alpha y}+\frac{\alpha}{2} e^{-\alpha y}=\alpha e^{-\alpha y}
$$

for $ y>0 . $ Note $ Y=|X| $ follows exponential $ (1 / \alpha) $ distribution.

2) $ X $ has PDF $ f_{X}(x)=\frac{\alpha}{2} e^{-\alpha|x|}, Y=X^{2} $

Again, Support $ (Y)=[0, \infty), $ so $ f_{Y}(y)=0 $ for $ y \leqslant 0 . $ For $ y>0 $

$$
\begin{aligned}
F_{Y}(y)=& P\left(X^{2} \leqslant y\right)=P(-\sqrt{y} \leqslant X \leqslant \sqrt{y}) \\
&=\int_{-\sqrt{y}}^{\sqrt{y}} f_{X}(x) \mathrm{d} x=\int_{-\sqrt{y}}^{0} \frac{\alpha}{2} e^{\alpha x} \mathrm{d} x+\int_{0}^{\sqrt{y}} \frac{\alpha}{2} e^{-\alpha x} \mathrm{d} x
\end{aligned}
$$

By differentiation,

$$
f_{Y}(y)=-\left(-\frac{1}{2 \sqrt{y}}\right) \frac{\alpha}{2} e^{-\alpha \sqrt{y}}+\frac{1}{2 \sqrt{y}} \cdot \frac{\alpha}{2} e^{-\alpha \sqrt{y}}=\frac{\alpha}{2 \sqrt{y}} e^{-\alpha \sqrt{y}}
$$

for $ y>0 $. And $ f_{Y}(y)=0 $ for $ y \leqslant 0 $

A Weibull distribution is given by

$$
f_{X}(x)=\left\{\begin{array}{ll}
\frac{\beta}{\delta}\left(\frac{x-\gamma}{\delta}\right)^{\beta-1} \exp \left[-\left(\frac{x-\gamma}{\delta}\right)^{\beta}\right] & \text { if } x>\gamma \\
0 & \text { otherwise }
\end{array}\right.
$$

$ Y $ follows Weibull distribution with $ \beta=\frac{1}{2}, \delta=\alpha^{-2}, \gamma=0 $

**Example**: Suppose $ X $ has PDF $ f_{X}(x)=\frac{3}{8}(x+1)^{2},-1<x<1, $ and

$$
Y=\left\{\begin{array}{ll}
1-X^{2} & \text { if } X \leqslant 0 \\
1-X & \text { if } X>0
\end{array}\right.
$$

Find the PDF of $ Y $.

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20200730205712.png)

Note that Support $ (Y)=(0,1], $ so for $ 0<y \leqslant 1 $

$$
\begin{array}{c}
F_{Y}(y)=P(Y \leqslant y)=P(-1<X \leqslant-\sqrt{1-y})+P(1-y \leqslant X<1) \\
=\int_{-1}^{-\sqrt{1-y}} f_{X}(x) d x+\int_{1-y}^{1} f_{X}(x) d x
\end{array}
$$

By differentiation,

$$
\begin{aligned}
f_{Y}(y) &=f_{X}(-\sqrt{1-y}) \frac{d(-\sqrt{1-y})}{d y}-f_{X}(1-y) \frac{d(1-y)}{d y} \\
&=\frac{3}{8}(1-\sqrt{1-y})^{2} \cdot \frac{1}{2 \sqrt{1-y}}+\frac{3}{8}(2-y)^{2}
\end{aligned}
$$

for $ 0<y \leqslant 1 $ and $ f_{Y}(y)=0 $ for $ y \leqslant 0 $ or $ y>1 $.

**Example (Probability Integral Transform)**: Suppose $ X $ has a continuous distribution $ F_{X}(x) $ which is strictly monotonically increasing. Find the PDF of $ Y=F_{X}(X) $.

The support of $ Y=F_{X}(X) $ is the unit interval $ [0,1] . $ Because
$ F_{X}(\cdot) $ is continuous and strictly monotonically increasing, its inverse function $ F_{X}^{-1}(\cdot) $ exists and is also strictly increasing. Thus, for $ 0 \leqslant y \leqslant 1 $,

$$
F_{Y}(y)=P\left(F_{X}(X) \leqslant y\right)=P\left(X \leqslant F_{X}^{-1}(y)\right)=F_{X}\left(F_{X}^{-1}(y)\right)=y
$$

If follows that the PDF

$$
f_{Y}(y)=\left\{\begin{array}{ll}1 & \text { if } 0 \leqslant y \leqslant 1 \\ 0 & \text { otherwise }\end{array} \quad(\text { i.e. } \quad Y \sim U[0,1])\right. 
$$

The application of probability integral transformation

- Let $ Z \sim U[0,1] $ and $ F_{X}(\cdot) $ is the CDF of some continuous random variable $ X $. If $ F_{X}(\cdot) $ is strictly increasing, then $ Y=F_{X}^{-1}(Z) $ has distribution $ F_{X}(\cdot) . $ (Why?)

- It can be used to simulate random samples of any CRV $ X $ :
  inverse transform sampling.
  - Generate a realization $ y $ from uniform distribution $ U[0,1] $
  - Solve for $ x $ from the equation $ F_X (x)=y $.
  - The value $ x=F_{X}^{-1}(y) $ is a realization of $ X $ with the specified distribution $ F_{X}(\cdot) $.
- When $ X $ is a DRV, the probability integral transform $ F_{X}(X) $ is no longer uniformly distributed. (How to generate random numbers from a discrete probability distribution via uniform distribution?)
- The result that $ F_{X}(X) \sim U[0,1] $ provides a basis for goodness-of-fit tests of distributional models. To **check whether a probability model $ F_{0}(\cdot) $ is correctly specified**, one can first compute the probability integral transform $ Y=F_{0}(X) $ and then check if $ Y $ follows the $ U[0,1] $ distribution using a sample $ \left(Y_{1}, Y_{2}, \ldots, Y_{n}\right) $ where $ Y_{i}=F_{0}\left(X_{i}\right) $.
- This is the basic idea behind the popular **Kolmogorov-Smirnov test** for a hypothesized distribution model.

#### Method 2: The transformation approach

**Theorem (Univariate Transformation)**: Let $ X $ be a CRV with PDF $ f_{X}(x) $ and let function $ g: \mathbb{R} \rightarrow \mathbb{R} $ be strictly **monotonic** and differentiable over the support of $ X $. Then the PDF of the random variable $ Y=g(X) $ is

$$
f_{Y}(y)= f_{X}\left[g^{-1}(y)\right]\left|\frac{d g^{-1}(y)}{d y}\right| =\left.\frac{1}{\left|g^{\prime}(x)\right|} f_{X}(x)\right|_{x=g^{-1}(y)}
$$

for any $ y $ in the support of $ Y, $ where $ x=g^{-1}(y) $ is the unique number in the support of $ X $ such that $ g(x)=y $.

**Theorem**: Suppose $ A_{1}, A_{2}, \ldots, A_{k} $ are disjoint regions and $ \bigcup_{i=1}^{k} A_{i}=\mathbb{R} $. Suppose $ g(x)=g_{i}(x) $ for all $ x \in A_{i}, i=1,2, \ldots, k $ and for each $ i $ $ g_{i}(x) $ is strictly monotonic and differentiable on region $ A_{i} . $ Then the PDF of $ Y=g(X) $ is given by

$$
f_{Y}(y)=\sum_{i=1}^{k} f_{X}\left(g_{i}^{-1}(y)\right) \frac{1}{\left|g_{i}^{\prime}\left(g_{i}^{-1}(y)\right)\right|}
$$

for all $ y $ in the support of $ Y $.

**Example (Square transformation)**: Suppose $ X $ is a continuous random variable. For $ y>0, $ the CDF of $ Y=X^{2} $ is $ F_{Y}(y)=P(Y \leq y)=P\left(X^{2} \leq y\right)=P(-\sqrt{y} \leq X \leq \sqrt{y}) $. Because $ X $ is continuous, we can drop the equality from the left endpoint and obtain

$$
\begin{aligned}
F_{Y}(y) &=P(-\sqrt{y}<X \leq \sqrt{y}) \\
&=P(X \leq \sqrt{y})-P(X \leq-\sqrt{y}) \\
&=F_{X}(\sqrt{y})-F_{X}(-\sqrt{y})
\end{aligned}
$$

The PDF of $ Y $ can now be obtained from the CDF by differentiation:

$$
\begin{aligned}
f_{Y}(y) &=\frac{d F_{Y}(y)}{d y} \\
&=\frac{d}{d y}\left(F_{Y}(\sqrt{y})-F_{X}(-\sqrt{y})\right) \\
&=\frac{1}{2 \sqrt{y}}\left(f_{X}(\sqrt{y})+f_{X}(-\sqrt{y})\right)
\end{aligned}
$$

## 3.6 Mathematical Expectations

**Definition (Expectation)**: Suppose $ X $ is a random variable with PMF or PDF $ f_{X}(x) $. Then the expectation of a measurable function $ g(X) $ is defined as

$$
\begin{aligned}
E[g(X)] &=\int_{-\infty}^{\infty} g(X) \mathrm{d} F_{X}(x) \\
&=\left\{\begin{array}{ll}
\sum_{x \in \Omega_{X}} g(x) f_{X}(x) & \text { if } X \text { is a } \mathrm{DRV} \\
\int_{-\infty}^{\infty} g(x) f_{X}(x) \mathrm{d} x & \text { if } X \text { is a } \mathrm{CRV}
\end{array}\right.
\end{aligned}
$$

where $ \Omega_{X} $ is the support of $ X $.

**Remarks**:

- $ E[g(X)] $ can be considered as the **weighted average** of $ g(X) $
- If $ E|g(X)|=\infty, $ we say $ E[g(X)] $ does not exist.
- If $ a $ is a constant, then $ E(a)=a $
- The expectation is a linear operator, namely,

$$
E\left[a \cdot g_{1}(X)+b \cdot g_{2}(X)\right]=a \cdot E\left[g_{1}(X)\right]+b \cdot E\left[g_{2}(X)\right]
$$

- $ Y=g(X) $ is also a random variable. Let the PMF or PDF of Y be $ f_{Y}(y), $ then we can also compute $ E[g(X)] $ by

$$
\begin{aligned}
E[g(X)] &=E(Y) \\
&=\left\{\begin{array}{ll}
\sum_{y \in \Omega_{Y}} y f_{Y}(y) & \text { if } Y \text { is } D R V \\
\int_{-\infty}^{\infty} y f_{Y}(y) d y & \text { if } Y \text { is } C R V
\end{array}\right.
\end{aligned}
$$

## 3.7 Moments

**Definition $ k $-th moment**: The $ k $-th **moment** of a random variable $ X $ is defined as

$$
E\left(X^{k}\right)=\left\{\begin{array}{ll}
\sum_{x \in \Omega_{X}} x^{k} f_{X}(x) & \text { if } X \text { is a } D R V \\
\int_{-\infty}^{\infty} x^{k} f_{X}(x) d x & \text { if } X \text { is a } C R V
\end{array}\right.
$$

Similarly, the $ k $-th **central moment** of a random variable $ X $ is defined as

$$
E(X-\mu_X)^{k}=\left\{\begin{array}{ll}
\sum_{x \in \Omega_{X}}\left(x-\mu_{X}\right)^{k} f_{X}(x) & \text { if } X \text { is a } D R V \\
\int_{-\infty}^{\infty}\left(x-\mu_{X}\right)^{k} f_{X}(x) d x & \text { if } X \text { is a } C R V
\end{array}\right.
$$

Relationship between uncentered moments and centered moments:

$$
E\left(X-\mu_{X}\right)^{k}=\sum_{i=0}^{k}\left(\begin{array}{c}
k \\
i
\end{array}\right)\left(-\mu_{X}\right)^{k-i} E\left(X^{i}\right)
$$

and

$$
E\left(X^{k}\right)=E\left[\left(X-\mu_{X}\right)+\mu_{X}\right]^{k}=\sum_{i=0}^{k}\left(\begin{array}{c}
k \\
i
\end{array}\right) \mu_{X}^{k-i} E\left(X-\mu_{X}\right)^{i}
$$

### 3.7.1 Mean

**Definition (Mean)**: The mean of a random variable $ X $ is defined as

$$
\mu_{X}=E(X)=\left\{\begin{array}{ll}
\sum_{x \in \Omega_{X}} x f_{X}(x) & \text { if } X \text { is a } D R V \\
\int_{-\infty}^{\infty} x f_{X}(x) d x & \text { if } X \text { is a } C R V
\end{array}\right.
$$

where $ \Omega_{X} $ is the support of $ X $.

**Remarks**:

- The mean $ \mu_X $ is also the expectation of $ X $ (i.e. $ g(X)=X $ ), or
    the first moment of $ X $.
- $ \mu_X $ is a measure of central tendency for the distribution of $ X $.
- The mean $ \mu x $ exists if and only if $ \sum_{x}|x| f_{X}(x)<\infty $ for $ D R V $ $ X $ or $ \int_{-\infty}^{\infty}|x| f_{X}(x) \mathrm{d} x<\infty $ for $ \mathrm{CRV} X $

**Example (Binomial Mean)**: lf $ X $ has a binomial distribution, its pmf is given by

$$
P(X=x)=\left(\begin{array}{l}
n \\
x
\end{array}\right) p^{x}(1-p)^{n-x}, x=0,1, \cdots, n
$$

where $ n $ is a positive integer, $ 0 \leq p \leq 1 $, and for every fixed pair $ n $ and $ p $ the pmf sums to 1. Calculate the $ E X $.

$$
\begin{aligned}
E X &=\sum_{x=0}^{n} x\left(\begin{array}{c}
n \\
x
\end{array}\right) p^{x}(1-p)^{n-x} \\
&=\sum_{x=0}^{n} x \frac{n !}{x !(n-x) !} p^{x}(1-p)^{n-x} \\
&=\sum_{x=0}^{n} \frac{n !}{(x-1) !(n-x) !} p^{x}(1-p)^{n-x}
\end{aligned}
$$

Since the $ x=0 $ term vanishes. Let $ y=x-1 $ and $ m=n-1 . $ Subbing $ x=y+1 $ and $ n=m+1 $ into the last sum (and using the fact that the limits $ x=1 $ and $ x=n $ correspond to $ \mathrm{y}=0 \text { and } y=n-1=m, \text { respectively }) $

$$
\begin{aligned}
E X &=\sum_{y=0}^{m} \frac{(m+1) !}{y !(m-y) !} p^{y+1}(1-p)^{m-y} \\
&=(m+1) p \sum_{y=0}^{m} \frac{m !}{y !(m-y) !} p^{y}(1-p)^{m-y} \\
&=n p
\end{aligned}
$$

**Example (Cauchy distribution)**: A random variable $ X $ follows Cauchy $ \left(x_{0}, \gamma\right) $ distribution if its PDF

$$
f_{X}(x)=\frac{1}{\pi \gamma}\left[\frac{\gamma^{2}}{\left(x-x_{0}\right)^{2}+\gamma^{2}}\right], \quad-\infty<x<\infty
$$

where $ x_{0} $ is the location parameter and $ \gamma $ is the scale parameter. 

Suppose $ X $ follows Cauchy (0,1) distribution, then

$$
E|X|=\int_{-\infty}^{\infty} \frac{|x|}{\pi\left(1+x^{2}\right)} \mathrm{d} x=\infty
$$

so **mean of Cauchy distribution does not exist**.

**Theorem**: Suppose $ E\left(X^{2}\right) $ exists. Then

$$
\mu x=\arg \min _{a} E(X-a)^{2}
$$

### 3.7.2 Variance

**Definition (Variance \& Standard Deviation)**: The variance of random variable $ X $ is defined as

$$
\begin{aligned}
\operatorname{Var}(X)&=\sigma_{X}^{2}= E\left(X-\mu_{X}\right)^{2} \\
&=\left\{\begin{array}{ll}
\sum_{x \in \Omega_{X}}\left(x-\mu_{X}\right)^{2} f_{X}(x) & \text { if } X \text { is a DRV, } \\
\int_{-\infty}^{\infty}\left(x-\mu_{X}\right)^{2} f_{X}(x) d x & \text { if } X \text { is a CRV. }
\end{array}\right.
\end{aligned}
$$

where $ \Omega X $ is the support of $ X $. The quantity $ \sigma_X =\sqrt{\sigma_{X}^{2}} $ is called the standard deviation of $ X $.

**Remarks**:

- $ \sigma_{X}^{2} $ is a measure of the degree of spread of a distribution around its mean.
- In economics, it is interpreted as a measure of **uncertainty** or risk. It is often called a measure of **volatility** of $ X $.
- If $ \sigma_{X}^{2}=0, $ then $ X=\mu_{X} $ with probability 1 and there is no variation in $ X $. This is so-called **degenerate distribution**.

**Theorem**

$$
\sigma_{X}^{2}=E\left(X^{2}\right)-\mu_{X}^{2}
$$

**Remark**: $ \sigma_{X}^{2} $ is called the second central moment and $ E\left(X^{2}\right) $ is
called the second moment.

**Theorem**: If $ Y=a X+b, $ then 

1. $ \mu_{Y}=a \mu_{X}+b $
2. $ \sigma_{Y}^{2}=a^{2} \sigma_{X}^{2} $

**Example (Portfolio selection)**: Assume that the investor likes higher return but lower risk. That is, his utility function $ u\left(\mu, \sigma^{2}\right) $ is such that $ \partial u / \partial \mu>0 $ (the more expected return, the better and $ \partial u / \partial \sigma^{2}<0 $ (the smaller risk, the better $ ) $. An example of $ u\left(\mu, \sigma^{2}\right) $ is

$$
u\left(\mu, \sigma^{2}\right)=a \mu-b \sigma^{2}, \quad a, b>0
$$

Assume that the investor has totally I dollars to be split between stock $ (w) $ and saving $ (I-w) . $ What is the portfolio that maximizes the utility function?

- The rate of return on stocks is a random variable $ Y $ with mean
    $ \mu_{Y} $ and variance $ \sigma_{Y}^{2} $.
- The rate of return on saving is constant $ r, $ which can be
    considered as a random variable $ Z $ with mean $ r $ and variance $ 0 . $
    Usually $ r<\mu_{Y} $.
- The return of a portfolio is $ X=w Y+(I-w) Z, $ then

$$
u\left(\mu, \sigma^{2}\right)=a\left[w \mu_{Y}+(I-w) r\right]-b w^{2} \sigma_{Y}^{2}
$$

- It is maximized when

$$
w=\frac{a\left(\mu_{Y}-r\right)}{2 b \sigma_{Y}^{2}}
$$

- If $ b=0 $ (i.e. the investor does not care risk), $ u\left(\mu, \sigma^{2}\right) $ is maximized at $ w=\infty $.
- If $ a=0, $ then $ u\left(\mu, \sigma^{2}\right) $ is maximized at $ w=0 $

### 3.7.3 Skewness

**Definition (Skewness)**: The third central moment $ E\left[\left(X-\mu_{X}\right)^{3}\right] $ is a measure of "skewness" (or asymmetry) of the distribution of $ X $. Skewness is defined as
$$
S_{X}=\frac{E\left[\left(X-\mu_{X}\right)^{3}\right]}{\sigma_{X}^{3}}
$$

The skewness has been used to measure financial crashes. Negative
(or positive) skewness indicates a higher (or lower) probability of
experiencing large losses than large gains.

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20200730210038.jpg)

### 3.7.4 Kurtosis

**Definition (Kurtosis)**: The fourth central moment $ E\left[\left(X-\mu_{X}\right)^{4}\right] $ is a measure of how heavy the tail of a distribution is. Kurtosis is defined as
$$
K_{X}=\frac{E\left[\left(X-\mu_{X}\right)^{4}\right]}{\sigma_{X}^{4}}
$$

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20200730210101.png)

Remarks:

- Kurtosis of normal distribution is 3, regardless of the values of
    parameters $ \mu $ and $ \sigma^{2} $.
- The excess kurtosis of a random variable $ X $ is defined as $ K_{X}-3 $.
- A distribution with positive excess kurtosis has a more acute peak around the mean and fatter tails. A distribution with
    negative excess kurtosis has a lower, wider peak and thinner
    tails.

## 3.8 Quantile

**Definition ($ \alpha $-quantile)**: Suppose random variable $ X $ has a $ \operatorname{CDF} F_{X}(x) . $ Let $ \alpha \in(0,1), $ then the $ \alpha $-quantile of the distribution $ F_X (x) $ is defined as $ Q(\alpha), $ which satisfies

$$
F_{X}(Q(\alpha))=P(X \leqslant Q(\alpha))=\alpha
$$

**Remarks**:

- When $ F_{X}(x) $ is continuous and strictly increasing,

$$
Q(\alpha)=F_{X}^{-1}(\alpha)
$$

- In case $ F_{X}(x) $ has flat regions or is discontinuous, we can define the $ \alpha $-quantile as

$$
Q(\alpha)=\inf \left\{x \in \mathbb{R}: F_{X}(x) \geqslant \alpha\right\}
$$

- For $ \alpha $-quantile $ Q(\alpha), $ we have

$$
\int_{-\infty}^{Q(\alpha)} f_{X}(x) \mathrm{d} x=\alpha
$$

- 0.5-quantile is called the median. $ 0.25 $-quantile and $ 0.75 $-quantile are called lower quartile and upper quartile.
- 0-quantile is the minimum value; 1-quantile is the maximum value.

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20200730210213.png)

**Difference between mean and median**:

- Median is the cutoff point that divides the population in half.
- Mean can be misleading when used to measure the location of
    highly skewed data. In contrast, **median is a more robust** measure of the central tendency of a distribution in the sense
    that it is not much affected by a few outliers.
- Median is the optimal solution for minimizing the mean absolute error, that is,

$$
Q(0.5)=\arg \min _{a} E|X-a|
$$

- while mean is the optimal solution for minimizing the mean
    squared error, that is,

$$
E(X)=\arg \min _{a} E(X-a)^{2}
$$

- For symmetric distribution, e.g. normal distribution, mean and median are the same. For skewed distributions, mean and
    median are different.

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20200730210243.png)

**Example (Value at Risk)**: The value at risk (VaR) at level $ \alpha, V_{t}(\alpha), $ of a portfolio over a certain time horizon is defined as

$$
P\left(X_{t}<-V_{t}(\alpha) \mid I_{t-1}\right)=\alpha
$$

where $ X_{t} $ is the return on the portfolio over the holding period $ t $, and $ I_{t-1} $ is the information available at time $ t-1 $. $ V_{t}(\alpha), $ which is the negative conditional quantile of portfolio return at level $ \alpha, $ is the threshold that actual loss will exceed with probability $ \alpha $.

## 3.9 Moment Generating Function

**Definition (Moment Generating Function)**: The moment generating function(MGF) of a random variable $ X $ is defined as

$$
\begin{aligned}
M_{X}(t) &=E\left[e^{t X}\right] \\
&=\left\{\begin{array}{ll}
\sum_{x \in \Omega_{X}} e^{t x} f_{X}(x) & \text { if } X \text { is a } D R V \\
\int_{-\infty}^{\infty} e^{t x} f_{X}(x) d x & \text { if } X \text { is a } C R V
\end{array}\right.
\end{aligned}
$$

Remarks:

- $ M_{X}(t) $ may not exist for some $ t \in \mathbb{R} . $ If the expectation does not exist for any small neighborhood of 0, then we say that MGF does not exist for the distribution of $ X $.
- The existence of the MGF $ M_{X}(t) $ implies the existence of an infinite set of moments.

**Theorem**: If $ M_{X}(t) $ exists for $ t $ in some neighborhood of $ 0, $ then

1. $ M_{X}(0)=1 $
2. for $ k=1,2, \ldots $, $ M_{X}^{(k)}(0)=E\left(X^{k}\right) $
3. the MGF of $ Y=a+b X $ is

$$
M_{Y}(t)=e^{a t} M_{X}(b t)
$$

for all $ t $ in a small neighborhood of 0.

**Proof**:

Assuming that we can differentiate under the integral sign, we have

$$
\begin{aligned}
\frac{d}{d t} M_{X}(t) &=\frac{d}{d t} \int_{-\infty}^{\infty} e^{t x} f_{X}(x) d x \\
&=\int_{-\infty}^{\infty}\left(\frac{d}{d t} e^{t x}\right) f_{X}(x) d x \\
&=\int_{-\infty}^{\infty}\left(x e^{t x}\right) f_{X}(x) d x \\
&=E\left(X e^{t X}\right)
\end{aligned}
$$

Thus, $ \left.\frac{d}{d t} M_{X}(t)\right|_{t=0}=\left.E\left(X e^{t X}\right)\right|_{t=0}=E X  $. 

Proceeding in an analogous manner, we can establish that

$$
\left.\frac{d^{n}}{d t^{n}} M_{X}(t)\right|_{t=0}=\left.E\left(X^{n} e^{t X}\right)\right|_{t=0}=E\left(X^{n}\right)
$$

### 3.9.1 Discrete Distributions

**Example (Binomial distribution)**: For binomial distribution with PMF

$$
\begin{array}{c}
f_{X}(x)=\left(\begin{array}{c}
n \\
x
\end{array}\right) p^{x}(1-p)^{n-x}, \quad x=0,1, \ldots, n \\
E(X)=n p, E\left(X^{2}\right)=n(n-1) p^{2}+n p, M_{X}(t)=\left(p e^{t}+1-p\right)^{n}
\end{array}
$$

**Proof**:

$$
\begin{aligned}
M_{X}(t) &=\sum_{x=0}^{n} e^{t x}\left(\begin{array}{c}
n \\
x
\end{array}\right) p^{x}(1-p)^{n-x} \\
&=\sum_{x=0}^{n}\left(\begin{array}{c}
n \\
x
\end{array}\right)\left(p e^{t}\right)^{x}(1-p)^{n-x}
\end{aligned}
$$

Using the binomial formula $$ \sum_{x=0}^{n}\left(\begin{array}{l}n \\ x\end{array}\right) u^{x} v^{n-x}, $$ hence, letting $$ u=p e^{t} $$ and $$ v=1-p, $$ we have

$$
M_{X}(t)=\left[p e^{t}+(1-p)\right]^{n}
$$

**Example (Poisson distribution)**: For Poisson distribution with PMF

$$
\begin{array}{c}
f_{X}(x)=e^{-\lambda} \frac{\lambda^{x}}{x !}, \quad x=0,1,2, \ldots, \\
E(X)=\lambda, E\left(X^{2}\right)=\lambda^{2}+\lambda, M_{X}(t)=\exp \left(e^{t} \lambda-\lambda\right)
\end{array}
$$

**Proof**:

$$
M_{Y}(t)=\sum_{x=0}^{\infty} e^{t x} \frac{e^{-\lambda \lambda^{x}}}{x !}=e^{-\lambda} \sum_{x=1}^{\infty} \frac{\left(e^{t} \lambda\right)^{x}}{x !}=e^{\lambda} e^{\lambda e^{t}}=e^{\lambda\left(e^{t}-1\right)} 
$$

**Example (Uniform distribution)**: For continuous uniform distribution on $ [a, b], $ its PDF

$$
f_{X}(x)=\left\{\begin{array}{ll}
\frac{1}{b-a} & \text { if } a \leqslant x \leqslant b \\
0 & \text { otherwise }
\end{array}\right.
$$

Then

$$
\begin{array}{c}
E\left(X^{k}\right)=\frac{b^{k+1}-a^{k+1}}{(k+1)(b-a)} \\
E(X)=\frac{a+b}{2}, \quad \sigma_{X}^{2}=\frac{(b-a)^{2}}{12} \\
M_{X}(t)=\frac{e^{t b}-e^{t a}}{(b-a) t}
\end{array}
$$

### 3.9.2 Continuous Distributions

**Example (Normal distribution)**: For normal distribution $ N\left(\mu, \sigma^{2}\right) $ with PDF

$$
\begin{array}{c}
f_{X}(x)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left[-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right], \quad-\infty<\mu<\infty, \sigma>0 \\
M_{X}(t)=\exp \left(\mu t+\sigma^{2} t^{2} / 2\right)
\end{array}
$$

**Proof**:

$$
M_{X}(t)=E\left(e^{x t}\right)=\int e^{x t} \frac{1}{\sqrt{2 \sigma^{2} \pi}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}} d x
$$

Define $$ z=\frac{x-\mu}{\sigma}, $$ which implies $$ x=\mu+\sigma z, $$ hence,

$$
M_{X}(t)=e^{\mu t} \int e^{z \sigma t} \frac{1}{\sqrt{2 \pi}} e^{\frac{-z^{2}}{2}}\left|\frac{d x}{d z}\right| d z=e^{\mu t} e^{\frac{1}{2} \sigma^{2} t^{2}}
$$

**Example (Gamma mgf)**

Gamma pdf : $$ f(x)=\frac{1}{\Gamma(\alpha) \beta^{\alpha}} x^{\alpha-1} e^{-x / \beta}, 0<x<\infty, \alpha>0, \beta>0 $$

Mgf:

$$
\begin{aligned}
M_{X}(t) &=\frac{1}{\Gamma(\alpha) \beta^{\alpha}} \int_{0}^{\infty} e^{t x} x^{\alpha-1} e^{-x / \beta} d x \\
&=\frac{1}{\Gamma(\alpha) \beta^{\alpha}} \int_{0}^{\infty} x^{\alpha-1} e^{-\left(\frac{1}{\beta}-t\right) x} d x \\
&=\frac{1}{\Gamma(\alpha) \beta^{\alpha}} \int_{0}^{\infty} x^{\alpha-1} e^{-x /\left(\frac{\beta}{1-\beta t}\right)} d x \\
&=\left(\frac{1}{1-\beta t}\right)^{\alpha} \int_{0}^{\infty} \frac{1}{\Gamma(\alpha)\left(\frac{\beta}{1-\beta t}\right)^{\alpha}} x^{\alpha-1} e^{-x /\left(\frac{\beta}{1-\beta t}\right)} d x \\
&=\left(\frac{1}{1-\beta t}\right)^{\alpha} \text { if } t<\frac{1}{\beta}
\end{aligned}
$$

**Example (Log-normal distribution)**: Suppose $ X $ follows a log-normal $ \left(\mu, \sigma^{2}\right) $ distribution. Let $ Y=\ln X $ then $ Y \sim N\left(\mu, \sigma^{2}\right) $ and therefore

$$
E\left(X^{k}\right)=E\left(e^{k Y}\right)=M_{Y}(k)=\exp \left(\mu k+\frac{\sigma^{2} k^{2}}{2}\right)
$$

The MGF $ M_{X}(t) $ does not exists for $ t>0 $.

### 3.9.3 Uniqueness of MGF

Recall that random variables $ X $ and $ Y $ are identically distributed if two CDF's $ F_{X}(\cdot) $ and $ F_{Y}(\cdot) $ are the same, i.e. $ F_{X}(x)=F_{Y}(x) $

- If $ X $ and $ Y $ are identically distributed, then for any $ g(\cdot) $,

$$
E[g(X)]=E[g(Y)]
$$

- Identity of the distributions of $ X $ and $ Y $ does not imply $ X=Y $.
    Example

**Example**: Suppose a fair coin is tossed $ n $ times, and let $ X $ be the number of heads obtained, $ Y $ be the number of tails obtained. Then
$ F_{X}(x)=F_{Y}(x) $ but $ X+Y=n $ and possibly $ X \neq Y $.

**Theorem (Uniqueness of MGF)**: Suppose two random variables $ X $ and $ Y $ with MGF's $ M_{X}(t) $ and $ M_{Y}(t) $ existing in a neighborhood of $ 0, N_{\epsilon}(0)=\{t:-\epsilon<t<\epsilon\} $. 

1. Then $ X $ and $ Y $ have the same $ M_{X}(t) $ and $ M_{Y}(t) $ for all $ t $ in $ N_{\epsilon}(0) $, if and only if $ F_{X}(u)=F_{Y}(u) $ for all $ u \in \mathbb{R} $.
2. If $ X $ and $ Y $ have bounded support, then $ F_{X}(u)=F_{Y}(u) $ for all $ u $ if and only if $ E\left(X^{r}\right)=E\left(Y^{r}\right) $ for all integers $ r=0,1,2, \cdots $

**Remarks**:

- If the MGF $ M_{X}(t) $ exists in a neighborhood of $ 0, $ it uniquely characterizes a distribution function.
- Given some MGF $ M_{X}(t), $ suppose we can find some $ \mathrm{CDF} $ $ F_{X}(x) $ that corresponds to $ M_{X}(t) . $ Then $ F_{X}(x) $ must be the only distribution that generates $ M_{X}(t) $
- Uniqueness of MGF can be used to prove CLT.

**Example**: A DRV $ X $ has $ M_{X}(t)=\frac{1}{2}+\frac{e^{t}}{4}+\frac{e^{-t}}{4} . $ Then its PMF

$$
f_{X}(x)=\left\{\begin{array}{ll}
1 / 4 & \text { if } x=-1 \\
1 / 2 & \text { if } x=0 \\
1 / 4 & \text { if } x=1
\end{array}\right.
$$

**Example**: Suppose $ M_{X}(t)=e^{2 t^{2}} . $ Recall the normal distribution $ N\left(\mu, \sigma^{2}\right) $ has $ \mathrm{MGF} $

$$
M_{X}(t)=\exp \left(\mu t+\frac{\sigma^{2}}{2} t^{2}\right)
$$

Let $ \mu=0 $ and $ \sigma^{2}=4, $ then $ M_{X}(t)=\exp \left(2 t^{2}\right) . $ So $ X \sim N(0,4) $

**Example**: If a DRV $ X $ has $ M_{X}(t)=\frac{1-r}{1-r e^{t}} $ for $ \left|e^{t}\right|<1 / r, $ where $ r>0 . $ What is the probability distribution of $ X ? $

Note that

$$
M_{X}(t)=(1-r) \sum_{x=0}^{\infty}\left(r e^{t}\right)^{x}=\sum_{x=0}^{\infty}(1-r) r^{x} \cdot e^{t x}=\sum_{x=0}^{\infty} f_{X}(x) e^{t x}
$$

where $ f_{X}(x)=(1-r) r^{x}, x=0,1,2, \ldots . $ since

$$
\sum_{x=0}^{\infty} f_{X}(x)=(1-r) \cdot \frac{1}{1-r}=1
$$

$ f_{X}(x) $ is the PMF of $ X $.

Existence of the MGF

- The existence of the MGF $ M_{X}(t) $ implies the existence of

$$
E(X), E\left(X^{2}\right), E\left(X^{3}\right), \ldots
$$

- The existence of all moments is not equivalent to the existence
    of moment generating function in a neighborhood of $ 0 . $

**Example (Log-normal distribution)**: Suppose $ X $ follows a log-normal $ \left(\mu, \sigma^{2}\right) $ distribution. Then

$$
E\left(X^{k}\right)=\exp \left(\mu k+\frac{\sigma^{2} k^{2}}{2}\right)
$$

but the MGF

$$
M_{X}(t)=E\left(e^{x t}\right)=\int_{0}^{\infty} \frac{1}{\sqrt{2 \pi} \sigma x} \exp \left[t x-\frac{(\ln x-\mu)^{2}}{2 \sigma^{2}}\right] \mathrm{d} x=\infty
$$

for $ t>0 $.

**The set of moments $ E\left(X^{k}\right), k=1,2, \ldots $ does not uniquely characterize a distribution function.**

**Example**: Consider two distributions with support $ (0, \infty) $ :

$$
f_{X}(x)=\frac{\exp \left[-\frac{(\ln x)^{2}}{2}\right]}{\sqrt{2 \pi} x}, \quad f_{Y}(y)=f_{X}(y)[1+\sin (2 \pi \ln y)]
$$

for $ x, y>0 . E\left(X^{k}\right)=E\left(Y^{k}\right) $ for $ k=1,2, \ldots $

**Theorem**: Let $ F_{X}(x) $ and $ F_{Y}(y) $ be two $ C D F^{\prime} s $ both of which have bounded support. Then $ F_{X}(z)=F_{Y}(z) $ for all $ z \in \mathbb{R} $ if and only if $ E\left(X^{k}\right)=E\left(Y^{k}\right) $ for all integers $ k=1,2, \ldots $

**Theorem (Convergence of MGF)**: Suppose $ \left\{X_{n}, n=1,2, \ldots\right\} $ is a sequence of random variables, each with $ \mathrm{CDF} F_{n}(x) $ and $ \mathrm{MGF} M_{n}(t) . $ Furthermore, suppose that

$$
\lim _{n \rightarrow \infty} M_{n}(t)=M_{X}(t)
$$

for all $ t $ in a neighborhood of $ 0, $ where $ M_{X}(t) $ is a MGF of a random variable $ X $ with $ \mathrm{CDF} F_{X}(x) . $ Then

$$
\lim _{n \rightarrow \infty} F_{n}(x)=F_{X}(x)
$$

for all continuous points $ x $ of $ F x(\cdot) $.

**Example (Poisson approximation)**: The MGF of the binomial distribution $ B(n, p) $ is

$$
M_{B}(t)=\left(p e^{t}+1-p\right)^{n}=\left[1+\frac{n p \cdot\left(e^{t}-1\right)}{n}\right]^{n}
$$

For every $ t, $ when $ n \rightarrow \infty $ and $ n p \rightarrow \lambda, $ we have

$$
M_{B}(t) \rightarrow e^{\lambda\left(e^{t}-1\right)}=M_{P}(t)
$$

which is the MGF of the Poisson distribution with parameter $ \lambda $.

## 3.10 Characteristic Function

**Definition (Characteristic Function)**: The characteristic function of a random variable $ X $ with $ \operatorname{CDF} F_{X}(x) $ is defined as

$$
\begin{aligned}
\varphi_{X}(t) &=E\left(e^{i t X}\right) \\
&=\int_{-\infty}^{\infty} e^{i t x} d F_{X}(x)
\end{aligned}
$$

where $ i=\sqrt{-1} $ and $ e^{i t x}=\cos (t x)+i \sin (t x) $.

For continuous random variable, the **characteristic function is the**
**Fourier transform of the PDF**, so the PDF $ f_{X}(x) $ can be recovered from the characteristic function by

$$
f_{X}(x)=\frac{1}{2 \pi} \int_{-\infty}^{\infty} e^{-i t x} \varphi_{X}(t) d t
$$

Properties of characteristic function:

- For any probability distribution, the characteristic function
    always exists and is bounded, i.e. for any $ t \in \mathbb{R} $,

$$
|\varphi x(t)| \leqslant \int_{-\infty}^{\infty}\left|e^{i t x}\right| d F_{X}(x)=\int_{-\infty}^{\infty} d F_{X}(x)=1
$$

- $ \varphi_{X}(0)=1 $
- $ \varphi x(t) $ is continuous over $ \mathbb{R} $
- If the MGF $ M_{X}(t) $ exists for $ t $ in some neighborhood of $ 0, $ then $ \varphi_{X}(t)=M_{X}(\text { it }) $ for all $ t \in \mathbb{R} $

**Theorem**: Suppose the $ k $-th moment of $ X $ exists. Then $ \varphi_{X}(t) $ is differentiable up to order $ k $ and

$$
\varphi_{X}^{(k)}(0)=\mathrm{i}^{k} E\left(X^{k}\right)
$$

**Theorem (Uniqueness of Characteristic Function)**: Suppose two random variables $ X $ and $ Y $ have characteristic functions $ \varphi_{X}(t) $ and $ \varphi_{Y}(t) $ respectively. Then $ X $ and $ Y $ are identically distributed if and only if $ \varphi x(t)=\varphi_{Y}(t) $ for all $ t \in \mathbb{R} $

**Remark**: It is important to check all $ t $ on the entire real line. But for MGF's, it is only necessary to check $ t $ in a neighborhood of $ 0 . $

**Theorem (Convergence of Characteristic Function)**: Let $ \left\{X_{n}\right\} $ be a sequence of random variables with CDF's $ F_{n}(x) $ and characteristic functions $ \varphi_{n}(t) $. Let $ X $ be a random variable with CDF $ F_X(x) $ and characteristic function $ \varphi_X (t) $. Let $ n \rightarrow \infty $.

1. If $ F_{n}(x) \rightarrow F_{X}(x) $ for all continuous points $ x $ of $ F_{X}(\cdot), $ then $ \varphi_{n}(t) \rightarrow \varphi_{X}(t) $ for every $ t \in \mathbb{R} $
2. Further, if $ \varphi_{n}(t) \rightarrow \varphi x(t) $ for all $ t \in \mathbb{R}, $ then $ F_{n}(x) \rightarrow F_{X}(x) $
    for all continuous points $ x $ of $ F x(\cdot) $.

**Leibnitz's Rule**: if $ f(x, \theta), a(\theta), $ and $ b(\theta) $ are differentiable respect to $ \theta, $ then

$$
\frac{d}{d \theta} \int_{a(\theta)}^{b(\theta)} f(x, \theta) d x=f(b(\theta), \theta) \frac{d}{d \theta} b(\theta)-f(a(\theta), \theta) \frac{d}{d \theta} a(\theta)+\int_{a(\theta)}^{b(\theta)} \frac{\partial}{\theta} f(x, \theta) d x
$$