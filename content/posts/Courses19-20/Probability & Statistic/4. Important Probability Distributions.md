# 4. Important Probability Distributions

[toc]

A class of probability measures, say PMF/ PDF $ f(x, \theta) $, indexed by parameter $ \theta, $ where the functional form $ f(\cdot, \cdot) $ is known

The family of probability distributions is called a class of parametric probability distribution models.

## 4.1 Discrete Probability Distributions

### 4.1.1 Discrete Uniform Distribution

A random variable $ X $ has a discrete uniform $ (1, \mathrm{N}) $ distribution if

$$
P(X=x \mid N)=\frac{1}{N}, x=1,2, \cdots, N
$$

where $ N $ is a specified integer. The distribution puts equal mass on each of the outcomes $ 1,2, \cdots, N $.

Since

$$
\sum_{i=1}^{k} i=\frac{k(k+1)}{2} \text { and } \sum_{i=1}^{n} i^{2}=\frac{k(k+1)(2 k+1)}{6}
$$

Hence, we have

$$
E(X)=\sum_{x=1}^{N} x P(X=x \mid N)=\sum_{x=1}^{N} x \frac{1}{N}=\frac{N+1}{2}
$$

and

$$
E X^{2}=\sum_{x=1}^{N} x^{2} \frac{1}{N}=\frac{(N+1)(2 N+1)}{6}
$$

and so

$$
\operatorname{Var}(X)=E X^{2}-(E X)^{2}=\frac{(N+1)(2 N+1)}{6}-\left(\frac{N+1}{2}\right)^{2}=\frac{(N+1)(N-1)}{12}
$$

### 4.1.2 Bernoulli Distribution

Bernoulli distribution has wide applications in economics and finance. Model binary data where the outcome only has two possibilities. $ X \sim $ Bernoulli $ (p) $, if $ \mathrm{PMF} $

$$
\begin{aligned}
f_{X}(x) &=\left\{\begin{array}{ll}
p & \text { if } x=1 \\
1-p & \text { if } x=0
\end{array}\right.\\
&=p^{x}(1-p)^{1-x}, \quad x \in\{0,1\}
\end{aligned}
$$

where the parameter $ p \in(0,1) $

- $ \boldsymbol{E}\left(X^{k}\right)=p $ for all $ k=1,2, \ldots $
- $ \operatorname{Var}(X)=p(1-p) $
- $ M_{X}(t)=p e^{t}+1-p, \quad t \in \mathbb{R} $
- The parameter $ p $ (the probability that the binary random variable takes value 1 ) will vary across individuals as a function of observable variables, say $ Z $. For example, logit model assume

$$
P(X=1 \mid Z)=\frac{1}{1+\exp \left(-\beta^{\prime} Z\right)}
$$

### 4.1.3 Binomial Distribution

If $ X_{1}, X_{2}, \ldots, X_{n} $ are i.i.d. Bernoulli $ (p) $ random variables, then

$$
X=\sum_{i=1}^{n} X_{i}
$$

follows a $ B(n, p) $ distribution. $ B(n, p) $ is the distribution of the number of successes in a sequence of $ n $ independent Bernoulli trials, each of which yields success with probability $ p $. Bernoulli $ (p) $ is in fact $ B(1, p) $.

$ X \sim B(n, p) $, if PMF

$$
f_{X}(x)=\left(\begin{array}{l}
n \\
x
\end{array}\right) p^{x}(1-p)^{n-x}, \quad x=0,1,2, \ldots, n
$$

where parameter $ n \in \mathbb{N} $ and parameter $ p \in(0,1) $

**Theorem**: For any real numbers $ x $ and $ y $ and integer $ n \geq 0 $,

$$
(x+y)^{n}=\sum_{i=0}^{n}\left(\begin{array}{l}
n \\
i
\end{array}\right) x^{i} y^{n-i}
$$

If we take $$ x=p $$ and $$ y=1-p, $$ we get

$$
1=(p+1-p)^{n}=\sum_{i=0}^{n}\left(\begin{array}{l}
n \\
i
\end{array}\right) p^{i}(1-p)^{n-i}
$$

- $ E(X)=n p $
- $ \operatorname{Var}(X)=n p(1-p) $
- $ M_{X}(t)=\left(p e^{t}+1-p\right)^{n} $

### 4.1.4 Geometric Distribution

Geometric distribution is the distribution of **the number of independent Bernoulli trials needed to get one success**,
supposing each trial has a probability $ p $ of success.

$ X \sim \text { Geom }(p) $, if PMF

$$
f_{X}(x)=(1-p)^{x-1} p, \quad x=1,2, \ldots
$$

where parameter $ p \in(0,1) $.

- $ E(X)=1 / p $
- $ \operatorname{Var}(X)=(1-p) / p^{2} $
- $ M_{X}(t)=\frac{p e^{t}}{1-(1-p) e^{t}}, \quad t<-\ln (1-p) $

**Memoryless property**: For integers $ s, t>0 $,

$$
P(X>t+s \mid X>t)=P(X>s)
$$

To show this, first note that

$$
\begin{aligned}
P(X>t) &=1-P(X \leqslant t) \\
&=1-\sum_{x=1}^{t} p(1-p)^{x-1} \\
&=1-\frac{p-p(1-p)^{t}}{1-(1-p)} \\
&=1-\left[1-(1-p)^{t}\right]=(1-p)^{t}
\end{aligned}
$$

and $ P(X>s)=(1-p)^{s}, P(X>t+s)=(1-p)^{t+s} $. Then

$$
\begin{aligned}
P(X>t+s \mid X>t) &=\frac{P(X>t+s, X>t)}{P(X>t)} \\
&=\frac{P(X>t+s)}{P(X>t)} \\
&=\frac{(1-p)^{t+s}}{(1-p)^{t}} \\
&=(1-p)^{s}=P(X>s)
\end{aligned}
$$

### 4.1.5 Negative Binomial Distribution

$ N B(r, p) $ is the distribution of the number of independent Bernoulli trials such that the $ r $-th success occurs at the $ X $-th trial, supposing each trial has a probability $ p $ of success, e.g. A family wish to have some fixed number of boys.

If $ X_{1}, X_{2}, \ldots, X_{r} $ are i.i.d. Geom $ (p) $ random variables, then

$$
X=\sum_{i=1}^{r} X_{i}
$$

follows a $ N B(r, p) $ distribution.

$ X \sim N B(r, p) $, if PMF

$$
\begin{aligned}
f_{X}(x)=&\left[\left(\begin{array}{l}
x-1 \\
r-1
\end{array}\right) p^{r-1}(1-p)^{(x-1)-(r-1)}\right] \cdot p \\
=&\left(\begin{array}{l}
x-1 \\
r-1
\end{array}\right) p^{r}(1-p)^{x-r}, \quad x=r, r+1, \ldots
\end{aligned}
$$

where parameter $ p \in(0,1) $.

- $ E(X)=r / p $
- $ \operatorname{Var}(X)=r(1-p) / p^{2} $
- $ M_{X}(t)=\left[\frac{p e^{t}}{1-(1-p) e^{t}}\right]^{r}, \quad t<-\ln (1-p) $

**Example**: Suppose that in a population of fruit flies, we are interested in the proportion having vestigial wings and decide to sample until we have found 100 such flies. The probability that we will have to examine at least $ N $ flies is

$$
\begin{aligned}
P(X \geq N) &=\sum_{x=N}^{\infty}\left(\begin{array}{c}
x-1 \\
99
\end{array}\right) p^{100} (1-p)^{x-100} \\
&=1-\sum_{x=100}^{N-1}\left(\begin{array}{c}
x-1 \\
99
\end{array}\right) p^{100}(1-p)^{x-100}
\end{aligned}
$$

### 4.1.6 Hyper-geometric Distribution

Large urn with $ N $ balls that are identical in every way except that $ M $ are red and $ N-M $ are green. We reach in, blindfolded, and select $ K $ balls at random. What is the probability that exactly $ x $ of the balls are red?

The numbers of 

- size $ K $ that can be drawn from $ N $ balls is $ C^K_N $. 
- choosing $ x $ red balls is $ C_M^x $ 
- choosing $ K-x $ green balls is $ C^{K-x}_{N-M} $

Then $ X $ has a hyper-geometric distribution given by

$$
P(X=x \mid N, M, K)=\frac{ C_M^x C^{K-x}_{N-M} }{ C^K_N }
$$

Support: $ M \geq x $ and $ N-M \geq K-x \Rightarrow M-(N-K) \leq x \leq M$

Expectation

$$
E X=\sum_{x=0}^{K} x \frac{\left(\begin{array}{c}
M \\
x
\end{array}\right)\left(\begin{array}{c}
N-M \\
K-x
\end{array}\right)}{\left(\begin{array}{c}
N \\
K
\end{array}\right)}
$$

by using the fact $ \sum_{x=1}^{K} \frac{\left(\begin{array}{c}M-1 \\ x-1\end{array}\right)\left(\begin{array}{c}N-M \\ K-x\end{array}\right)}{\left(\begin{array}{c}N-1 \\ K-1\end{array}\right)}=1, $ we have

$$
E X=\frac{K M}{N} \sum_{x=1}^{K} \frac{\left(\begin{array}{c}
M-1 \\
x-1
\end{array}\right)\left(\begin{array}{c}
N-M \\
K-x
\end{array}\right)}{\left(\begin{array}{c}
N-1 \\
K-1
\end{array}\right)}=\frac{K M}{N}
$$

Variance

$$
\operatorname{Var}(X)=\frac{K M}{N}\left(\frac{(N-M)(N-K)}{N(N-1)}\right)
$$

### 4.1.7 Poisson Distribution

$ X \sim $ Poisson $ (\lambda) $, if $ \mathrm{PMF} $

$$
f_{X}(x)=e^{-\lambda} \frac{\lambda^{x}}{x !}, \quad x=0,1,2, \ldots
$$

where the intensity parameter $ \lambda>0 $.

Recall the Taylor series expansion of $ e^{y}, e^{y}=\sum_{i=0}^{\infty} \frac{y^{i}}{i !} $.

$$
\sum_{x=0}^{\infty} P(X=x \mid \lambda)=e^{-\lambda} \sum_{x=0}^{\infty} \frac{\lambda^{x}}{x !}=e^{-\lambda} e^{\lambda}=1
$$

- Mean of $ X $

$$
\begin{aligned}
E X &=\sum_{x=0}^{\infty} x \frac{e^{-\lambda} \lambda^{x}}{x !}=\sum_{x=1}^{\infty} x \frac{e^{-\lambda} \lambda^{x}}{x !} \\
&=\lambda e^{-\lambda} \sum_{x=1}^{\infty} \frac{\lambda^{x-1}}{(x-1) !}=\lambda
\end{aligned}
$$

- $ \operatorname{Var}(X)=\lambda $
- $ \boldsymbol{M}_{X}(t)=e^{\lambda\left(e^{t}-1\right)} $

#### Poisson process

Counting **the number of random events** starting from $ t=0 $. $ N(t)= $ the number of events that have occurred during the time period $ [0, t] $.

Assumptions:

1. Stationary: For all $ m \geqslant 0 $ and for any time intervals $ \Delta_{1}=\Delta_{2}, P\left(m \text { events in } \Delta_{1}\right)=P\left(m \text { events in } \Delta_{2}\right) $
2. Independent increments: For all $ m \geqslant 0 $ and for any time interval $ (t, t+s], P(m \text { events in }(t, t+s]) $ is independent of how many events have occurred earlier or how they have occurred;
3. Sequencing: The occurring of two or more events in a very small time interval is practically impossible, i.e. $ \lim _{\Delta \rightarrow 0} P(N(\Delta)>1) / \Delta=0 $

**A stochastic process $ \{N(t)\} $ satisfying these three assumptions is called a stationary Poisson process.**

Suppose $ N(0)=0, $ then there exists $ \lambda>0 $ such that
$$
P(N(t)=m)=\frac{(\lambda t)^{m} e^{-\lambda t}}{m !}, \quad m=0,1,2, \ldots
$$

that is, for $ t>0, N(t) \sim $ Poisson $ (\lambda t) $

$ \lambda $ is the **average number of events during a unit time period**.

#### Poisson approximation

We can use a Poisson $ (\lambda) $ distribution to approximate a $ B(n, p) $ distribution when $ p \rightarrow 0 $ and $ n p \rightarrow \lambda $ as $ n \rightarrow \infty $.

The MGF of the binomial distribution $ B(n, p) $ is
$$
M_{B}(t)=\left(p e^{t}+1-p\right)^{n}=\left[1+\frac{n p \cdot\left(e^{t}-1\right)}{n}\right]^{n}
$$

For every $ t, $ when $ n \rightarrow \infty $ and $ n p \rightarrow \lambda, $ we have

$$
M_{B}(t) \rightarrow e^{\lambda\left(e^{t}-1\right)}=M_{P}(t)
$$

which is the MGF of Poisson $ (\lambda) $.

We can also show that the PMF of the binomial distribution
$ B(n, p) $ converges to the PMF of Poisson distribution.

**Example**: A typesetter on average makes one error in every 500 words typeset. What is the probability that there will be no more than two errors in 1500 words?

Use binomial distribution: Assume that making error in typing a word follows a Bernoulli distribution with $ p=1 / 500 $. Then the number of errors in 1500 words, $ X, $ follows a binomial distribution B(1500, p). So, 

$$
P(X \leqslant 2)=\sum_{x=0}^{2}
\left(\begin{array}{c}
1500 \\
x
\end{array}\right)
\left(\frac{1}{500}\right) \times\left(\frac{499}{500}\right)^{1500-x} \approx 0.4230
$$

Use Poisson approximation: Let $ \lambda=n p=3 $, then

$$
P(X \leqslant 2) \approx \sum_{x=0}^{2} \frac{\lambda^{x} e^{-\lambda}}{x !}=e^{-3}\left(1+\frac{3}{1 !}+\frac{3^{2}}{2 !}\right) \approx 0.4232
$$

## 4.2 Continuous Probability Distributions

### 4.2.1 Continuous Uniform Distribution

$ X \sim U[a, b] $, if $ \mathrm{PDF} $
$$
f_{X}(x)=\left\{\begin{array}{ll}
\frac{1}{b-a} & \text { if } a \leqslant x \leqslant b \\
0 & \text { otherwise }
\end{array}\right.
$$

- $ \boldsymbol{E}(X)=\frac{a+b}{2}, \quad E\left(X^{k}\right)=\frac{1}{b-a} \cdot \frac{b^{k+1}-a^{k+1}}{k+1} $
- $ \operatorname{Var}(X)=\frac{1}{12}(b-a)^{2} $
- $ M_{X}(t)=\frac{1}{t(b-a)}\left(e^{t b}-e^{t a}\right), \quad t \in \mathbb{R} $

### 4.2.2 Beta Distribution

$ X \sim \operatorname{Beta}(\alpha, \beta) $, if $ \mathrm{PDF} $

$$
f_{X}(x)=\frac{1}{B(\alpha, \beta)} x^{\alpha-1}(1-x)^{\beta-1}, \quad 0 \leqslant x \leqslant 1
$$

where $ \alpha>0, \beta>0, $ and $ B(\alpha, \beta) $ is the Beta function

$$
B(\alpha, \beta)=\int_{0}^{1} t^{\alpha-1}(1-t)^{\beta-1} \mathrm{d} t
$$

Moments

$$
\begin{aligned}
E X^{n} &=\frac{1}{B(\alpha, \beta)} \int_{0}^{1} x^{n} x^{\alpha-1}(1-x)^{\beta-1} d x \\
&=\frac{1}{B(\alpha, \beta)} \int_{0}^{1} x^{\alpha+n-1}(1-x)^{\beta-1} d x
\end{aligned}
$$

We now recognize the integrand as the kernel of a beta $ (\alpha+n, \beta) $ PDF, which means

$$
E\left(X^{n}\right)=\frac{B(\alpha+n, \beta)}{B(\alpha, \beta)}=\frac{\Gamma(\alpha+n) \Gamma(\alpha+\beta)}{\Gamma(\alpha+\beta+n) \Gamma(\alpha)}
$$

Set $ \mathrm{n}=1, \mathrm{n}=2, $ We then have

$$
E X=\frac{\alpha}{\alpha+\beta} \text { and } \operatorname{Var}(X)=\frac{\alpha \beta}{(\alpha+\beta)^{2}(\alpha+\beta+1)}
$$

And $ \boldsymbol{M}_{X}(t)=1+\sum_{j=1}^{\infty}\left(\prod_{i=0}^{j-1} \frac{\alpha+i}{\alpha+\beta+i}\right) \frac{t^{j}}{j !}, \quad t \in \mathbb{R} $.

### 4.2.3 Gamma Distribution

$ X \sim \text { Gamma }(\alpha, \beta) $, if $ \mathrm{PDF} $

$$
f_{X}(x)=\left\{\begin{array}{ll}
\frac{1}{\beta^{\alpha} \Gamma(\alpha)} x^{\alpha-1} e^{-x / \beta} & \text { if } x>0 \\
0 & \text { if } x \leqslant 0
\end{array}\right.
$$

where $ \alpha>0 $ is the shape parameter, $ \beta>0 $ is the scale parameter, and $ \Gamma(\alpha)=\int_{0}^{\infty} t^{\alpha-1} e^{-t} \mathrm{d} t $ is the **Gamma function**.

**Remark: Gamma function is an extension of the factorial function**.

1. $ \Gamma(k)=(k-1) ! $ if $ k \in \mathbb{N} $
2. $ \Gamma(\alpha+1)=\alpha \Gamma(\alpha) $
3. $ \Gamma(1 / 2)=\sqrt{\pi} $

Mean

$$
E X=\frac{1}{\Gamma(\alpha) \beta^{\alpha}} \int_{0}^{\infty} x x^{\alpha-1} e^{-x / \beta} d x
$$

Since $ \int_{0}^{\infty} x^{\alpha-1} e^{-x / \beta} d x=\Gamma(\alpha) \beta^{\alpha} $ (property of PDF),

$$
\begin{aligned}
E X &=\frac{1}{\Gamma(\alpha) \beta^{\alpha}} \int_{0}^{\infty} x^{\alpha} e^{-x / \beta} d x \\
&=\frac{1}{\Gamma(\alpha) \beta^{\alpha}} \Gamma(\alpha+1) \beta^{\alpha+1} \\
&=\frac{\alpha \Gamma(\alpha) \beta}{\Gamma(\alpha)} \\
&=\alpha \beta
\end{aligned}
$$

Moreover, we have

- $ E\left(X^{k}\right)=\beta^{k} \prod_{i=0}^{k-1}(\alpha+i) $
- $ \operatorname{Var}(X)=\alpha \beta^{2} $
- $ \boldsymbol{M}_{X}(t)=(1-\beta t)^{-\alpha}, \quad t<1 / \beta $

If $ X_{1}, X_{2}, \ldots, X_{n} $ are independent and $ X_{i} \sim \operatorname{Gamma}\left(\alpha_{i}, \beta\right) $ for $ i=1,2, \ldots, n, $ then

$$
\sum_{i=1}^{n} X_{i} \sim \text { Gamma }\left(\sum_{i=1}^{n} \alpha_{i}, \beta\right)
$$

If $ X \sim \operatorname{Gamma}(\alpha, \beta) $ and $ c>0, $ then

$$
c X \sim \text { Gamma }(\alpha, c \beta)
$$

Gamma-Poisson Relation, If $ X $ is a Gamma $ (\alpha, \beta) $ random variable, where $ \alpha $ is an integer, then for any $ X $

$$
P(X \leq x)=P(Y \geq \alpha)
$$

If we set $ \alpha=p / 2, $ where $ p $ is an integer, and $ \beta=2, $ then the gamma PDF becomes

$$
f(x \mid p)=\frac{1}{\Gamma(p / 2) 2^{p / 2}} x^{p / 2-1} e^{-x / 2}
$$

which is the **Chi-squared PDF** with p degrees of freedom.

When we set $ \alpha=1, $ we then have

$$
f(x \mid \beta)=\frac{1}{\beta} e^{-x / \beta}, 0<x<\infty
$$

which is an **exponential PDF**. It is also memoryless (lifetimes).

### 4.2.4 Normal Distribution

Importance of the normal distribution: Central Limit Theorem

Under certain conditions, the **sample average** of sufficiently large number of i.i.d. random variables, **will be approximately normally distributed**, regardless of the underlying distribution.

$ X \sim N\left(\mu, \sigma^{2}\right) $, if $ \mathrm{PDF} $

$$
f_{X}(x)=\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}, \quad x \in \mathbb{R}
$$

where $ \mu \in \mathbb{R} $ and $ \sigma^{2}>0 $, and

$$
E(X)=\mu, \quad \operatorname{Var}(X)=\sigma^{2}
$$

Standardize normal distributions:

$$
\begin{array}{l}
X \sim N\left(\mu, \sigma^{2}\right) \Rightarrow \frac{X-\mu}{\sigma} \sim N(0,1) \\
X \sim N(0,1) \Rightarrow \mu+\sigma X \sim N\left(\mu, \sigma^{2}\right)
\end{array}
$$

The MGF of $ Y \sim N(0,1) $ is

$$
\begin{aligned}
M_{Y}(t)&=E\left(e^{t Y}\right)=\int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{t y} e^{-y^{2} / 2} d y \\
&=\int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}\left(y^{2}-2 t y+t^{2}\right)+\frac{1}{2} t^{2}} d y \\
&=e^{t^{2} / 2} \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}(y-t)^{2}} \mathrm{d} y=e^{t^{2} / 2}
\end{aligned}
$$

Then the MGF of $ X=\mu+\sigma Y \sim N\left(\mu, \sigma^{2}\right) $ is

$$
\begin{aligned}
M_{X}(t)&=E\left(e^{t X}\right)=E \left[e^{t(\mu+\sigma Y)}\right]=e^{\mu t} E\left(e^{\sigma t Y}\right) \\
&=e^{\mu t} M_{Y}(\sigma t)=e^{\mu t} e^{\sigma^{2} t^{2} / 2}=e^{\mu t+\sigma^{2} t^{2} / 2}
\end{aligned}
$$

All odd central moments, $ E(X-\mu)^{2 k-1} $ for $ k \in \mathbb{N}, $ are $ 0 . $

Even central moments are given by

$$
E(X-\mu)^{2 k}=\sigma^{2 k}(2 k-1) ! !, \quad k \in \mathbb{N}
$$

Calculate the even central moments
1. Brute-force integration

$$
\int_{-\infty}^{\infty}(x-\mu)^{2 k} \cdot \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}} \mathrm{d} x
$$

2. Differentiate the MGF of $ Y=X-\mu $ and evaluate $ M_{Y}^{(2 k)}(0) $
3. Using Stein's Lemma

**Lemma (Stein's Lemma)**: Suppose $ X \sim N\left(\mu, \sigma^{2}\right), $ and $ g(\cdot) $ is a differentiable function satisfying $ E\left|g^{\prime}(X)\right|<\infty . $ Then

$$
E[g(X)(X-\mu)]=\sigma^{2} E\left[g^{\prime}(X)\right]
$$

Apply Stein's Lemma to calculate $ E(X-\mu)^{4} $ :

Let $ g(X)=(X-\mu)^{3}, $ then

$$
E(X-\mu)^{4}=E\left[(X-\mu)^{3}(X-\mu)\right]=E[g(X)(X-\mu)]
$$

By Stein's Lemma,

$$
E(X-\mu)^{4}=\sigma^{2} E\left[3(X-\mu)^{2}\right]=3 \sigma^{4}
$$

Thus, the kurtosis of normal distribution is $ 3 . $

**Normal Approximation**: Let $ X \sim $Binomial $ (25,0.6) $. We can approximate $ X $ with a normal random variable, $ Y, $ with mean $ \mu=25(0.6)=15 $ and standard deviation $ \sigma=(25(0.6)(0.4))^{1 / 2}=2.45 . $ Thus

$$
P(X \leq 13) \approx P(Y \leq 13)=P\left(Z \leq \frac{13-15}{2.45}\right)=P(Z \leq-0.82)=0.206
$$

The exact binomial calculation gives

$$
P(X \leq 13)=\sum_{x=0}^{13}\left(_{x}^{25}\right)(0.6)^{x}(0.4)^{25-x}=0.267
$$

### 4.2.5 Chi-Square Distribution

$ \chi_{\nu}^{2} $ is the distribution of the sum of $ \nu $ squared independent
standard normal random variables. $ X \sim \chi_{\nu}^{2} $, if $ \mathrm{PDF} $

$$
f_{X}(x)=\left\{\begin{array}{ll}
\frac{1}{2^{\nu / 2} \Gamma(\nu / 2)} \times^{\frac{\nu}{2}-1} e^{-\frac{x}{2}} & \text { if } x>0 \\
0 & \text { if } x \leqslant 0
\end{array}\right.
$$

where $ \nu>0 $ is the degree of freedom.

- $ \chi_{\nu}^{2} $ is Gamma $ (\nu / 2,2) $
- $ E(X)=\nu $
- $ \operatorname{Var}(X)=2 \nu $
- $ M_{X}(t)=(1-2 t)^{-\nu / 2}, \quad t<\frac{1}{2} $

### 4.2.6 The Student's t Distribution

Let $ X \sim N(0,1) $ and $ Y_{n} \sim \chi_{n}^{2}, $ where $ X $ and $ Y_{n} $ are independent. Then the distribution of the random variable

$$
T_{n}=\frac{X}{\sqrt{Y_{n} / n}}
$$

is called **the (Student's) t distribution** with n degrees of freedom and is denoted by $ t_{n} $.

The conditional density $ h_{n}(x \mid y) $ of $ T_{n} $ given $ Y_{n}=y $ is the density of the $ N(1, n / y) ; $ hence, the unconditional density of $ T_{n} $ is

$$
\begin{aligned}
h_{n}(x) &=\frac{\exp \left(-\left(x^{2} / n\right) y / 2\right)}{\sqrt{n / y} \sqrt{2 \pi}} \times \frac{y^{n / 2-1} \exp (-y / 2)}{\Gamma(n / 2) 2^{n / 2}} d y \\
&=\frac{\Gamma((n+1) / 2)}{\sqrt{n \pi} \Gamma(n / 2)\left(1+x^{2} / n\right)^{(n+1) / 2}}
\end{aligned}
$$

- The expectation of $ T_{n} $ does not exist if $ n=1, $ and is zero for $ n \geq 2 $ by symmetry. 
- The variance of $ T_{n} $ is infinite for $ n=2, $ whereas for $ n \geq 3 $ var $ \left(T_{n}\right)=E\left(T_{n}^{2}\right)=\frac{n}{n-2} $

### 4.2.7 The F distribution

Let $ X \sim \chi_{m}^{2} $ and $ Y_{n} \sim \chi_{n}^{2}, $ where $ X_{m} $ and $ Y_{n} $ are independent. Then the distribution of the random variable

$$
F=\frac{X_{m} / m}{Y_{n} / n}
$$

is said to be $ \mathrm{F} $ with $ \mathrm{m} $ and $ \mathrm{n} $ degrees of freedom and is denoted by $ F_{m, n} $.

Its' density is

$$
h_{m, n}(x)=\frac{m^{m / 2} \Gamma(m / 2+n / 2) x^{m / 2-1}}{n^{m / 2} \Gamma(m / 2) \Gamma(n / 2)[1+m x / n]^{m / 2+n / 2}}
$$

Expectation

$$
E(F)=\left\{\begin{array}{ll}
n / n-2, & \text { if } n \geq 3 \\
=\infty, & \text { if } n=1,2
\end{array}\right.
$$
Variance

$$
\operatorname{var}(F)=\left\{\begin{array}{ll}
\frac{2 n^{2}(m+n-4)}{m(n-2)^{2}(n-4)}, & \text { if } n \geq 5 \\
=\infty, & \text { if } n=3,4 \\
=\text { not defined }, & \text { if } n=1,2
\end{array}\right.
$$

### 4.2.8 Log-normal Distribution

$ X \sim $ Log-normal $ \left(\mu, \sigma^{2}\right) $, if $ \mathrm{PDF} $

$$
f_{X}(x)=\left\{\begin{array}{ll}
\frac{1}{\sqrt{2 \pi} \sigma x} e^{-\frac{(\ln x-\mu)^{2}}{2 \sigma^{2}}} & \text { if } x>0 \\
0 & \text { if } x \leqslant 0
\end{array}\right.
$$

If $ Y \sim N\left(\mu, \sigma^{2}\right), $ then $ X=e^{Y} \sim $ Log-normal $ \left(\mu, \sigma^{2}\right) $

Calculate the moments of log-normal distribution using the MGF of normal distribution

$$
E\left(X^{k}\right)=E\left(e^{k Y}\right)=M_{Y}(k)=e^{k \mu+\sigma^{2} k^{2} / 2}
$$

So $ E(X)=e^{\mu+\sigma^{2} / 2} $, and $ \operatorname{Var}(X)=e^{2 \mu+\sigma^{2}}\left(e^{\sigma^{2}}-1\right) $. But MGF does not exist.

**Example**: Assume

$$
X_{t}=X_{t-1}\left(1+Y_{t}\right)
$$

and $ \left\{Y_{t}\right\} $ is a sequence of i.i.d. random variables such that $ Y_{t} $ is independent of $ X_{t-1} $. Then

$$
X_{T}=X_{0} \prod_{t=1}^{T}\left(1+Y_{t}\right) \Rightarrow \ln X_{T}=\ln X_{0}+\sum_{t=1}^{T} \ln \left(1+Y_{t}\right)
$$

By the CLT, for sufficiently large $ T, \sum_{t=1}^{T} \ln \left(1+Y_{t}\right) $ will be approximately normally distributed, after suitable standardization. Thus, $ \prod_{t=1}^{n}\left(1+Y_{t}\right) $ and $ X_{T} $ are approximately log-normally distributed.

### 4.2.9 Cauchy Distribution

$ X \sim \operatorname{Cauchy}(\mu, \sigma) $, if PDF

$$
f_{X}(x)=\frac{1}{\pi \sigma}\left[1+\left(\frac{x-\mu}{\sigma}\right)^{2}\right]^{-1}, \quad x \in \mathbb{R}
$$

where $ \mu $ is the location parameter and $ \sigma>0 $ is the scale parameter.

Cauchy distribution is symmetric about $ \mu $ but **has heavier tails than normal distribution**.

- $ E\left(X^{k}\right) $ does not exist for any $ k \geqslant 1 $
- MGF does not exist.

### 4.2.10 Exponential Distribution

$ X \sim \operatorname{Exp}(\beta) $, if $ \mathrm{PDF} $

$$
f_{X}(x)=\left\{\begin{array}{ll}
\frac{1}{\beta} e^{-x / \beta} & \text { if } x>0 \\
0 & \text { if } x \leqslant 0
\end{array}\right.
$$

CDF
$$
F_X(x) = 1-e^{-x/ \beta}
$$
where $ \beta>0 $ is the scale parameter.

- $ \operatorname{Exp}(\beta) $ is Gamma $ (1, \beta) $
- $ E(X)=\beta $
- $ \operatorname{Var}(X)=\beta^{2} $
- $ \boldsymbol{M}_{X}(t)=\frac{1}{1-\beta t}, \quad t<1 / \beta $

**Exponential distribution is the only continuous distribution with memoryless property**.

In a Poisson process with intensity/rate $ \lambda, $ **the waiting time between consecutive events** follows $ \operatorname{Exp}(1 / \lambda) $ distribution.

Exponential distribution is the only continuous distribution that has a constant hazard rate.

Remark: Hazard rate or hazard function is defined as

$$
\begin{aligned}
\lambda(x) &=\lim _{\Delta x \rightarrow 0^{+}} \frac{P(X \leqslant x+\Delta x \mid X>x)}{\Delta x} \\
&=\lim _{\Delta x \rightarrow 0^{+}} \frac{P(x<X \leqslant x+\Delta x)}{P(X>x) \cdot \Delta x} \\
&=\frac{1}{P(X>x)} \lim _{\Delta x \rightarrow 0^{+}} \frac{\int_{x}^{x+\Delta x} f_{X}(u) \mathrm{d} u}{\Delta x} \\
&=\frac{f_{X}(x)}{P(X>x)}=\frac{f_{X}(x)}{1-F_{X}(x)}
\end{aligned}
$$

### 4.2.11 Double Exponential Distribution

The double exponential distribution is formed by **reflecting the exponential distribution around its mean**, The PDF is given by

$$
f(x \mid \mu, \sigma)=\frac{1}{2 \sigma} e^{-|x-\mu| / \sigma},-\infty<x<\infty,-\infty<\mu<\infty, \sigma>0
$$

Mean and Variance

$$
E X=\mu \text { and } \operatorname{Var}(X)=2 \sigma^{2}
$$

