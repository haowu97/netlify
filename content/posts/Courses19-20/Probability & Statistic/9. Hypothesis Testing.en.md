# 9. Hypothesis Testing

## 9.1 Introduction

Given a realization $x_{1}, \ldots, x_{n}$ of a random sample $X_{1}, \ldots, X_{n}$ from some population distribution $f(x ; \theta),$ we want to know whether the true parameter $\theta$ belongs to some specific subset $\Theta_{0}$ of the parameter space $\Theta$.

【Example 9.1】**Return to Education**: Let $\theta$ measure the change in hourly wage given another year of education, holding all other factors fixed. Labor economists are interested in testing whether the return to education, controlling other factors, is zero. That is, whether or not $ \theta $ equal to zero.

### 9.1.1 Hypothesis

【Definition 9.1】**Hypothesis**: a hypothesis is a statement about the population or population parameter. The two **complementary hypotheses** in a hypothesis testing problem are called the **null hypothesis** and the **alternative hypothesis**, denoted by $H_{0}$ and $H_{1}\left(\text { or } H_{A}\right),$ respectively.

**$H_{0}$ and $ H_1 $**

- A **null hypothesis** is a statement about the population or some attributes of the population. The **alternative hypothesis** is that the statement in the null hypothesis is false.
- The goal of hypothesis testing is to decide, based on an observed data set $\mathbf{x}^{n}$ generated from a population, which of two complementary hypotheses is true.
- Suppose a random sample $\mathrm{X}^{n}$ is generated from a population distribution $f(x, \theta)$ with some unknown value of parameter $\theta \in \Theta$, where $\Theta$ is a known finite-dimensional parameter space. In hypothesis testing, the parameter space $\Theta$ is divided into two mutually exclusive and collectively exhaustive subsets $\Theta_{0}$ and $\Theta_{A},$ namely $\Theta_{0} \cap \Theta_{A}=\varnothing$ and $\Theta_{0} \cup \Theta_{A}=\Theta$.
- The problem is to determine to which of these two subsets the true value of $\theta$ belongs. That is, based upon an observed data set $\mathrm{x}^{n}$, one is trying to choose between the two hypotheses

$$
\mathbb{H}_{0}: \theta \in \Theta_{0}
$$

​		versus

$$
\mathbb{H}_{A}: \theta \in \Theta_{A}
$$

The first hypothesis $\mathbb{H}_{0}$ is called the null hypothesis, and the second, $\mathbb{H}_{A}$, is called the alternative hypothesis.

$\mathbb{H}_{0}$ is called the "**null**" hypothesis because it is often stated as "no effects" or "no relationship". One example, is $\mathbb{H}_{0}: \theta=0$ versus $\mathbb{H}_{A}: \theta \neq 0,$ as is the case of Example 9.1

【Example 9.2】If $\theta$ denotes the proportion of defective items for some manufactured product. We might be interested in testing $\mathbb{H}_{0}: \theta \leq \theta_{0}$ versus $\mathbb{H}_{A}: \theta>\theta_{0},$ where $\theta_{0}$ is the maximum acceptable proportion of defective items. The null hypothesis states that the proportion of defective items is below an unacceptable threshold. This hypothesis testing is the basic idea for **statistical quality control**.

【Example 9.3】**Constant Return to Scale Hypothesis**:  a production function

$$
Y=F(L, K)
$$

tells how much output $Y$ to produce using inputs of labor $L$ and capital $K$. A production technology is said to display a constant return to scale if the output increases by the proportion as the inputs increase; that is, for all $\lambda>0$,

$$
\lambda F(L, K)=F(\lambda L, \lambda K)
$$

Suppose a production function is given by

$$
Y=A K^{\alpha} L^{\beta}
$$

where $Y$ is the output, $K$ and $L$ are the capital and labor inputs, $A$ is a constant, and $\theta=(\alpha, \beta)$ is a parameter vector. Then the constant return to scale hypothesis can be stated as

$$
\mathbb{H}_{0}: \alpha+\beta=1
$$

The alternative hypothesis $\mathbb{H}_{1}: \alpha+\beta \neq 1$ consists of two cases: $\alpha+\beta>1$ and $\alpha+\beta<1$, which imply an increasing return to scale and a decreasing return to scale respectively.

The hypotheses can be divided into two basic categories simple hypotheses and composite hypotheses.

【Definition 9.2】**Simple Hypothesis Versus Composite Hypothesis**: A hypothesis is simple if and only if it contains exactly one population, i.e. **parameter $\theta$ only has one value**. If the hypothesis contains more than one population, it is called a composite hypothesis.

- In Example $9.1, \mathbb{H}_{0}$ contains only one parameter value, so $\mathbb{H}_{0}$ is a simple hypothesis. In contrast, the null hypotheses in Examples 9.2 and 9.3 contain more than one parameter values, so they are composite hypotheses.

### 9.1.2 Hypothesis Testing

【Definition 9.3】**Hypothesis Testing**: A hypothesis testing procedure or a hypothesis test is a decision rule that specifies

1. for what sample values $\mathbf{x}^{n}$ the decision is made to accept $\mathbb{H}_{0}$ as true, and
2. for what sample values $\mathbf{x}^{n}, \mathbb{H}_{0}$ is rejected and $\mathbb{H}_{A}$ is accepted as true.

The key to the construction of the decision rule for a hypothesis testing procedure is to determine the rules of rejection or acceptance of the null hypothesis.

【Definition 9.4】**Critical Region or Rejection Region**: The set $\mathbb{C}$ of the sample points of the random sample $\mathrm{X}^{n}$ for which $\mathbb{H}_{0}$ will be rejected is called the rejection region or critical region. The complement of the rejection region is called the acceptance region.

**Remarks:**

A standard approach to hypothesis testing is to choose a statistic $T\left(\mathbf{X}^{n}\right)$ and use it to divide the sample space of $\mathrm{X}^{n}$ into two mutually exclusive and exhaustive regions

$$
\mathbb{A}_{n}(c)=\left\{\mathbf{x}^{n}: T\left(\mathbf{x}^{n}\right) \leq c\right\}
$$

and

$$
\mathbb{C}_{n}(c)=\left\{\mathbf{x}^{n}: T\left(\mathbf{x}^{n}\right)>c\right\}
$$

for some pre-specified constant $c$.

The first region, $\mathbb{A}_{n}(c),$ is the acceptance region, and the second, $\mathbb{C}_{n}(c),$ is the rejection region. The cutoff point $c$ is called the critical value and $T\left(\mathbf{X}^{n}\right)$ is called a test statistic.

An important issue in hypothesis testing is to determine a suitable value of $c$ given a data set $\mathbf{x}^{n} .$ In general, we need to know the sampling distribution of $T\left(\mathbf{X}^{n}\right)$ under $\mathbb{H}_{0}$ in order to determine the threshold value $c$.

【Example 9.4】Suppose $\mathrm{X}^{n}$ is an IID $ N\left(\mu, \sigma^{2}\right)$ random sample, where $\mu$ is unknown but $\sigma^{2}$ is known. We are interested in testing for $\mathbb{H}_{0}: \mu=\mu_{0}$ versus $\mathbb{H}_{A}: \mu \neq \mu_{0},$ where $\mu_{0}$ is a known number. Here, $\Theta_{0}=\left\{\mu_{0}\right\}$ contains one parameter value $\mu_{0}$ and $\Theta_{A}$ contains all parameter values on the real line $\mathbb{R}$ except $\mu_{0}$.

**Remarks:**

To test the null hypothesis $\mathbb{H}_{0}: \mu=\mu_{0},$ we consider the following test statistic

$$
T\left(\mathbf{X}^{n}\right)=\frac{\bar{X}_{n}-\mu_{0}}{\sigma / \sqrt{n}}
$$

where $\sigma$ is known.

Under the null hypothesis $\mathbb{H}_{0}: \mu=\mu_{0},$ we have
$$
\begin{aligned}
T\left(\mathbf{X}^{n}\right) &=\frac{\bar{X}_{n}-\mu_{0}}{\sigma / \sqrt{n}} \\
& \sim N(0,1)
\end{aligned}
$$


Under $\mathbb{H}_{A}: \mu \neq \mu_{0}$

$$
\begin{aligned}
T\left(\mathbf{X}^{n}\right) = &\frac{\bar{X}_{n}-\mu}{\sigma / \sqrt{n}}+\frac{\mu-\mu_{0}}{\sigma / \sqrt{n}} \\

\sim &N\left(\frac{\sqrt{n}\left(\mu-\mu_{0}\right)}{\sigma}, 1\right)
\end{aligned}
$$

which diverges to infinity with probability approaching one as $n \rightarrow \infty$.

One can accept $\mathbb{H}_{A}$ if $\left|T\left(\mathbf{X}^{n}\right)\right|$ is large. How large $T\left(\mathbf{X}^{n}\right)$ should be in order to be considered as "large" is determined by the sampling distribution $(N(0,1))$ of $T\left(\mathbf{X}^{n}\right)$ under $\mathbb{H}_{0}$.

Specifically, by setting $c=z_{\alpha / 2},$ where $z_{\alpha / 2}$ is the upper-tailed critical value of $N(0,1)$ at level $\frac{\alpha}{2} \in(0,1),$ i.e., $P\left(Z>z_{\alpha / 2}\right)=\frac{\alpha}{2}$ where $Z \sim N(0,1),$ we can define the acceptance and rejection regions as follows:

$$
\begin{array}{l}
\mathrm{A}_{n}(c)=\left\{\mathrm{x}^{n}:\left|\frac{\bar{x}_{n}-\mu_{0}}{\sigma / \sqrt{n}}\right| \leq z_{\frac{\alpha}{2}}\right\} \\
\mathbb{C}_{n}(c)=\left\{\mathrm{x}^{n}:\left|\frac{\bar{x}_{n}-\mu_{0}}{\sigma / \sqrt{n}}\right|>z_{\frac{\alpha}{2}}\right\}
\end{array}
$$

The $N(0,1)$ test decision rule:

1. Accept $\mathbb{H}_{0}: \mu=\mu_{0}$ at the significance level $\alpha$ if

$$
\mathbf{x}^{n} \in \mathbb{A}_{n}(c)
$$

2. Reject $\mathbb{H}_{0}: \mu=\mu_{0}$ at the significance level $\alpha$ if

$$
\mathbf{x}^{n} \in \mathbb{C}_{n}(c)
$$

The significance level $\alpha$ is the probability that the above decision rule will wrongly reject a correct null hypothesis. This error is called the Type I error and is unavoidable given any finite sample size $n$.

### 9.1.3 Evaluation of Hypothesis Testing

【Definition 9.5】**Power of Test** If $\mathbb{C}$ is the rejection region of a test of the null hypothesis $\mathbb{H}_{0}: \theta \in \Theta_{0},$ then the function $\pi(\theta)=P_{\theta}\left(\mathrm{X}^{n} \in \mathbb{C}\right)$ is called the power of the test with the rejection region $\mathbb{C},$ where $P_{\theta}(\cdot)$ is the probability measure when the random sample follows the distribution $f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta\right)$

**The power function $\pi(\theta)$ is the probability of rejecting $\mathbb{H}_{0}$**. 

In Example 9.4, the power of the test statistic $T\left(\mathbf{X}^{n}\right)=\sqrt{n}\left(\bar{X}_{n}-\mu_{0}\right) / \sigma$ is given by

$$
\pi(\mu)=P\left(\left|\frac{\bar{X}_{n}-\mu_{0}}{\sigma / \sqrt{n}}\right|>z_{\alpha / 2}\right)
$$

【Definition 9.6】**Type I and Type II Errors**: If $\mathbb{H}_{0}: \theta \in \Theta_{0}$ holds and the observed data $\mathbf{x}^{n}$ falls into the critical region $\mathbb{C},$ then a Type I error is made. The probability of making Type I error is

$$
\alpha(\theta) \equiv P_{\theta}\left(\mathbf{X}^{n} \in \mathbb{C} | \mathbb{H}_{0}\right)
$$

If $\mathbb{H}_{A}: \theta \in \Theta_{0}^{c}$ holds and the observed data $\mathbf{x}^{n}$ is in the acceptance region, then a Type II error is made. The probability of making Type II error is

$$
\begin{aligned}
\beta(\theta) & \equiv P_{\theta}\left(\mathrm{X}^{n} \in \mathbb{A} | \mathbb{H}_{A}\right) \\
&=1-P_{\theta}\left(\mathrm{X}^{n} \in \mathbb{C} | \mathbb{H}_{A}\right)
\end{aligned}
$$

**Remarks:**

Under $\mathbb{H}_{0}: \theta \in \Theta_{0}$, the power function $\pi(\theta)$ is the probability of making Type I error, namely incorrectly rejecting a correct null hypothesis.

Type I errors are unavoidable because under $\mathbb{H}_{0}$, because a test statistic $T\left(\mathbf{X}^{n}\right)$ may still take large values with nontrivial (though small) probabilities. 

Under $\mathbb{H}_{A}: \theta \in \Theta_{0}^{c}$, the power function $\pi(\theta)$ is the probability of rejecting a false null hypothesis, and it is equal to $1-\beta(\theta)$, where $\beta(\theta)$ is the probability of making Type II error, namely accepting an false null hypothesis. 

A test is called **unbiased** if $P\left[T\left(\mathrm{X}^{n}\right)>c | \mathbb{H}_{A}\right]>P\left[T\left(\mathrm{X}^{n}\right)>c | \mathbb{H}_{0}\right]$. That is, the
probability to reject $\mathbb{H}_{0}$ when $\mathbb{H}_{0}$ is false is strictly larger than when it is true.

**There exists a trade off between Type I errors and Type II errors given any sample size $n$.** For any given $n$, if the critical region $\mathbb{C}$ shrinks, the probability of making Type error deceases, but the probability of making Type II error increases. similarly, if the critical region $\mathbb{C}$ increases, the Type II error $\beta(\theta)$ decreases, but Type I error $\alpha(\theta)$ increases.

Usually, hypothesis tests are evaluated and compared through their probabilities of making mistakes. The classical approach to hypothesis testing is to bound the probability of Type I error by some value $\alpha \in(0,1)$ over all values of $\theta$ in $\Theta_{0}$ and to try to find a test that minimizes the probability of a Type II error over all values of $\theta$ in $\Theta_{A}$.

Specifically, one is trying to find a test statistic $T\left(\mathbf{X}^{n}\right)$ that satisfies 
$$
P\left[T\left(\mathbf{X}^{n}\right)>c | \mathbb{H}_{0}\right] \leq \alpha
$$
 for all $\theta \in \Theta_{0}$ and has the property that 
$$
P\left[T\left(\mathbf{X}^{n}\right)>c | \mathbb{H}_{A}\right] \geq P\left[G\left(\mathbf{X}^{n}\right)>c | \mathbb{H}_{A}\right]
$$
for any other test statistic $G\left(\mathbf{X}^{n}\right)$ for all $\theta \in \Theta_{A}$. 

In other words, **one is trying to find a test statistic $T\left(\mathrm{X}^{n}\right)$ that has the best power while its Type I error is under control**. A test $T\left(\mathbf{X}^{n}\right)$ that has these properties is called uniformly most powerful, where by uniformity it is meant that the test is most powerful for all $\theta \in \Theta_{A}$ We now provide a formal definition.

【Definition 9.7】**Uniformly Most Powerful Test**: Let $\mathbb{T}$ be a class of tests for testing $\mathbb{H}_{0}: \theta \in \Theta_{0}$ versus $\mathbb{H}_{A}: \theta \in \Theta_{A} \cdot$ A test $T\left(X^{n}\right)$ in class $\mathbb{T},$ with power function $\pi(\theta),$ is a uniformly most powerful test over $\mathbb{T}$ if $\pi(\theta) \geq \tilde{\pi}(\theta)$ for all $\theta \in \Theta_{A},$ where $\tilde{\pi}(\theta)$ is the power function of an other test $G\left(\mathbf{X}^{n}\right)$ in class $\mathbb{T}$.

**Remarks:**

If $T\left(\mathbf{X}^{n}\right)$ has level $\alpha$ and $P\left[T\left(\mathbf{X}^{n}\right)>c | \mathbb{H}_{0}\right]=\alpha,$ then the test is called a **size $\alpha$ test**. 

Suppose for a test statistic $T\left(\mathrm{X}^{n}\right), P\left[T\left(\mathrm{X}^{n}\right)>c | \mathbb{H}_{0}\right] \leq \alpha$. Then the value of $\alpha$ gives the maximum Type I error for the test statistic $T\left(\mathrm{X}^{n}\right)$, and is called the **level of the test**.

Obviously, the class of level $\alpha$ tests contains the set of size $\alpha$ tests.

Usually, we will choose $\mathbb{T}$ to be a class of tests with the same level or same size $\alpha$. In complicated testing situations, however, it may be computationally impossible to construct a size $\alpha$ test. In such situations, a researcher must be satisfied with level $\alpha$ test, realizing that some compromises may be made.

### 9.1.4 Gap Between Economic Hypothesis and Statistical Hypothesis

To test a economic hypothesis, we have to transform it into a statistical hypothesis and then test the statistical hypothesis using an observed economic data.

In transforming an economic hypothesis into a statistical hypothesis, some auxiliary assumptions are often imposed. This induces a gap between the original economic hypothesis and the resulting statistical hypothesis.

This gap may cause some problem in economic interpretation of the empirical results testing the statistical hypothesis.

【Example 9.5】**Efficient Market Hypothesis** Suppose $R_{t}$ is the return on some asset portfolio in time period $t,$ and $I_{t-1}=\left(R_{t-1}, R_{t-2}, \cdots\right)$ denotes the historical asset return information available at time $t-1 .$ The asset market is called informationally weakly efficient if

$$
E\left(R_{t} | I_{t-1}\right)=E\left(R_{t}\right)
$$

That is, the historical asset return information has no predictive power for future asset return To test this economic hypothesis, one can consider a linear autoregressive model

$$
R_{t}=\alpha_{0}+\sum_{j=1}^{k} \alpha_{j} R_{t-j}+\varepsilon_{t}
$$

where $\varepsilon_{t}$ is a stochastic disturbance. Under the efficient market hypothesis (EMH), we have

$$
\mathbb{H}_{0}: \alpha_{1}=\alpha_{2}=\cdots=\alpha_{k}=0
$$

If one has evidence that at least one $\alpha_{j}, j \in\{1, \cdots, k\},$ is not zero, it will imply that the efficient market hypothesis doesn't hold.

However, **when one does not reject the statistical hypothesis $\mathbb{H}_{0},$ this does not necessarily imply that the original economic hypothesis-the efficient market hypothesis holds**. The reason is that the linear autoregressive model is just one of many (possibly infinite) ways to test predictability of the historical asset returns for future asset return. In other words, the predictability may arise in a nonlinear manner. Therefore, there exists a gap between the efficient market hypothesis and the statistical hypothesis $\mathbb{H}_{0}$. Because of this gap, when one does not reject $\mathbb{H}_{0}$ one can only say that no evidence against the efficient market hypothesis is found rather than conclude that the efficient market hypothesis holds.

## 9.2 Neyman-Pearson Lemma

【Theorem 9.1】**Neyman-Pearson Lemma**: Consider testing a simple null hypothesis $\mathbb{H}_{0}: \theta=\theta_{0}$ versus a simple alternative hypothesis $\mathbb{H}_{A}: \theta=\theta_{1},$ where the PMF/PDF of the random sample $\mathrm{X}^{n}$ corresponding to $\theta_{i}, i \in\{0,1\},$ is $f_{\mathrm{X}^{n}}\left(\mathrm{x}^{n}, \theta_{i}\right) .$ Suppose a test with rejection and acceptance regions $\mathbb{C}_{n}(c)$ and $\mathbb{A}_{n}(c)$ respectively is defined as follows:

(a)​
$$
\mathbb{C}_{n}(c)=\left\{\mathbf{x}^{n}: \frac{f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{1}\right)}{f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{0}\right)}>c\right\}
$$

and

$$
\mathbb{A}_{n}(c)=\left\{\mathbf{x}^{n}: \frac{f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{1}\right)}{f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{0}\right)} \leq c\right\}
$$

for some constant $c \geq 0,$ and

(b)​
$$
P\left[\mathrm{X}^{n} \in \mathbb{C}_{n}(c) | \mathbb{H}_{0}\right]=\alpha
$$

Then

1. **[Sufficiency]** Any test that satisfies conditions (a) and (b) is a uniformly most powerful level $\alpha$ test.
2. **[Necessity]** If there exists a test satisfying conditions (a) and (b) with $c>0$, then every uniformly most powerful level $\alpha$ test is a size $\alpha$ test ( i.e., satisfying condition (b) ), and every uniformly most powerful level $\alpha$ test satisfies condition (a) except perhaps on a set $A$ in the sample space of $\mathbf{X}^{n}$ satisfying $P\left(\mathbf{X}^{n} \in A | \mathbb{H}_{0}\right)=P\left(\mathbf{X}^{n} \in A | \mathbb{H}_{A}\right)=0$.

**Proof**:

Note that

$$
\begin{aligned}
P\left[\mathbf{X}^{n}\in \mathbb{C}_{n}(c) | \mathbb{H}_{0}\right] &=E\left\{1\left[\mathbf{X}^{n} \in \mathbb{C}_{n}(c)\right] | \mathbb{H}_{0}\right\} \\
&=\int 1\left[\mathbf{x}^{n} \in \mathbb{C}_{n}(c)\right] f\left(\mathbf{x}^{n}, \theta_{0}\right) d \mathbf{x}^{n}
\end{aligned}
$$

where $1(\cdot)$ is the indicator function.

**[Sufficiency]** We first show that a test (denoted as $T\left(\mathbf{X}^{n}\right)$ ) that satisfies conditions (a) and (b) is uniformly most powerful. Suppose there is another test (denoted $\left.T_{1}\left(\mathrm{X}^{n}\right)\right)$ with $E\left\{1\left[\mathrm{X}^{n} \in \mathbb{C}_{1 n}\right] | \mathbb{H}_{0}\right\} \leq \alpha$. (The test $T_{1}\left(\mathrm{X}^{n}\right)$ need not be a likelihood ratio test.) We shall show that $T_{1}\left(\mathrm{X}^{n}\right)$ is not more powerful than $T\left(\mathbf{X}^{n}\right)$.

Observe that if $1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}(c)\right]>1\left[\mathrm{x}^{n} \in \mathbb{C}_{1 n}\right]$, i.e. $\mathrm{x}^{n} \in \mathbb{C}_{n}(c)$ and $\mathrm{x}^{n} \notin \mathbb{C}_{1 n}$, then the sample point $\mathrm{x}^{n}$ is in the critical region $\mathbb{C}_{n}(c)$ of the test $T\left(\mathbf{X}^{n}\right),$ and so $f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{1}\right)>c f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{0}\right) ;$ 

on the other hand, if $1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}(c)\right]<1\left[\mathrm{x}^{n} \in \mathbb{C}_{1 n}\right]$, i.e. $\mathrm{x}^{n} \notin \mathbb{C}_{n}(c)$ and $ \mathrm{x}^{n} \in \mathbb{C}_{1 n} $, then the sample point $\mathrm{x}^{n}$ is in the acceptance region $\mathbb{A}_{n}(c)$ of the test $T\left(\mathbf{X}^{n}\right),$ and so $f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{1}\right) \leq c f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{0}\right) .$ In either case, we have:
$$
\left\{1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}(c)\right]-1\left[\mathrm{x}^{n} \in \mathbb{C}_{1 n}\right]\right\}\left[f_{\mathrm{X}^{n}}\left(\mathrm{x}^{n}, \theta_{1}\right)-c f_{\mathrm{X}^{n}}\left(\mathrm{x}^{n}, \theta_{0}\right)\right] \geq 0
$$

Thus,

$$
\int_{\mathbb{R}^{n}}\left\{1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}(c)\right]-1\left[\mathrm{x}^{n} \in \mathbb{C}_{1 n}\right]\right\}\left[f_{\mathrm{X}^{n}}\left(\mathrm{x}^{n}, \theta_{1}\right)-c f_{\mathrm{X}^{n}}\left(\mathrm{x}^{n}, \theta_{0}\right)\right] d \mathrm{x}^{n} \geq 0
$$
This implies

$$
\begin{aligned}
& \int_{\mathbb{R}^{n}}\left\{1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}(c)\right]-1\left[\mathrm{x}^{n} \in \mathbb{C}_{1 n}\right]\right\} f_{\mathrm{X}^{n}}\left(\mathrm{x}^{n}, \theta_{1}\right) d \mathrm{x}^{n} \\
\geq & c \int_{\mathbb{R}^{n}}\left\{1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}(c)\right]-1\left[\mathrm{x}^{n} \in \mathbb{C}_{1 n}\right]\right\} f_{\mathrm{X}^{n}}\left(\mathrm{x}^{n}, \theta_{0}\right) d \mathrm{x}^{n}
\end{aligned}
$$

Because (test level/size)
$$
\begin{aligned}
\int_{\mathbb{R}^{n}} 1\left[\mathrm{x}^{n} \in \mathbb{C}_{1 n}\right] f_{\mathrm{X}^{n}}\left(\mathrm{x}^{n}, \theta_{0}\right) d \mathrm{x}^{n} \leq \alpha,\\
 \int_{\mathbb{R}^{n}} 1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}(c)\right] f_{\mathrm{X}^{n}}\left(\mathrm{x}^{n}, \theta_{0}\right) \mathrm{x}^{n}=\alpha
 \end{aligned}
$$
and $c \geq 0$, we have
$$
c \int_{\mathbb{R}^{n}}\left\{1\left[\mathbf{x}^{n} \in \mathbb{C}_{n}(c)\right]-1\left[\mathbf{x}^{n} \in \mathbb{C}_{1 n}\right]\right\} f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{0}\right) d \mathbf{x}^{n} \geq 0
$$

It follows that

$$
\int_{\mathbb{R}^{n}}\left\{1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}(c)\right]-1\left[\mathrm{x}^{n} \in \mathbb{C}_{1 n}\right]\right\} f_{\mathrm{X}^{n}}\left(\mathrm{x}^{n}, \theta_{1}\right) d \mathrm{x}^{n} \geq 0
$$

which implies
$$
P\left[\mathbf{X}^{n} \in \mathbb{C}_{n}(c) | \mathbb{H}_{A}\right] \geq P\left[\mathbf{X}^{n} \in \mathbb{C}_{1 n} | \mathbb{H}_{A}\right]
$$
That is, the test $T_{1}\left(\mathbf{X}^{n}\right)$ is not more powerful than $T\left(\mathbf{X}^{n}\right)$.

**[Necessity]** Assume that $T\left(\mathbf{X}^{n}\right)$ is a test that satisfies Conditions (a) and (b) with $c>0$.

(i) We first show that a uniformly most powerful level $\alpha$ test (denoted as $T_{2}\left(\mathbf{X}^{n}\right)$ ) is a size $\alpha$ test (i.e., satisfies Condition (b) ). 

Suppose it is not a size $\alpha$ test. Then $\int_{\mathbb{R}^{n}} 1\left[\mathrm{x}^{n} \in \mathbb{C}_{2 n}\right] f_{\mathrm{X}^{n}}\left(\mathrm{x}^{n}, \theta_{0}\right) d \mathrm{x}^{n}<\alpha$. Given
Condition (b) for the test $T\left(\mathbf{X}^{n}\right),$ we have $\int_{\mathbb{R}^{n}} \mathbf{1}\left[\mathbf{x}^{n} \in \mathbb{C}_{n}(c)\right] f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{0}\right) d \mathbf{x}^{n}=\alpha .$ It follows that
$$
\int_{\mathbb{R}^{n}}\left\{1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}(c)\right]-1\left[\mathrm{x}^{n} \in \mathbb{C}_{2 n}\right]\right\} f_{\mathrm{X}^{n}}\left(\mathrm{x}^{n}, \theta_{0}\right) d \mathrm{x}^{n}>0
$$

Observe that, if $1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}(c)\right]-1\left[\mathrm{x}^{n} \in \mathbb{C}_{2 n}\right]>0,$ then $\mathrm{x}^{n}$ is in the rejection region $\mathbb{C}_{n}(c)$ of the test $T\left(\mathbf{X}^{n}\right),$ and so $f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{1}\right)>c f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{0}\right) ;$ if $1\left[\mathbf{x}^{n} \in \mathbb{C}_{n}(c)\right]-1\left[\mathbf{x}^{n} \in \mathbb{C}_{2 n}\right]<0$, 
then $\mathbf{x}^{n}$ is in the acceptance region $\mathbb{A}_{n}(c)$ of the test $T\left(\mathbf{X}^{n}\right),$ and so $f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{1}\right) \leq c f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{0}\right) .$ It follows that
$$
\int_{\mathbb{R}^{n}}\left\{1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}(c)\right]-1\left[\mathrm{x}^{n} \in \mathbb{C}_{2 n}\right]\right\}\left[f_{\mathrm{X}^{n}}\left(\mathrm{x}^{n}, \theta_{1}\right)-c f_{\mathrm{X}^{n}}\left(\mathrm{x}^{n}, \theta_{0}\right)\right] d \mathrm{x}^{n} \geq 0
$$

Therefore,

$$
\begin{array}{l}
\quad \int_{\mathbb{R}^{n}}\left\{1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}(c)\right]-1\left[\mathrm{x}^{n} \in \mathbb{C}_{2 n}\right]\right\} f_{\mathrm{X}^{n}}\left(\mathrm{x}^{n}, \theta_{1}\right) d \mathrm{x}^{n} \\
\geq c \int_{\mathbb{R}^{n}}\left\{1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}(c)\right]-1\left[\mathrm{x}^{n} \in \mathbb{C}_{2 n}\right]\right\} f_{\mathrm{X}^{n}}\left(\mathrm{x}^{n}, \theta_{0}\right) d \mathrm{x}^{n} \\
>0
\end{array}
$$

given $c>0$. This implies 
$$
P\left[\mathrm{X}^{n} \in \mathbb{C}_{n}(c) | \mathbb{H}_{A}\right]>P\left[\mathrm{X}^{n} \in \mathbb{C}_{2 n} | \mathbb{H}_{A}\right],
$$
suggesting that the test $T_{2}\left(\mathbf{X}^{n}\right)$ is not uniformly most powerful, a contradiction. Thus, a uniformly most powerful level $\alpha$ test is a size $\alpha$ test

(ii) Next, we show that every uniformly most powerful level $\alpha$ test must satisfy Conditions (a) and (b) except perhaps on a zero probability set $A$ in the sample space of $\mathrm{X}^{n}$ satisfying $P\left(\mathrm{X}^{n} \in A | \mathbb{H}_{0}\right)=P\left(\mathrm{X}^{n} \in A | \mathbb{H}_{A}\right)=0 .$ 

Suppose $T^{*}\left(\mathrm{X}^{n}\right)$ is a most powerful level $\alpha$ test with the rejection region $\mathbb{C}_{n}^{*}$. Because every most powerful test with level $\alpha$ is a size $\alpha$ test, we have
$$
\int_{\mathbb{R}^{n}} 1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}^{*}\right] f_{\mathrm{X}^{n}}\left(\mathrm{x}^{n}, \theta_{0}\right) d \mathrm{x}^{n}=\alpha=\int_{\mathbb{R}^{n}} 1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}(c)\right] f_{\mathrm{X}^{n}}\left(\mathrm{x}^{n}, \theta_{0}\right) d \mathrm{x}^{n}
$$

Also, both $T\left(\mathbf{X}^{n}\right)$ and $T^{*}\left(\mathbf{X}^{n}\right)$ are most powerful tests, so they are equally powerful under $\mathbb{H}_{A},$ namely

$$
\int_{\mathbb{R}^{n}}\left\{1\left[\mathbf{x}^{n} \in \mathbb{C}_{n}(c)\right]-1\left[\mathbf{x}^{n} \in \mathbb{C}_{n}^{*}\right]\right\} f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{1}\right) d \mathbf{x}^{n}=0
$$

Thus

$$
\int_{\mathbb{R}^{n}}\left\{1\left[\mathbf{x}^{n} \in \mathbb{C}_{n}(c)\right]-1\left[\mathbf{x}^{n} \in \mathbb{C}_{n}^{*}\right]\right\}\left[f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{1}\right)-c f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{0}\right)\right] d \mathbf{x}^{n}=0
$$

Now, observe that if $1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}(c)\right]-1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}^{*}\right]>0,$ then $\mathrm{x}^{n}$ must be in the rejection region $\mathbb{C}_{n}(c)$ of the test $T\left(\mathbf{X}^{n}\right),$ and so $f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{1}\right)-c f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{0}\right)>0 ;$ if $\mathbf{1}\left[\mathbf{x}^{n} \in \mathbb{C}_{n}(c)\right]-\mathbf{1}\left[\mathbf{x}^{n} \in \mathbb{C}_{n}^{*}\right]<0,$ then $\mathbf{x}^{n}$ must be in the acceptance region of the test $T\left(\mathbf{X}^{n}\right),$ and so $f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{1}\right)-c f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{0}\right) \leq 0 .$ In both cases, the product
$\left\{1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}(c)\right]-1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}^{*}\right]\right\}\left[f_{\mathrm{X}^{n}}\left(\mathrm{x}^{n}, \theta_{1}\right)-c f_{\mathrm{X}^{n}}\left(\mathrm{x}^{n}, \theta_{0}\right)\right] \geq 0 $.

Given that the integral of this nonnegative product is $0,$ we have that $1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}(c)\right]-1\left[\mathrm{x}^{n} \in \mathbb{C}_{n}^{*}\right]=0$ for all $\mathrm{x}^{n}$ in the sample space of $\mathrm{X}^{n}$ except for a set with zero probability. Therefore, the sets $\mathbb{C}_{n}^{*}$ and $\mathbb{C}_{n}(c)$ are identical except on the set of probability 0.

**Remarks:**

The Neyman-Pearson lemma provides a method of finding a uniformly most powerful test when both the null and the alternative are simple hypotheses.

The likelihood ratio test is the uniformly most powerful test.

However, when a hypothesis is composite, i.e., the hypothesis contains more than one parameter value, the lemma may not hold. See Hong and Lee (2013) for an example.

【Corollary 9.2】**Likelihood Ratio Test and Sufficient Statistics**: Suppose $T\left(\mathrm{X}^{n}\right)$ is a sufficient statistic for $\theta$ and $g\left(t, \theta_{i}\right)$ is the PMF/PDF of $T\left(\mathbf{X}^{n}\right)$ corresponding to $\theta_{i}, i \in\{0,1\}$. Then any test based on $T\left(\mathbf{X}^{n}\right)$ with rejection region $\mathbb{C}_{n}(c)$ is a uniformly most powerful level $\alpha$ test for $\mathbb{H}_{0}: \theta=\theta_{0}$ against $\mathbb{H}_{A}: \theta=\theta_{1}$ if the test has the rejection and acceptance regions

$$
\mathbb{C}_{n}(c)=\left\{t: \frac{g\left(t, \theta_{1}\right)}{g\left(t, \theta_{0}\right)}>c\right\}
$$

and

$$
\mathbb{A}_{n}(c)=\left\{t: \frac{g\left(t, \theta_{1}\right)}{g\left(t, \theta_{0}\right)} \leq c\right\}
$$

for some $c \geq 0,$ where $P\left[T\left(\mathbf{X}^{n}\right) \in \mathbb{C}_{n}(c) | \mathbb{H}_{0}\right]=\alpha$.

**Proof:**

Suppose the test $T^*(\mathbf{X}^n)$ with rejection region
$$
\mathbb{C}^*_{n}(c)=\left\{\mathbf{x}^{n}: \frac{f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{1}\right)}{f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{0}\right)}>c\right\}
$$
As $T\left(\mathbf{X}^{n}\right)$ is is a sufficient statistic for $\theta$ and $g\left(t, \theta_{i}\right)$ is the PMF/PDF of $T\left(\mathbf{X}^{n}\right)$ 
$$
\begin{aligned}
f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_i\right) &=f_{T\left(\mathbf{X}^{n}\right)}\left[T\left(\mathbf{x}^{n}\right), \theta_i\right] f_{\mathbf{X}^{n} | T\left(\mathbf{X}^{n}\right)}\left[\mathbf{x}^{n} | T\left(\mathbf{x}^{n}\right)\right] \\
&=f_{T\left(\mathbf{X}^{n}\right)}\left[T\left(\mathbf{x}^{n}\right), \theta_i\right] h\left(\mathbf{x}^{n}\right)\\
&=g(t,\theta_i)\cdot h(\mathbf{x}^{n})
\end{aligned}
$$
where the conditional distribution $f_{\mathbf{X}^{n} | T\left(\mathbf{X}^{n}\right)}\left[\mathbf{x}^{n} | T\left(\mathbf{x}^{n}\right)\right]$ of $\mathbf{X}^{n}=\mathbf{x}^{n}$ given $T\left(\mathbf{X}^{n}\right)=T\left(\mathbf{x}^{n}\right)$ does not depend on $\theta$ and is denoted as function $h\left(\mathbf{x}^{n}\right)$.

Then, we have 
$$
\begin{aligned}
&\frac{f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{1}\right)}{f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{0}\right)}>c \\
\iff &\frac{g(t,\theta_1) \cdot h(\mathbf{X}^{n})}{g(t,\theta_0)\cdot h(\mathbf{X}^{n})} >c \\
\iff &\frac{g(t,\theta_1)}{g(t,\theta_0)} >c 
\end{aligned}
$$
Namely,
$$
\mathbf{X}^n \in \mathbb{C}^*_n(c) \iff T(\mathbf{X}^n) \in \mathbb{C}_n(c)
$$
It follows that test based on $T\left(\mathbf{X}^{n}\right)$ with rejection region $\mathbb{C}_{n}(c)$ is equal to $T^*(\mathbf{X}^n)$.

As $P\left[T\left(\mathbf{X}^{n}\right) \in \mathbb{C}_{n}(c) | \mathbb{H}_{0}\right]=\alpha$, then
$$
P\left[\mathbf{X}^{n}\in \mathbb{C}^*_{n}(c) | \mathbb{H}_{0}\right]=\alpha
$$
Therefore the test $T^*(\mathbf{X}^n)$ satisfies Neyman-Pearson lemma. Test based on $T\left(\mathbf{X}^{n}\right)$ with rejection region $\mathbb{C}_{n}(c)$, equal to $T^*(\mathbf{X}^n)$, is a uniformly most powerful level $\alpha$ test.

Thus, the likelihood ratio test based on the random sample $X^{n}$ can be reduced to a likelihood ratio test based on the sufficient statistic $T\left(\mathrm{X}^{n}\right)$ of $\theta$ which remains to be the uniformly most powerful test.

【Example 9.6】Suppose $X^{n}$ is an IID random sample from an Exponential $(\theta)$ distribution $f(x, \theta)=\frac{1}{\theta} e^{-x / \theta}$ for $x \geq 0 .$ Find a uniformly most powerful level $\alpha$ test for $\mathbb{H}_{0}: \theta=1$ versus $\mathbb{H}_{A}: \theta=2$.

**Solution:** 

The likelihood function of the random sample $\mathrm{X}^{n}$ is
$$
\begin{aligned}
f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta\right) &=\prod_{i=1}^{n} f\left(x_{i}, \theta\right) \\
&=\frac{1}{\theta^{n}} e^{-n \bar{x}_{n} / \theta}
\end{aligned}
$$

where $\bar{x}_{n}=n^{-1} \sum_{i=1}^{n} x_{i}$. By the factorization theorem in Theorem $6.14, \bar{X}_{n}$ is a sufficient statistic for $\theta$. Since the sum of IID exponential $(\theta)$ random variables follows a Gamma $(n, \theta)$ distribution (see Example 5.34), and so the sample mean $\bar{X}_{n} \sim$ Gamma $\left(n, \frac{\theta}{n}\right) .$ Thus, its PDF

$$
g\left(\bar{x}_{n}, \theta\right)=\frac{n^{n}}{(n-1) ! \theta^{n}} \bar{x}_{n}^{n-1} e^{-n \bar{x}_{n} / \theta}, \text { for } \bar{x}_{n}>0
$$
It follows that the likelihood ratio

$$
\begin{aligned}
\frac{f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{1}\right)}{f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta_{0}\right)} &=\frac{g\left(\bar{x}_{n}, \theta_{1}\right)}{g\left(\bar{x}_{n}, \theta_{0}\right)} \\
&=\frac{\frac{1}{2^{n}} e^{-\frac{n}{2} \bar{x}_{n}}}{e^{-n \bar{x}_{n}}} \\
&=\frac{1}{2^{n}} e^{\frac{n}{2} \bar{x}_{n}}
\end{aligned}
$$

Define a test with the following one-dimensional rejection region

$$
\bar{x}_{n} \in \mathbb{C}_{n}(c) \text { if } \frac{1}{2^{n}} e^{\frac{n}{2} \bar{x}_{n}}>c
$$

or equivalently,

$$
\bar{x}_{n} \in \mathbb{C}_{n}(c) \text { if } \bar{x}_{n}>2 \ln 2+\frac{2 \ln c}{n}
$$

Then we determine the value of $c$ so that the test has size $\alpha$. This requires

$$
\begin{aligned}
\alpha &=\int_{2 \ln 2+2 n-1 \ln c}^{\infty} g\left(\bar{x}_{n}, \theta_{0}\right) d \bar{x}_{n} \\
&=\int_{2 \ln 2+2 n^{-1} \ln c}^{\infty} \frac{n^{n}}{(n-1) !} \bar{x}_{n}^{n-1} e^{-n \bar{x}_{n}} d \bar{x}_{n}
\end{aligned}
$$

Solving this nonlinear equation, we can obtain $c=c(\alpha, n)$ as a function of $\alpha$ and $n$ but it has no closed form solution. By the Neyman-Pearson lemma and Corollary 9.2, the above test is the uniformly most powerful size $\alpha$ test.

## 9.3 Wald Test

Consider the hypotheses of interest
$$
\mathbb{H}_{0}: g(\theta)=0
$$
versus
$$
\mathbb{H}_{A}: g(\theta) \neq 0
$$
where $ g: \mathbb{R}^{p} \rightarrow \mathbb{R}^{q} $ is a continuously differentiable $ q $-dimensional vector-valued function of a $ p $-dimensional parameter vector $ \theta $

The integer $ q $ is the number of restrictions on parameter vector $ \theta $. We assume $ q \leq p $.

One important example is a linear vector-valued function
$$
g(\theta)=R \theta-r
$$
where $ R $ is a $ q \times p $ known constant matrix, $ r $ is a $ q \times 1 $ known constant vector. The null hypothesis
$$
\mathbb{H}_{0}: R \theta=r
$$
imposes $ q $ linear restrictions on the $ p $-dimensional parameter vector $ \theta $.

### 9.3.1 Regularity Conditions

【Assumption W.1】**Asymptotic Normality**: $ \sqrt{n}\left(\hat{\theta}-\theta_{0}\right) \stackrel{d}{\rightarrow} N(0, V)$, where $ V $ is a $ p \times p $ symmetric bounded and nonsingular matrix, $ \theta_{0} $ is the true parameter value which is an interior point in $ \Theta, $ and $ \Theta $ is a compact parameter space.

Assumption W.1 allows the estimator $ \hat{\theta} $ to be any root-$ n $ consistent asymptotically normal estimator. Both MLE and GMM estimator satisfy Assumption W.1.

【Assumption W.2】**Consistent Variance Estimator**: $ \hat{V} \stackrel{p}{\rightarrow} V $ as $ n \rightarrow \infty $.

Assumption W.2 assumes that there exists a consistent estimator $ \hat{ V} $ for the asymptotic variance $ V $ of $ \sqrt{n}\left(\hat{\theta}-\theta_{0}\right) . $ 

【Assumption W.3】**Smooth Condition on Restriction Function**: $ g: \mathbb{R}^{p} \rightarrow \mathbb{R}^{q} $ is a continuously differentiable function of $ \theta \in \Theta, $ and the $ q \times p $ matrix $ G\left(\theta_{0}\right)=\frac{\partial}{\partial \theta} g\left(\theta_{0}\right) $ has rank $ q $ (i.e., full rank), where $ q \leq p $.

Let $ g(\theta)=[g_1(\theta), g_2(\theta), \ldots, g_q(\theta)]' $, then the gradient of $g(\theta) $ is
$$
G(\theta)=\left[\begin{array}{cc}
 \frac{\partial g_{1}(\theta)}{\partial \theta_{1}} & \frac{\partial g_{1}(\theta)}{\partial \theta_{2}} &\cdots &\frac{\partial g_{1}(\theta)}{\partial \theta_{p}} \\ 
 \frac{\partial g_{2}(\theta)}{\partial \theta_{1}} & \frac{\partial g_{2}(\theta)}{\partial \theta_{2}} &\cdots &\frac{\partial g_{2}(\theta)}{\partial \theta_{p}} \\
 \cdots & \cdots & \cdots & \cdots \\
 \frac{\partial g_{q}(\theta)}{\partial \theta_{1}} & \frac{\partial g_{q}(\theta)}{\partial \theta_{2}} &\cdots &\frac{\partial g_{q}(\theta)}{\partial \theta_{p}}
 \end{array}\right]
$$
Assumption W.3 is a regularity condition on the restriction function $ g(\cdot) . $ The full rank condition for the $ q \times p $ matrix $ G\left(\theta_{0}\right) $ and $ q \leq p $ ensure that the $ q \times q $ symmetric matrix $ G\left(\theta_{0}\right) V G\left(\theta_{0}\right)^{\prime} $ is nonsingular.

### 9.3.2 Wald Test

【Theorem 9.3】**Wald Test**: Suppose Assumptions W.1-W.3 and $ \mathbb{H}_{0} $ hold. Then as $ n \rightarrow \infty $
$$
W=n \cdot g(\hat{\theta})^{\prime}\left[G(\hat{\theta}) \hat{V} G(\hat{\theta})^{\prime}\right]^{-1} g(\hat{\theta}) \stackrel{d}{\rightarrow} \chi_{q}^{2}
$$
**Proof:**

To test $ \mathbb{H}_{0}: g\left(\theta_{0}\right)=0, $ a natural approach is to base a test on statistic $ g(\hat{\theta}), $ where $ \hat{\theta} $ is a consistent estimator of $ \theta_{0} . $ Because $ g(\cdot) $ is continuous, we always have $ g(\hat{\theta}) \stackrel{p}{\rightarrow} g\left(\theta_{0}\right) $ whenever $ \hat{\theta} \stackrel{p}{\rightarrow} \theta_{0} $ as $ n \rightarrow \infty . $ It follows that $ g(\hat{\theta}) $ will be close to zero under $ \mathbb{H}_{0} $ and will converge to a nonzero limit under $ \mathbb{H}_{A} $. Thus, we can test $ \mathbb{H}_{0} $ by checking whether $ g(\hat{\theta}) $ is close to zero. 

How large the value of $ g(\hat{\theta}) $ should be in order to be considered as significantly different from zero will be determined by the sampling distribution of $ g(\hat{\theta}) $ under $ \mathbb{H}_{0} $.

By the mean value theorem/中值定理, we have
$$
g(\hat{\theta})=g\left(\theta_{0}\right)+G(\bar{\theta})\left(\hat{\theta}-\theta_{0}\right)
$$

where $ \bar{\theta}=\lambda \hat{\theta}+(1-\lambda) \theta_{0} $ for some $ \lambda \in[0,1], $ and the gradient function
$$
G(\theta)=\frac{d g(\theta)}{d \theta}
$$
is a $ q \times p $ matrix. 

Given that $ \left\|\bar{\theta}-\theta_{0}\right\|=\left\|\lambda\left(\hat{\theta}-\theta_{0}\right)\right\| \leq\left\|\hat{\theta}-\theta_{0}\right\| \stackrel{p}{\rightarrow} 0 $ as $ n \rightarrow \infty $ and the continuity of $ G(\cdot), $ we have $ G(\bar{\theta}) \stackrel{p}{\rightarrow} G\left(\theta_{0}\right) $ as $ n \rightarrow \infty . $ 

Then by the asymptotic normality that $ \sqrt{n}\left(\hat{\theta}-\theta_{0}\right) \stackrel{d}{\rightarrow} N(0, V), $ and the Slutsky’s theorem, we have
$$
\begin{aligned}
\sqrt{n}\left[g(\hat{\theta})-g\left(\theta_{0}\right)\right]&=G(\bar{\theta}) \sqrt{n} \left(\hat{\theta}-\theta_{0}\right)\\ 
&\stackrel{d}{\rightarrow} N\left(0, G\left(\theta_{0}\right) V G\left(\theta_{0}\right)^{\prime}\right)

\end{aligned}
$$
Under $ \mathbb{H}_{0}: g\left(\theta_{0}\right)=0, $ we have
$$
\begin{aligned}
\sqrt{n} g(\hat{\theta}) &\stackrel{d}{\rightarrow} N\left(0, G\left(\theta_{0}\right) V G\left(\theta_{0}\right)^{\prime}\right)\\
&=O_p(1)
\end{aligned}
$$
since the $ q \times q $ matrix $ G\left(\theta_{0}\right) V G\left(\theta_{0}\right)^{\prime} $ is nonsingular given the full rank condition on $ G\left(\theta_{0}\right) $ and the nonsingularity condition on $ V, $ the quadratic form
$$
\sqrt{n} g(\hat{\theta})^{\prime}\left[G\left(\theta_{0}\right) V G\left(\theta_{0}\right)^{\prime}\right]^{-1} \sqrt{n} g(\hat{\theta}) 
\stackrel{d}{\rightarrow} \chi_{q}^{2}
$$

Because $ G(\hat{\theta}) \stackrel{p}{\rightarrow} G\left(\theta_{0}\right) $ as $ n \rightarrow \infty $ by continuity of $ G(\cdot) $ and $ \hat{\theta} \stackrel{p}{\rightarrow} \theta_{0}, $ and $ \hat{V} \stackrel{p}{\rightarrow} V $ by Assumption W.2, we have
$$
G(\hat{\theta}) \hat{V} G(\hat{\theta})^{\prime} \stackrel{p}{\rightarrow} G\left(\theta_{0}\right) V G\left(\theta_{0}\right)^{\prime}
$$
Namely, $G(\hat{\theta}) \hat{V} G(\hat{\theta})^{\prime} -G\left(\theta_{0}\right) V G\left(\theta_{0}\right)^{\prime}=o_p(1)$, with $\sqrt{n} g(\hat{\theta})=O_p(1)$, also the stochastic matrix $ G(\hat{\theta}) \hat{V} G(\hat{\theta})^{\prime} $ is nonsingular for $ n $ sufficiently large. We have
$$
\begin{aligned}
&\sqrt{n} g(\hat{\theta})^{\prime}\left[G(\hat{\theta}) \hat{V} G(\hat{\theta})^{\prime}\right]^{-1} \sqrt{n} g(\hat{\theta}) -\sqrt{n} g(\hat{\theta})^{\prime}\left[G\left(\theta_{0}\right) V G\left(\theta_{0}\right)^{\prime}\right]^{-1} \sqrt{n} g(\hat{\theta}) \\
&\sqrt{n} g(\hat{\theta})^{\prime}\left\{ \left[G(\hat{\theta}) \hat{V} G(\hat{\theta})^{\prime}\right]^{-1} -\left[G\left(\theta_{0}\right) V G\left(\theta_{0}\right)^{\prime}\right]^{-1} \right\} \sqrt{n} g(\hat{\theta}) \\
=& O_p(1) \times o_p(1) \times O_p(1)\\
=& o_p(1)
\end{aligned}
$$
i.e., 
$$
\sqrt{n} g(\hat{\theta})^{\prime}\left[G(\hat{\theta}) \hat{V} G(\hat{\theta})^{\prime}\right]^{-1} \sqrt{n} g(\hat{\theta}) \stackrel{p}{\rightarrow} \sqrt{n} g(\hat{\theta})^{\prime}\left[G\left(\theta_{0}\right) V G\left(\theta_{0}\right)^{\prime}\right]^{-1} \sqrt{n} g(\hat{\theta})
$$
With $\sqrt{n} g(\hat{\theta})^{\prime}\left[G\left(\theta_{0}\right) V G\left(\theta_{0}\right)^{\prime}\right]^{-1} \sqrt{n} g(\hat{\theta}) 
\stackrel{d}{\rightarrow} \chi_{q}^{2}$, by Lemma 7.6 (Asymptotic Equivalence), the Wald test statistic
$$
\begin{aligned}
W=& n \cdot g(\hat{\theta})^{\prime}\left[G(\hat{\theta}) \hat{V} G(\hat{\theta})^{\prime}\right]^{-1} g(\hat{\theta}) \\
& \stackrel{d}{\rightarrow} \chi_{q}^{2}
\end{aligned}
$$
under $ \mathbb{H}_{0} $.

**Remarks:**

$ \hat{W} $ is a quadratic form in the difference $ \sqrt{n} g(\hat{\theta}) $ and $ \sqrt{n} g\left(\theta_{0}\right)=0, $ weighted by the asymptotic variance estimator $ G(\hat{\theta}) \hat{V} G(\hat{\theta})^{\prime} $ of $ \sqrt{n}\left[g(\hat{\theta})-g\left(\theta_{0}\right)\right] $.

The Wald test is an asymptotically size $ \alpha $ test that rejects $ \mathbb{H}_{0}: g\left(\theta_{0}\right)=0 $ when $ \hat{W} $ exceeds the $ (1-\alpha) $ th quantile of the $ \chi_{q}^{2} $ distribution.

Under $ \mathbb{H}_{A}: g\left(\theta_{0}\right) \neq 0, $ we have $ g(\hat{\theta}) \stackrel{p}{\rightarrow} g\left(\theta_{0}\right) \neq 0$, $G(\hat{\theta}) \stackrel{p}{\rightarrow} G\left(\theta_{0}\right)$, and $ \hat{V} \stackrel{p}{\rightarrow} V . $ It follows that
$$
\frac{W}{n} \stackrel{p}{\rightarrow} g\left(\theta_{0}\right)^{\prime}\left[G\left(\theta_{0}\right) V G\left(\theta_{0}\right)^{\prime}\right]^{-1} g\left(\theta_{0}\right)>0
$$

under $ \mathbb{H}_{A} $. In other words, with probability approaching one, the Wald statistic $ W $ diverges to positive infinity at the rate of $ n, $ thus ensuring asymptotic power one of the test under $ \mathbb{H}_{A} $ at any given significance level $ \alpha \in(0,1) $. 

$ \hat{W} $ is applicable for many root- $ n $ consistent estimators $ \hat{\theta} $. We now consider a special but important case: that is, when $ \hat{\theta} $ is an MLE. In this case, $ V=-H^{-1}\left(\theta_{0}\right), $ and we can use the asymptotic variance estimator $ \hat{V}=[-\hat{H}(\hat{\theta})]^{-1}, $ where the sample Hessian matrix
$$
\hat{H}(\theta)=\frac{1}{n} \sum_{i=1}^{n} \frac{\partial^{2} \ln f\left(X_{i}, \theta\right)}{\partial \theta \partial \theta^{\prime}}
$$
The resulting Wald test statistic can be constructed as follows:
$$
\hat{W}=n g(\hat{\theta})^{\prime}\left[-G(\hat{\theta}) \hat{H}^{-1}(\hat{\theta}) G(\hat{\theta})^{\prime}\right]^{-1} g(\hat{\theta})
$$
If in addition the regularity conditions of Section 8.3 hold, we can show $ \hat{H}(\hat{\theta}) \rightarrow H\left(\theta_{0}\right) $ as $ n \rightarrow \infty $ almost surely, and therefore $ W \rightarrow \chi_{q}^{2} $ under $ \mathbb{H}_{0} $. 

Note that the Wald test statistic $ W $ only involves estimation under the alternative $ \mathbb{H}_{A} $.

## 9.4 Lagrangian Multiplier (LM) Test

Next, we introduce the Lagrange Multiplier (LM) test. It is also called Rao's (1959) efficient score test in statistics.

Suppose we have an IID random sample $ \mathrm{X}^{n} $ from the population $ f\left(x, \theta_{0}\right), $ where $ \theta_{0} $ is an unknown parameter value in $ \Theta $ (i.e. correct model specification).

Consider the normalized log-likelihood
$$
\hat{l}(\theta)=\frac{1}{n} \sum_{i=1}^{n} \ln f\left(X_{i}, \theta\right)
$$
and the constrained maximum likelihood estimator that solves the constrained maximization problem
$$
\begin{aligned}
\tilde{\theta}=\arg &\max _{\theta \in \Theta} \hat{l}(\theta) \\
&s.t.  g(\theta)=0 
\end{aligned}
$$
【Theorem 9.4】**LM Test**: Suppose Assumptions M.1-M.6, Assumption W.3, and $ \mathbb{H}_{0} $ hold. Define
$$
L M=n \tilde{\lambda}^{\prime} G(\tilde{\theta})[-\hat{H}(\tilde{\theta})]^{-1} G(\tilde{\theta})^{\prime} \tilde{\lambda}
$$
Then under $ \mathbb{H}_{0} $
$$
L M \stackrel{d}{\rightarrow} \chi_{q}^{2} \text { as } n \rightarrow \infty
$$
Define the Lagrangian function
$$
L(\theta, \lambda)=\hat{l}(\theta)+\lambda^{\prime} g(\theta)
$$
where $ \lambda $ is a $q \times 1$ Lagrangian multiplier. Let $ \tilde{\lambda} $ be the corresponding maximizing value of $ \lambda $.

Then the first order conditions (FOC) are

$$
\begin{aligned}
\frac{\partial L(\tilde{\theta}, \tilde{\lambda})}{\partial \theta} &=\frac{\partial \hat{l}(\tilde{\theta})}{\partial \theta}+G(\tilde{\theta})^{\prime} \tilde{\lambda}=0 \quad &(9.1)\\
\frac{\partial L(\tilde{\theta}, \tilde{\lambda})}{\partial \lambda} &=g(\tilde{\theta})=0 \quad &(9.2)
\end{aligned}
$$

Equation (9.1) is $p \times 1$ and equation (9.2) is $q \times 1$. Therefore, we have $p+q$ equations, and also $ p+q $ parameters to estimate ($ \theta $ and $ \lambda $).

By equation (1) and the mean value theorem/ 中值定理, we have
$$
\begin{aligned}
G(\tilde{\theta})^{\prime} \tilde{\lambda} &=-\frac{d \hat{l}(\tilde{\theta})}{d \theta} \\
&=-\frac{d \hat{l}\left(\theta_{0}\right)}{d \theta}-\frac{d^{2} \hat{l}\left(\bar{\theta}_{a}\right)}{d \theta d \theta^{\prime}}\left(\tilde{\theta}-\theta_{0}\right)
\end{aligned}
$$

where $ \bar{\theta}_{a}=a \tilde{\theta}+(1-a) \theta_{0} $ for some $ a \in[0,1], $ which lies on the segment between $ \tilde{\theta} $ and $ \theta_{0} $ Note

$$
\frac{d^{2} \hat{l}(\theta)}{d \theta d \theta^{\prime}}=\hat{H}(\theta)
$$

is the sample Hessian matrix. Given the regularity conditions in Section 8.3, we have shown there that $ \hat{H}(\hat{\theta}) \rightarrow H\left(\theta_{0}\right) $ as $ n \rightarrow \infty $ almost surely for any consistent estimator $ \hat{\theta} $ of $ \theta_{0} $. 

Because $ H\left(\theta_{0}\right) $ is nonsingular, $ \hat{H}^{-1}(\hat{\theta}) \rightarrow H^{-1}\left(\theta_{0}\right) $ as $ n \rightarrow \infty $ almost surely, and $ \hat{H}^{-1}(\hat{\theta}) $ exists for $ n $ sufficiently large. It follows that
$$
\hat{H}\left(\bar{\theta}_{a}\right)^{-1} G(\tilde{\theta})^{\prime} \tilde{\lambda}=-\hat{H}\left(\bar{\theta}_{a}\right)^{-1} \frac{d \hat{l}\left(\theta_{0}\right)}{d \theta}-\left(\tilde{\theta}-\theta_{0}\right) \quad (9.3)
$$

Next, by the mean value theorem again, we have
$$
0=g(\tilde{\theta})=g\left(\theta_{0}\right)+G\left(\bar{\theta}_{b}\right)\left(\tilde{\theta}-\theta_{0}\right)
$$
where $ \bar{\theta}_{b}=b \tilde{\theta}+(1-b) \theta_{0} $ for some $ b \in[0,1], $ which lies on the segment between $ \tilde{\theta} $ and $ \theta_{0} $. 

It follows that under $ \mathbb{H}_{0}: g\left(\theta_{0}\right)=0, $ we have
$$
G\left(\bar{\theta}_{b}\right)\left(\tilde{\theta}-\theta_{0}\right)=0 \quad (9.4)
$$
Hence, multiplying Eq.(9.3) by $ G\left(\bar{\theta}_{b}\right) $ and using Eq.(9.4), we obtain
$$
\begin{aligned}
& G\left(\bar{\theta}_{b}\right) \hat{H}\left(\bar{\theta}_{a}\right)^{-1} G(\tilde{\theta})^{\prime} \tilde{\lambda} \\
=&-G\left(\bar{\theta}_{b}\right) \hat{H}\left(\bar{\theta}_{a}\right)^{-1} \frac{d \hat{l}\left(\theta_{0}\right)}{d \theta}-G\left(\bar{\theta}_{b}\right)\left(\tilde{\theta}-\theta_{0}\right) \\
=&-G\left(\bar{\theta}_{b}\right) \hat{H}\left(\bar{\theta}_{a}\right)^{-1} \frac{d \hat{l}\left(\theta_{0}\right)}{d \theta}
\end{aligned}
$$
By the continuity of function $G(\cdot)$, $ \left\|\bar{\theta}_{a}-\theta_{0}\right\| \leq\left\|\tilde{\theta}-\theta_{0}\right\| \rightarrow 0,\left\|\bar{\theta}_{b}-\theta_{0}\right\| \leq\left\|\tilde{\theta}-\theta_{0}\right\| \rightarrow 0 $ and $ \hat{H}(\hat{\theta}) \rightarrow H\left(\theta_{0}\right) $ as $ n \rightarrow \infty $ almost surely, we have
$$
G\left(\bar{\theta}_{b}\right) \hat{H}\left(\bar{\theta}_{a}\right)^{-1} G(\tilde{\theta})^{\prime} \rightarrow G\left(\theta_{0}\right) H^{-1}\left(\theta_{0}\right) G\left(\theta_{0}\right)^{\prime} \text { a.s. }
$$
and the latter is nonsingular. Therefore, for $ n $ sufficiently large, $ G\left(\bar{\theta}_{b}\right) \hat{H}\left(\bar{\theta}_{a}\right)^{-1} G(\tilde{\theta})^{\prime} $ is nonsingular as well. Hence,
$$
\begin{aligned}
\sqrt{n} \tilde{\lambda} &=-\left[G\left(\bar{\theta}_{b}\right) \hat{H}\left(\bar{\theta}_{a}\right)^{-1} G(\tilde{\theta})^{\prime}\right]^{-1} G\left(\bar{\theta}_{b}\right) \hat{H}\left(\bar{\theta}_{a}\right)^{-1} \sqrt{n} \frac{d \hat{l}\left(\theta_{0}\right)}{d \theta} \\
&=-\hat{A} \sqrt{n} \frac{d \hat{l}\left(\theta_{0}\right)}{d \theta}, \text { say. }
\end{aligned}
$$

By CLT for an IID random sequence, we have (refer section 8.3.3)
$$
\sqrt{n} \frac{d \hat{l}\left(\theta_{0}\right)}{d \theta}=\frac{1}{\sqrt{n}} \sum_{i=1}^{n} \frac{\partial \ln f\left(X_{i}, \theta_{0}\right)}{\partial \theta} \stackrel{d}{\rightarrow} N\left(0, I\left(\theta_{0}\right)\right)
$$
where $ I\left(\theta_{0}\right) $ is Fisher's information matrix evaluated at $ \theta=\theta_{0} $

On the other hand.
$$
\hat{A} \stackrel{p}{\rightarrow}\left[G\left(\theta_{0}\right) H\left(\theta_{0}\right)^{-1} G\left(\theta_{0}\right)^{\prime}\right]^{-1} G\left(\theta_{0}\right) H\left(\theta_{0}\right)=A_{0}, \text { say. }
$$
It follows from the Slutsky's theorem that
$$
\begin{aligned}
& \sqrt{n} \tilde{\lambda} \stackrel{d}{\rightarrow} N\left(0, A_{0} I\left(\theta_{0}\right) A_{0}^{\prime}\right) \\
\sim & N\left(0,-\left[G\left(\theta_{0}\right) H\left(\theta_{0}\right)^{-1} G\left(\theta_{0}\right)^{\prime}\right]^{-1}\right)
\end{aligned}
$$
where we have used the fact that
$$
\begin{aligned}
A_{0} I\left(\theta_{0}\right) A_{0}^{\prime} &=\left[G\left(\theta_{0}\right) H\left(\theta_{0}\right)^{-1} G\left(\theta_{0}\right)^{\prime}\right]^{-1} G\left(\theta_{0}\right) H\left(\theta_{0}\right)^{-1} I\left(\theta_{0}\right) H\left(\theta_{0}\right)^{-1} G\left(\theta_{0}\right)^{\prime}\left[G\left(\theta_{0}\right) H\left(\theta_{0}\right)^{-1} G\left(\theta_{0}\right)^{\prime}\right]^{-1} \\
&=-\left[G\left(\theta_{0}\right) H\left(\theta_{0}\right)^{-1} G\left(\theta_{0}\right)^{\prime}\right]^{-1}
\end{aligned}
$$

given the information matrix equality $ I\left(\theta_{0}\right)+H\left(\theta_{0}\right)=0 . $ It follows that the quadratic form
$$
-n \tilde{\lambda}^{\prime} G\left(\theta_{0}\right) H\left(\theta_{0}\right)^{-1} G\left(\theta_{0}\right)^{\prime} \tilde{\lambda} \stackrel{d}{\rightarrow} \chi_{q}^{2}
$$
under $ \mathbb{H}_{0} $

**Remarks**
An asymptotically size $ \alpha $ LM test will reject the null hypothesis $ \mathbb{H}_{0}: g\left(\theta_{0}\right)=0 $ when $ L M $ exceeds the $ (1-\alpha) $ th quantile of the $ \chi_{q}^{2} $ distribution.

Question: What is the interpretation for the Lagrangian multiplier $ \tilde{\lambda}  $?

When the parametric restriction $ g\left(\theta_{0}\right)=0 $ is valid, the restricted estimator $ \tilde{\theta} $ should be near the point that maximizes the log-likelihood. Therefore, the slope of the log-likelihood function should be close to zero at the restricted estimator $ \theta $.

As can be seen from the FOC that $ G(\tilde{\theta})^{\prime} \tilde{\lambda}=-\frac{d \hat{l}(\tilde{\theta})}{d \theta}, $ the LM test is based on the slope of the log-likelihood at the point where the function is maximized subject to the constraint.

Alternatively, the Lagrange multiplier $ \tilde{\lambda} $ measures the magnitude of the departure of $ g(\tilde{\theta}) $ from $ g\left(\theta_{0}\right)=0 $.

- If the constraint $ g\left(\theta_{0}\right)=0 $ holds, then $ g(\tilde{\theta}) $ will be close to $ 0, $ and so is $ \tilde{\lambda} $.
- If the constraint $ g\left(\theta_{0}\right) \neq 0, $ then $ g(\tilde{\theta}) $ will be significantly different from  0. As a result, $ \tilde{\lambda} $ will be large, giving the LM test power to reject $ \mathbb{H}_{0} $. How large $ \sqrt{n} \tilde{\lambda} $ is considered as "large" is determined by its sampling distribution.

Thus under large $\tilde{\lambda} $, we can reject null hypothesis. That's why LM test use quadratic form of  $ \tilde{\lambda} $ intuitively.

The LM test is convenient to use in practice, because only the null model, which is usually simpler, has to be estimated.

## 9.5 Likelihood Ratio Test(之后待整理)

Since $ \mathrm{X}^{n} $ is an IID random sample from the population $ f_{X}(x)=f\left(x, \theta_{0}\right), $ where $ \theta_{0} $ is unknown, we have the likelihood function of $ \mathrm{X}^{n} $
$$
f_{\mathbf{X}^{n}}\left(\mathbf{X}^{n}, \theta\right)=\prod_{i=1}^{n} f\left(X_{i}, \theta\right)
$$

We define the likelihood ratio statistic
$$
\begin{aligned}
\hat{\Lambda} &=\frac{\max _{\theta \in \Theta} f_{\mathbf{X}^{n}}\left(\mathbf{X}^{n}, \theta\right)}{\max _{\theta \in \Theta_{0}} f_{\mathbf{X}^{n}}\left(\mathbf{X}^{n}, \theta\right)} \\
&=\frac{\prod_{i=1}^{n} f\left(X_{i}, \hat{\theta}\right)}{\prod_{i=1}^{n} f\left(X_{i}, \tilde{\theta}\right)}
\end{aligned}
$$
where $ \hat{\theta} $ and $ \tilde{\theta} $ are the unconstrained and constrained MLEs respectively, namely,
$$
\begin{aligned}
\hat{\theta} &=\arg \max _{\theta \in \Theta} \hat{l}(\theta) \\
\tilde{\theta} &=\arg \max _{\theta \in \Theta_{0}} \hat{l}(\theta)
\end{aligned}
$$

with

$$
\hat{l}(\theta)=\frac{1}{n} \sum_{i=1}^{n} \ln f\left(X_{i}, \theta\right) = \frac{1}{n} \ln \hat{L}(\theta | X^n)
$$

is the sample average of log-likelihood functions, and $ \Theta_{0} $ is the parameter space $ \Theta $ subject to the constraint $ g(\theta)=0, $ i.e., $ \Theta_{0}=\{\theta \in \Theta: g(\theta)=0\} $. Mathematically, $\hat{\Lambda} \geq 1$.

Suppose the null hypothesis $ \mathbb{H}_{0}: g\left(\theta_{0}\right)=0 $ holds. Then both unconstrained and constrained MLEs $ \hat{\theta} $ and $ \tilde{\theta} $ are consistent for $ \theta_{0} $, and imposing the restriction should not lead to a large reduction in the log-likelihood function. Therefore, we expect that the likelihood ratio $ \hat{\Lambda} $ will be close to 1.

On the other hand, if $ \mathbb{H}_{0} $ is false, then the unconstrained MLE $ \hat{\theta} $ is consistent for $ \theta_{0} $ but the constrained MLE $ \theta $ is not. As a consequence, we expect that the likelihood ratio $ \hat{\Lambda} $ is larger than 1.

Hence, we can test $ \mathbb{H}_{0} $ by comparing whether $ \hat{\Lambda} $ is significantly larger than 1 or whether $ \ln \hat{\Lambda} $ is greater than 0. How large $ \hat{\Lambda} $ or $ \ln \hat{\Lambda} $ must be in order to be considered as significantly large will be determined by the sampling distribution of $ \hat{\Lambda} $.

Formally, we define the likelihood ratio test statistic as follows.
$$
L R=2 \ln \hat{\Lambda}=2 n[\hat{l}(\hat{\theta})-\hat{l}(\tilde{\theta})]
$$
【Theorem 9.5】**LR Test**: Suppose Assumptions M.1-M.6, Assumption W.3, and $ \mathbb{H}_{0} $ hold. Then under $ \mathbb{H}_{0} $
$$
L R \stackrel{d}{\rightarrow} \chi_{q}^{2} \text { as } n \rightarrow \infty
$$
**Proof:**

By a second order Taylor's series expansion of $ \hat{l}(\tilde{\theta}) $ around the unconstrained MLE $ \hat{\theta}, $ we have
$$
\begin{aligned}
L R &=2 n\left\{\hat{l}(\hat{\theta})-\left[\hat{l}(\hat{\theta})+\frac{d \hat{l}(\hat{\theta})}{d \theta}(\tilde{\theta}-\hat{\theta})+\frac{1}{2}(\tilde{\theta}-\hat{\theta})^{\prime} \frac{d^{2} \hat{l}\left(\bar{\theta}_{a}\right)}{d \theta d \theta^{\prime}}(\tilde{\theta}-\hat{\theta})\right]\right\} \\
&=\sqrt{n}(\tilde{\theta}-\hat{\theta})^{\prime}\left[-\hat{H}\left(\bar{\theta}_{a}\right)\right] \sqrt{n}(\tilde{\theta}-\hat{\theta})
\end{aligned}
$$
where $ \bar{\theta}_{a}=a \tilde{\theta}+(1-a) \hat{\theta} $ for some $ a \in[0,1], $ lies on the segment between $ \tilde{\theta} $ and $ \hat{\theta}, $ and $ \frac{d}{d \theta} \hat{l}(\hat{\theta})=0 $ which is the FOC of the unconstrained MLE $ \hat{\theta}, $ and again
$$
\hat{H}(\theta)=\frac{d^{2} \hat{l}(\theta)}{d \theta d \theta^{\prime}}
$$
is the sample Hessian matrix.

Next, applying the mean value theorem for $ \frac{d}{d \theta} \hat{l}(\hat{\theta}) $ around the constrained $ \mathrm{MLE} \tilde{\theta}, $ we obtain
$$
\begin{aligned}
0 &=\frac{d \hat{l}(\hat{\theta})}{d \theta} \\
&=\frac{d \hat{l}(\tilde{\theta})}{d \theta}+\hat{H}\left(\bar{\theta}_{b}\right)(\hat{\theta}-\tilde{\theta})
\end{aligned}
$$
where $ \bar{\theta}_{b}=b \tilde{\theta}+(1-b) \hat{\theta} $ for some $ b \in[0,1], $ lies on the segment between $ \tilde{\theta} $ and $ \hat{\theta} $. This and the FOC of the constrained MLE that $ G(\tilde{\theta})^{\prime} \tilde{\lambda}=-\frac{d}{d \theta} \hat{l}(\tilde{\theta}) $ imply
$$
\begin{aligned}
\sqrt{n}(\hat{\theta}-\tilde{\theta}) &=-\hat{H}\left(\bar{\theta}_{b}\right)^{-1} \sqrt{n} \frac{d \hat{l}(\tilde{\theta})}{d \theta} \\
&=\hat{H}\left(\bar{\theta}_{b}\right)^{-1} G(\tilde{\theta})^{\prime} \sqrt{n} \tilde{\lambda}
\end{aligned}
$$
This relationship provides an alternative interpretation for the multiplier $ \tilde{\lambda}, $ namely, it measures the difference between the unconstrained and constrained MLEs $ \hat{\theta} $ and $ \tilde{\theta} $.

It follows that
$$
L R=-\sqrt{n} \tilde{\lambda} G(\tilde{\theta}) \hat{H}\left(\bar{\theta}_{b}\right)^{-1} \hat{H}\left(\bar{\theta}_{a}\right) \hat{H}\left(\bar{\theta}_{b}\right)^{-1} G(\tilde{\theta})^{\prime} \sqrt{n} \tilde{\lambda}
$$
Because
$$
\begin{aligned}
L R-L M &=-\sqrt{n} \tilde{\lambda}\left[G(\tilde{\theta}) \hat{H}\left(\bar{\theta}_{b}\right)^{-1} \hat{H}\left(\bar{\theta}_{a}\right) \hat{H}\left(\bar{\theta}_{b}\right)^{-1} G(\tilde{\theta})^{\prime}-G(\tilde{\theta}) \hat{H}(\tilde{\theta})^{-1} G(\tilde{\theta})^{\prime}\right] \sqrt{n} \tilde{\lambda} \\
&=O_{P}(1) o_{P}(1) O_{P}(1) \\
&=o_{P}(1)
\end{aligned}
$$

where $ \sqrt{n} \tilde{\lambda}=O_{P}(1) $ by Lemma 7.11 given $ \sqrt{n} \tilde{\lambda} \stackrel{d}{\rightarrow} N\left(0,-\left[G\left(\theta_{0}\right) H\left(\theta_{0}\right)^{-1} G\left(\theta_{0}\right)^{\prime}\right]^{-1}\right) $ as
shown in Section 9.4, and
$$
\begin{aligned}
&G(\tilde{\theta}) \hat{H}\left(\bar{\theta}_{b}\right)^{-1} \hat{H}\left(\bar{\theta}_{a}\right) \hat{H}\left(\bar{\theta}_{b}\right)^{-1} G(\tilde{\theta})^{\prime}-G(\tilde{\theta}) \hat{H}(\tilde{\theta})^{-1} G(\tilde{\theta})^{\prime} \\
\stackrel{p}{\rightarrow} &G\left(\theta_{0}\right) H\left(\theta_{0}\right)^{-1} H\left(\theta_{0}\right) H\left(\theta_{0}\right)^{-1} G\left(\theta_{0}\right)^{\prime}-G\left(\theta_{0}\right) H\left(\theta_{0}\right)^{-1} G\left(\theta_{0}\right)^{\prime}\\
=&0
\end{aligned}
$$
It follows that $ L R $ and $ L M $ are **asymptotically equivalent** under $ \mathbb{H}_{0} $. As a result, by the **asymptotic equivalence lemma** and $ L M \stackrel{d}{\rightarrow} \chi_{q}^{2} $ as $ n \rightarrow \infty $ under $ \mathbb{H}_{0}, $ we have $L R \stackrel{d}{\rightarrow} \chi_{q}^{2} \text { as } n \rightarrow \infty$.

**Remarks:**

In fact, it can also be shown that the Wald test statistic $ W $ based on MLE $ \hat{\theta} $ and the LM test statistic $ L M $ are asymptotically equivalent under $ \mathbb{H}_{0} $. This implies that all three tests are asymptotically equivalent under $ \mathbb{H}_{0} $.

The LR test statistic $ L R $ involves both constrained and unconstrained MLE estimators. However, it is very convenient to compute, because the sample log-likelihood value is the objective function and is usually reported by statistical software when a probability distribution model $ f(x, \theta) $ is estimated by the MLE

信息矩阵恒等式不成立（模型不正确设定的情况下），LR完全不能用，LM与Wald可用只是会更加复杂

Theorem $ 5(9.6) .[\text { LR } \text { Test Based on Sufficient Statistic }]: $ If $ T\left(\mathrm{X}^{n}\right) $ is a sufficient statistic for $ \theta, $ and $ L R\left(\mathbf{X}^{n}\right) $ and $ L R\left[T\left(\mathbf{X}^{n}\right)\right] $ are the likelihood ratio tests based on $ \mathbf{X}^{n} $ and $ T\left(\mathbf{X}^{n}\right) $ respectively, then
$$
L R\left(\mathbf{X}^{n}\right)=L R\left[T\left(\mathbf{X}^{n}\right)\right]
$$
Proof:
By the Factorization theorem, the PMF/PDF of $ \mathbf{X}^{n} $ can be written as
$$
f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}, \theta\right)=g\left[T\left(\mathbf{x}^{n}\right), \theta\right] h\left(\mathbf{x}^{n}\right), \text { for all } \theta \in \Theta
$$
where $ g(t, \theta) $ is the $ \mathrm{PMF} / \mathrm{PDF} $ of $ T\left(\mathrm{X}^{n}\right) $ and $ h\left(\mathrm{x}^{n}\right) $ does not depend on $ \theta $ It follows that

$$
\begin{aligned}
L R\left(\mathbf{X}^{n}\right) &=2 n \ln \hat{\Lambda} \\
&=2 n \ln \left[\frac{f_{\mathbf{X}^{n}}\left(\mathbf{X}^{n}, \hat{\theta}\right)}{f_{\mathbf{X}^{n}}\left(\mathbf{X}^{n}, \tilde{\theta}\right)}\right] \\
&=2 n \ln \left\{\frac{g\left[T\left(\mathbf{X}^{n}\right), \hat{\theta}\right] h\left(\mathbf{X}^{n}\right)}{g\left[T\left(\mathbf{X}^{n}\right), \tilde{\theta}\right] h\left(\mathbf{X}^{n}\right)}\right\} \\
&=2 n \ln \left\{\frac{g\left[T\left(\mathbf{X}^{n}\right), \hat{\theta}\right]}{g\left[T\left(\mathbf{X}^{n}\right), \tilde{\theta}\right]}\right\} \\
&=L R\left[T\left(\mathbf{X}^{n}\right)\right]
\end{aligned}
$$
This implies that the LR test statistic depends on $ \mathrm{X}^{n} $ only through the statistic $ T\left(\mathrm{X}^{n}\right) $ when $ T\left(\mathbf{X}^{n}\right) $ is a sufficient statistic for $ \theta $

## 9.6 Illustrative Examples

### 9.6.1 Hypothesis Testing under the Bernoulli Distribution

Suppose $ \mathbf{X}^{n} $ is an IID random sample from a Bernoulli $ (\theta) $ distribution, where a Bernoulli random variable takes two possible values:
$$
X_{i}=\left\{\begin{array}{l}
1, \text { with probability } \theta \\
0, \text { with probability } 1-\theta
\end{array}\right.
$$
The parameter $ \theta \in(0,1) $ is unknown.

Suppose we are interested in testing
$$
\mathbb{H}_{0}: \theta=\theta_{0}
$$
versus
$$
\mathbb{H}_{1}: \theta \neq \theta_{0}
$$
Hence, we have $ g(\theta)=\theta-\theta_{0}, $ and the gradient
$$
G(\theta)=\frac{d g(\theta)}{d \theta}=1
$$
since the population PMF $ f(x, \theta)=\theta^{x}(1-\theta)^{1-x} $ for $ x=0,1, $ the log-likelihood function of the IID random sample $ \mathbf{X}^{n} $ is given by
$$
\begin{aligned}
\ln \hat{L}\left(\theta | \mathbf{X}^{n}\right) &=\sum_{i=1}^{n} \ln f\left(X_{i}, \theta\right) \\
&=n \bar{X}_{n} \ln \theta+n\left(1-\bar{X}_{n}\right) \ln (1-\theta)
\end{aligned}
$$
where the sample mean $ \bar{X}_{n} $ is a sufficient statistic for $ \theta $. The FOC of the MLE is
$$
\frac{\partial \ln \hat{L}\left(\theta | \mathbf{X}^{n}\right)}{\partial \theta}=\frac{n \bar{X}_{n}}{\hat{\theta}}-\frac{n-n \bar{X}_{n}}{1-\hat{\theta}}=0
$$
Thus we have the MLE $ \hat{\theta}=\bar{X}_{n} $.

#### The Wald Test

Recall the Hessian matrix
$$
H(\theta)=E_{\theta}\left[\frac{\partial^{2} \ln f\left(X_{i}, \theta\right)}{\partial \theta^{2}}\right]
$$
Because
$$
\frac{\partial^{2} \ln f\left(X_{i}, \theta\right)}{\partial \theta^{2}}=-\frac{X_{i}}{\theta^{2}}-\frac{1-X_{i}}{(1-\theta)^{2}}
$$
the sample Hessian matrix
$$
\begin{aligned}
\hat{H}(\theta) &=n^{-1} \sum_{i=1}^{n} \frac{\partial^{2} \ln f\left(X_{i}, \theta\right)}{\partial \theta} \\
&=-\frac{\sum_{i=1}^{n} X_{i}}{n \theta^{2}}-\frac{\sum_{i=1}^{n}\left(1-X_{i}\right)}{n(1-\theta)^{2}} \\
&=-\frac{\bar{X}_{n}}{\theta^{2}}-\frac{1-\bar{X}_{n}}{(1-\theta)^{2}}
\end{aligned}
$$
Hence, we have
$$
\hat{H}(\hat{\theta})=-\frac{1}{\bar{X}_{n}\left(1-\bar{X}_{n}\right)}
$$

The Wald test statistic
$$
\begin{aligned}
\hat{W} &=n g(\hat{\theta})^{\prime}\left[-G(\hat{\theta}) \hat{H}^{-1}(\hat{\theta}) G(\hat{\theta})^{\prime}\right]^{-1} g(\hat{\theta}) \\
&=\frac{n\left(\hat{\theta}-\theta_{0}\right)^{2}}{\bar{X}_{n}\left(1-\bar{X}_{n}\right)} \\
&=\frac{n\left(\bar{X}_{n}-\theta_{0}\right)^{2}}{\bar{X}_{n}\left(1-\bar{X}_{n}\right)} \stackrel{d}{\rightarrow} \chi_{1}^{2} \text { as } n \rightarrow \infty
\end{aligned}
$$
under $ \mathbb{H}_{0} . $ Hence, $ \sqrt{n}\left(\bar{X}_{n}-\theta_{0}\right) \stackrel{d}{\rightarrow} N\left(0, \sigma^{2}\right) $.

#### The LM Test

Define the Lagrangian function
$$
L(\theta, \lambda)=\hat{l}(\theta)+\lambda^{\prime} g(\theta)=\hat{l}(\theta)+\lambda\left(\theta-\theta_{0}\right)
$$
where the normalized log-likelihood
$$
\hat{l}(\theta)=\bar{X}_{n} \ln \theta+\left(1-\bar{X}_{n}\right) \ln (1-\theta)
$$
The first order conditions for the constrained MLE are
$$
\begin{aligned}
\frac{\partial L(\tilde{\theta}, \tilde{\lambda})}{\partial \theta} &=\frac{\partial \hat{l}(\tilde{\theta})}{\partial \theta}+\tilde{\lambda}=0 \\
\frac{\partial L(\tilde{\theta}, \tilde{\lambda})}{\partial \lambda} &=g(\tilde{\theta})=\tilde{\theta}-\theta_{0}=0
\end{aligned}
$$

It follows that $ \tilde{\theta}=\theta_{0}, $ and
$$
\begin{aligned}
\tilde{\lambda} &=-\frac{\partial \hat{l}(\tilde{\theta})}{\partial \theta} \\
&=-\frac{\bar{X}_{n}}{\tilde{\theta}}+\frac{1-\bar{X}_{n}}{1-\tilde{\theta}} \\
&=-\frac{\bar{X}_{n}-\tilde{\theta}}{\tilde{\theta}(1-\tilde{\theta})} \\
&=-\frac{\bar{X}_{n}-\theta_{0}}{\theta_{0}\left(1-\theta_{0}\right)}
\end{aligned}
$$
This indicates that $ \tilde{\lambda} $ measures the difference between the unconstrained MLE $ \hat{\theta} $ and the constrained MLE $ \tilde{\theta}=\theta_{0} $.

Also, the sample Hessian matrix
$$
\begin{aligned}
\hat{H}(\tilde{\theta}) &=-\frac{\bar{X}_{n}}{\theta_{0}^{2}}-\frac{1-\bar{X}_{n}}{\left(1-\theta_{0}\right)^{2}} \\
&=-\frac{\bar{X}_{n}\left(1-\theta_{0}\right)^{2}+\left(1-\bar{X}_{n}\right) \theta_{0}^{2}}{\theta_{0}^{2}\left(1-\theta_{0}\right)^{2}}
\end{aligned}
$$

Therefore, we have
$$
\begin{aligned}
L M &=-n \tilde{\lambda}_{n}^{\prime} G(\tilde{\theta}) \hat{H}(\tilde{\theta})^{-1} G(\tilde{\theta})^{\prime} \tilde{\lambda} \\
&=n\left[-\frac{\bar{X}_{n}-\theta_{0}}{\theta_{0}\left(1-\theta_{0}\right)}\right]^{2}\left[\frac{\bar{X}_{n}\left(1-\theta_{0}\right)^{2}+\left(1-\bar{X}_{n}\right) \theta_{0}^{2}}{\theta_{0}^{2}\left(1-\theta_{0}\right)^{2}}\right]^{-1} \\
&=\frac{n\left(\bar{X}_{n}-\theta_{0}\right)^{2}}{\bar{X}_{n}\left(1-\theta_{0}\right)^{2}+\left(1-\bar{X}_{n}\right) \theta_{0}^{2}}
\end{aligned}
$$
#### The LR Test

Finally, we calculate the LR test statistic
$$
\begin{aligned}
L R &=2 n[\hat{l}(\hat{\theta})-\hat{l}(\tilde{\theta})] \\
&=2 n\left[\bar{X}_{n} \ln \left(\frac{\bar{X}_{n}}{\theta_{0}}\right)+\left(1-\bar{X}_{n}\right) \ln \left(\frac{1-\bar{X}_{n}}{1-\theta_{0}}\right)\right]
\end{aligned}
$$
### 9.6.2 Hypothesis Testing under the Normal Distribution

Suppose $ X^{n} $ is an IID random sample from a $ N\left(\mu, \sigma^{2}\right) $ population, where $ \theta=\left(\mu, \sigma^{2}\right) $ is unknown.

We are interested in testing the hypotheses
$$
\mathbb{H}_{0}: \mu=\mu_{0}
$$

versus
$$
\mathbb{H}_{A}: \mu \neq \mu_{0}
$$
where $ \mu_{0} $ is a known number. This is equivalent to choosing the test function
$$
g(\theta)=\mu-\mu_{0}
$$
It follows that
$$
G(\theta)=\frac{d g(\theta)}{d \theta}=(1,0)
$$
is a two-dimensional row vector.

Since the PDF of a $ N\left(\mu, \sigma^{2}\right) $ population is
$$
f(x, \theta)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
$$
The normalized log-likelihood function of the random sample $ \mathbf{X}^{n} $ is
$$
\hat{l}\left(\theta | \mathbf{X}^{n}\right)=-\frac{1}{2} \ln (2 \pi)-\frac{1}{2} \ln \left(\sigma^{2}\right)-\frac{1}{2 \sigma^{2}} \frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}
$$

For the unconstrained MLE, we have obtained $ \hat{\theta}=\left(\hat{\mu}, \hat{\sigma}^{2}\right), $ where
$$
\begin{aligned}
\hat{\mu} &=\bar{X}_{n} \\
\hat{\sigma}^{2} &=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}_{n}\right)^{2}
\end{aligned}
$$
Also, the sample Hessian matrix
$$
\hat{H}(\theta)=\left[\begin{array}{cc}
-\frac{1}{\sigma^{2}} & -\frac{1}{\sigma^{4}} \frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\mu\right) \\
-\frac{1}{\sigma^{4}} \frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\mu\right) & \frac{1}{2 \sigma^{4}}-\frac{1}{\sigma^{6}} \frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}
\end{array}\right]
$$
When $ \theta=\hat{\theta}, $ we have
$$
\hat{H}(\hat{\theta})=\left[\begin{array}{cc}
-\frac{1}{\hat{\sigma}^{2}} & 0 \\
0 & -\frac{1}{2 \hat{\sigma}^{4}}
\end{array}\right]
$$
The Wald Test Statistic
$$
\begin{aligned}
\hat{W} &=-n g(\hat{\theta})^{\prime}\left[G(\hat{\theta}) \hat{H}^{-1}(\hat{\theta}) G(\hat{\theta})^{\prime}\right]^{-1} g(\hat{\theta}) \\
&=\frac{n\left(\bar{X}_{n}-\mu_{0}\right)^{2}}{\hat{\sigma}^{2}} \\
&\stackrel{d}{\rightarrow}  \chi_{1}^{2}
\end{aligned}
$$
under $ \mathbb{H}_{0} $.

#### The LM Test Statistic

Consider the constrained MLE
$$
\tilde{\theta}=\max _{\theta \in \Theta} \hat{l}\left(\theta | \mathbf{X}^{n}\right)
$$
subject to the constrain that $ \mu=\mu_{0} $
Define the Lagrangian function
$$
L(\theta, \lambda)=\hat{l}\left(\theta | \mathbf{X}^{n}\right)+\lambda\left(\mu-\mu_{0}\right)
$$
The FOCs are:
$$
\begin{aligned}
\frac{\partial L(\tilde{\theta}, \tilde{\lambda})}{\partial \mu} &=\frac{1}{\tilde{\sigma}^{2}} \frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\tilde{\mu}\right)+\tilde{\lambda}=0 \\
\frac{\partial L(\tilde{\theta}, \tilde{\lambda})}{\partial \sigma^{2}} &=-\frac{1}{2 \tilde{\sigma}^{2}}+\frac{1}{2 \tilde{\sigma}^{4}} \frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\tilde{\mu}\right)^{2}=0 \\
\frac{\partial L(\tilde{\theta}, \tilde{\lambda})}{\partial \lambda} &=\tilde{\mu}-\mu_{0}=0
\end{aligned}
$$

Solving for these FOCs, we obtain
$$
\begin{aligned}
\tilde{\mu} &=\mu_{0} \\
\tilde{\sigma}^{2} &=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\mu_{0}\right)^{2} \\
\tilde{\lambda} &=-\frac{1}{\tilde{\sigma}^{2}}\left(\bar{X}_{n}-\mu_{0}\right)
\end{aligned}
$$
and the sample Hessian matrix
$$
\hat{H}(\tilde{\theta})=\left[\begin{array}{cc}
-\frac{1}{\tilde{\sigma}^{2}} & -\frac{1}{\tilde{\sigma}^{4}}\left(\bar{X}_{n}-\mu_{0}\right) \\
-\frac{1}{\tilde{\sigma}^{4}}\left(\bar{X}_{n}-\mu_{0}\right) & -\frac{1}{2 \tilde{\sigma}^{4}}
\end{array}\right]
$$
It follows that the LM test statistic
$$
\begin{aligned}
L M &=-n \tilde{\lambda}^{\prime} G(\tilde{\theta}) \hat{H}^{-1}(\tilde{\theta}) G(\tilde{\theta})^{\prime} \tilde{\lambda} \\
&=\frac{n\left(\bar{X}_{n}-\mu_{0}\right)^{2}}{\tilde{\sigma}^{2}-2\left(\bar{X}_{n}-\mu_{0}\right)^{2}}
\end{aligned}
$$

![image-20200531114939203](C:\Users\Wuhao\AppData\Roaming\Typora\typora-user-images\image-20200531114939203.png)

#### The Likelihood Ratio Test Statistic

Since
$$
\begin{array}{l}
\hat{l}(\hat{\theta})=-\frac{1}{2} \ln (2 \pi)-\frac{1}{2} \ln \left(\hat{\sigma}^{2}\right)-\frac{1}{2} \\
\hat{l}(\tilde{\theta})=-\frac{1}{2} \ln (2 \pi)-\frac{1}{2} \ln \left(\tilde{\sigma}^{2}\right)-\frac{1}{2}
\end{array}
$$
It follows that
$$
\begin{aligned}
L R &=2 n\left[\hat{l}\left(\hat{\theta} | \mathbf{X}^{n}\right)-\hat{l}\left(\tilde{\theta} | \mathbf{X}^{n}\right)\right] \\
&=n \ln \left(\frac{\tilde{\sigma}^{2}}{\hat{\sigma}^{2}}\right)
\end{aligned}
$$
It is interesting to observe that the LR principle tests the hypothesis on the population mean by comparing the variance estimators under the null and alternative hypotheses. Here, the likelihood ratio is a log-function of the sample variance ratio. Intuitively, when $ \mu \neq \mu_{0}$, $ \tilde{\sigma}^{2}=n^{-1} \sum_{i=1}^{n}\left(X_{i}-\mu_{0}\right)^{2} $ is not a consistent estimator for $ \sigma^{2} . $ It will be larger than $ \hat{\sigma}^{2} $ when $ n $ is large, giving the LR its power.

![image-20200531115139421](C:\Users\Wuhao\AppData\Roaming\Typora\typora-user-images\image-20200531115139421.png)

## 9.7 Conclusion

Hypothesis testing is one of the most important tasks in statistical inference. In this chapter we have introduced basic ideas of hypothesis testing in statistical inference.

We introduce the well-known Neyman-Pearman lemma that a likelihood ratio based test will be uniformly most powerful test for simple hypotheses.

We discuss three important testing methods, namely the Wald test, the Lagrange Multiplier (LM) test, and the Likelihood Ratio (LR) test, and show they are asymptotically equivalent to each other under the null hypothesis.

**It is important to note that all hypothesis tests considered in this chapter assume that the population distribution model is correctly specified.**

When the population distribution model is misspecified, the Wald test statistic and the LM test statistics have to be modified to use a consistent asymptotic variance estimator which is robust to model misspecification. However, it is impossible to modify the Likelihood ratio test statistic.

Finally, when testing economic hypotheses, we usually need to transform an economic hypothesis into a statistical hypothesis on model parameters. since some auxiliary conditions are often imposed in such a transformation, there usually exists a gap between the original economic hypothesis and the resulting statistical hypothesis. Caution is needed to interpret the testing results of the statistical hypotheses.

