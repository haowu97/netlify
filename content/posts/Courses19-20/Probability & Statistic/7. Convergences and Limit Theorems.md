# 7 Convergences and Limit Theorems

## 7.1 Limits and Orders of Magnitude

【limit】Let $\left\{b_{n}, n=1,2, \ldots\right\}$ be a sequence of (non-stochastic) real numbers. If there exists a real number $b$ such that for every real number $\varepsilon>0,$ there exists a finite integer $N(\varepsilon)$ such that
$$
\left|b_{n}-b\right|<\varepsilon \quad \text { for all } n \geqslant N(\varepsilon)
$$
then $b$ is called the limit of the sequence $\left\{b_{n}\right\} .$ We write $b_{n} \rightarrow b$ as $n \rightarrow \infty,$ or $\lim _{n \rightarrow \infty} b_{n}=b$

Remarks: If $a_{n} \rightarrow a$ and $b_{n} \rightarrow b$ as $n \rightarrow \infty,$ then when $n \rightarrow \infty$

- $a_{n}+b_{n} \rightarrow a+b$
- $a_{n} b_{n} \rightarrow a b$
- $ a_{n} / b_{n} \rightarrow a / b$ if $b \neq 0$

【Continuity】The function $g: \mathbb{R} \rightarrow \mathbb{R}$ is continuous at point b if for any sequence $\left\{b_{n}\right\}$ such that $b_{n} \rightarrow b$ as $n \rightarrow \infty,$ we have
$$
g\left(b_{n}\right) \rightarrow g(b) \text { as } n \rightarrow \infty
$$

Remarks 1:

- An alternative but equivalent definition: for each given $\epsilon>0$ there exists a $\delta=\delta(\epsilon)$ such that whenever $\left|b_{n}-b\right|<\delta,$ we have $\left|g\left(b_{n}\right)-g(b)\right|<\epsilon$.

- When $g(\cdot)$ is continuous at $b,$ we can write
$$
\lim _{b_{n} \rightarrow b} g\left(b_{n}\right)=g\left(\lim _{n \rightarrow \infty} b_{n}\right)=g(b)
$$

In other words, the limit of a sequence of values for a continuous function is equal to the value of the function at the limit.

Remarks 2:

- for any $\varepsilon>0,$ there exists $\delta(\varepsilon)>0$ such that $|g(x)-g(b)|<\varepsilon$ for all $b<x<b+\delta(\varepsilon),$ then we say $g$ is right continuous at $b,$ denoted by $\lim _{x \rightarrow b^{+}} g(x)=g(b)$
- for any $\varepsilon>0,$ there exists $\delta(\varepsilon)>0$ such that $|g(x)-g(b)|<\varepsilon$ for all $b-\delta(\varepsilon)<x<b,$ then we say $g$ is left continuous at $b,$ denoted by $\lim _{x \rightarrow b^{-}} g(x)=g(b)$
- Function $g$ is continuous at $b$ if and only if $g$ is left continuous and right continuous at $b$

【Order of Magnitude】Suppose $\left\{f_{n}>0, n=1,2, \ldots\right\}$ is a sequence of real numbers. A sequence $\left\{b_{n}, n=1,2, \ldots\right\}$ is at most of order $f_{n},$ denoted by
$$
b_{n}=\mathcal{O}\left(f_{n}\right) \quad \text { or } \quad b_{n} / f_{n}=\mathcal{O}(1)
$$
if for some (fixed and sufficiently large) number $0<M<\infty$ , there exists a finite integer $N(M)$ such that for all $n > N(M)$, 
$$
\left|b_{n} / f_{n}\right|< M
$$
A sequence $\left\{b_{n}, n=1,2, \ldots\right\}$ is of order smaller than $f_{n}$ denoted by
$$
b_{n}=o\left(f_{n}\right) \quad \text { or } \quad b_{n} / f_{n}=o(1)
$$
if for every real number $\epsilon > 0 $there exists a finite integer $N(\epsilon)$ such that for all $n > N(\epsilon)$, we have
$$
\left|b_{n} / f_{n}\right|< \epsilon
$$
namely, $b_{n} / f_{n} \rightarrow 0$ as $n \rightarrow \infty$

Remark: 

- $b_{n}=o\left(f_{n}\right)$ implies $b_{n}=\mathcal{O}\left(f_{n}\right)$
- In the definition of $b_{n}=O\left(n^{\lambda}\right),$ the constant $M$ is usually set to be a very big number. Note that it suffices to find one constant $M$ only.
- For $\lambda>0, b_{n}=O\left(n^{\lambda}\right)$ implies that $b_{n}$ grows to infinity at a rate slower than or at most equal to $n^{\lambda}$. In particular, if $\lim _{n \rightarrow \infty} \frac{b_{n}}{n \lambda}=C<\infty$, then $b_{n}=O\left(n^{\lambda}\right)$ or $n^{-\lambda} b_{n}=O(1)$

【Lemma 1】Let $a_{n}$ and $b_{n}$ be scalars

(1) If $a_{n}=O\left(n^{\lambda}\right)$ and $b_{n}=O\left(n^{\mu}\right),$ then
$$
\begin{aligned}
a_{n} b_{n} &=O\left(n^{\lambda+\mu}\right) \\
a_{n}+b_{n} &=O\left(n^{\kappa}\right)
\end{aligned}
$$
where $\kappa=\max (\lambda, \mu)$

(2) If $a_{n}=o\left(n^{\lambda}\right)$ and $b_{n}=o\left(n^{\mu}\right),$ then
$$
\begin{aligned}
a_{n} b_{n} &=o\left(n^{\lambda+\mu}\right) \\
a_{n}+b_{n} &=o\left(n^{\kappa}\right)
\end{aligned}
$$
where $\kappa=\max (\lambda, \mu)$

(3) If $a_{n}=O\left(n^{\lambda}\right)$ and $b_{n}=o\left(n^{\mu}\right),$ then
$$
\begin{aligned}
a_{n} b_{n} &=o\left(n^{\lambda+\mu}\right) \\
a_{n}+b_{n} &=
\begin{cases}
O(n^{\lambda}) \quad &\text{if } \lambda \geq \mu \\
o(n^{\mu}) \quad &\text{if } \lambda < \mu
\end{cases}
\end{aligned}
$$
##### Proof:

(1) Because $a_{n}$ grows at most at rate $n^{\lambda}, b_{n}$ grows at most at rate $n^{\mu},$ the product $a_{n} b_{n}$ will grow at most at rate $n^{\lambda+\mu} .$ This is because for all $n$ sufficiently large (i.e. for all $n \geq N(M))$
$$
\begin{aligned}
\left|\frac{a_{n} b_{n}}{n^{\lambda+\mu}}\right| &=\left|\frac{a_{n}}{n^{\lambda}} \frac{b_{n}}{n^{\mu}}\right| \\
& \leq M \cdot M=M^{2}
\end{aligned}
$$
On the other hand, the sum $a_{n}+b_{n}$ will be dominated by the term that grows to infinity faster: For all $n$ sufficiently large,
$$
\begin{aligned}
\left|\frac{a_{n}+b_{n}}{n^{\kappa}}\right| &=\left|\frac{a_{n}}{n^{\kappa}}+\frac{b_{n}}{n^{\kappa}}\right| \\
& \leq \epsilon+M \leq 2 M
\end{aligned}
$$

Therefore, $a_{n}+b_{n}=O\left(n^{\kappa}\right)$

(2) Similar to the proof of Result (1)

(3) The product $a_{n} b_{n}=o\left(n^{\lambda+\mu}\right)$ because
$$
\begin{aligned}
\left|\frac{a_{n} b_{n}}{n^{\lambda+\mu}}\right| &=\left|\frac{a_{n}}{n^{\lambda}} \frac{b_{n}}{n^{\mu}}\right| \\
& \leq M \cdot\left|\frac{b_{n}}{n^{\mu}}\right| \\
& \rightarrow 0 \text { as } n \rightarrow \infty
\end{aligned}
$$

given $a_{n}=O\left(n^{\lambda}\right)$ and $b_{n}=o\left(n^{\mu}\right)$

## 7.2 Motivation for Convergence Concepts

Why do we need convergence concepts in econometrics?

Recall that a random sample $\mathrm{X}^{n}=\left(X_{1}, \cdots, X_{n}\right)$ of size $n$ is a sequence of random variables $X_{1}, \cdots, X_{n} .$ It can be viewed as an n-dimensional real-valued random vector, where the dimension $n$ may go to infinity. Its realization is an n-dimensional vector $\mathbf{x}^{n}=\left(x_{1}, \ldots, x_{n}\right) .$ A realization $\mathbf{x}^{\mathbf{n}}$ of $\mathrm{X}^{n}$ is usually called a sample point or a data set generated from the random sample $\mathrm{X}^{n}$.

Since $\mathrm{X}^{n}$ is a sequence of $n$ random variables, we can use the joint probability distribution of $\mathrm{X}^{n}$ to characterize the random sample. Put $\mathrm{x}^{i-1}=\left(x_{i-1}, \cdots, x_{1}\right)$ for $i=1,2, \cdots .$ Then repeatedly applying the multiplication rule, the joint PMF/PDF of X $^{n}$ 

$$
f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}\right)=\prod_{i=1}^{n} f_{X_{i}\left|\mathbf{X}^{i-1}\left(x_{i} | \mathbf{X}^{i-1}\right)\right.}
$$

where $f_{X_{i}\left|\mathbf{X}^{i-1}\left(X_{i} | \mathbf{X}^{i-1}\right)\right.}$ is the conditional PMF/PDF of $X_{i}$ given $\mathbf{X}^{i-1}=\mathbf{x}^{i-1} .$ By convention, $f_{X_{1} | \mathrm{X}^{0}}\left(x_{1} | x^{0}\right)=f_{X_{1}}\left(x_{1}\right)$ is the unconditional PMF/PDF of $X_{1}$.

When $\mathrm{X}^{n}$ is an IID random sample from a population $\mathrm{PMF} / \mathrm{PDF} f_{X}(\cdot)$, the joint $\mathrm{PMF} / \mathrm{PDF}$ of the random sample $\mathrm{X}^{n}$ is given by
$$
f_{\mathbf{X}^{n}}\left(\mathbf{x}^{n}\right)=\prod_{i=1}^{n} f_{X}\left(x_{i}\right)
$$

This joint probability distribution is called the sampling distribution of the random sample $\mathrm{X}^{n}$. It completely describes the probability law of the random sample $\mathrm{X}^{n}$.

For an IID random sample $X^{n}$, each $X_{i}$ has the same PMF/PDF $f_{X}(x)$, the so-called population, $f_{X}(x)$ is assumed to be a parametric model in the sense that $f_{X}(x)=f(x, \theta)$ for some value of a finite-dimensional parameter $\theta,$ where the functional form of $f(\cdot, \cdot)$ is known but $\theta$ is unknown. 

For example, if $f_{X}(x)$ is assumed to follow a normal $N\left(\mu, \sigma^{2}\right)$ distribution, we have

$$
\begin{aligned}
f_{X}(x) &=f(x, \theta) \\
&=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{1}{2 \vec{z}^{2}}(x-\mu)^{2}}
\end{aligned}
$$

where $\theta=\left(\mu, \sigma^{2}\right)$

One important objective of statistical analysis is to estimate the unknown parameter $\theta$ when we are given a data set $\mathrm{x}^{n}$, which is a realization of the random sample $\mathrm{X}^{n}$. An estimator for $\theta$ is a function of $\mathrm{X}^{n}$, and so it is a statistic. Recall that a statistic $Z_{n}=T\left(\mathrm{X}^{n}\right)$ is a function of $\mathrm{X}^{n}$ only and does not involve any unknown parameters. It is a random variable or vector.

To motivate the importance of various convergence concepts, we consider two simple statistic the sample mean and sample variance. Let $\mathrm{X}^{n}$ be an IID random sample of size $n,$ from a population distribution with mean $\mu$ and variance $\sigma^{2}$. Suppose the mean $\mu$ is unknown, so we need to use the sample information $\mathbf{X}^{n}$ to estimate it. It is expected that the sample mean estimator
$$
\bar{X}_{n}=T\left(\mathbf{X}^{n}\right)=n^{-1} \sum_{i=1}^{n} X_{i}
$$

can be used to estimate $\mu$. 

Similarly, suppose $\sigma^{2}$ is unknown. We can use the sample variance estimator
$$
S_{n}^{2}=(n-1)^{-1} \sum_{i=1}^{n}\left(X_{i}-\bar{X}_{n}\right)^{2}
$$

to estimate $\sigma^{2}$.

If the sample size $n$ is sufficiently large, then it is expected that $\bar{X}_{n}$ will be "close" to $\mu$ and $S_{n}^{2}$ will be "close" to $\sigma^{2}$. The larger the sample size $n$ is, the closer $\bar{X}_{n}$ is to $\mu$ and the closer $S_{n}^{2}$ is to $\sigma^{2}$.

**Question**: How can one measure the closeness of $\bar{X}_{n}$ to $\mu$ and the closeness of $S_{n}^{2}$ to $\sigma^{2}$ ?

Since $\bar{X}_{n}$ and $S_{n}^{2}$ are random variables, the convergence concepts for non-stochastic sequences reviewed in Section 7.1 do not apply.

Recall that both $\bar{X}_{n}$ and $S_{n}^{2}$ are mappings from $S$ to the real line; that is, $\bar{X}_{n}: S \rightarrow \mathbb{R}$ and $S_{n}^{2}: S \rightarrow \mathbb{R}^{+},$ where $S$ is the sample space of the underlying random experiment. Suppose a random experiment is conducted, a basic outcome $s \in S$ occurs. Then we observe a data set $\mathbf{x}^{n}=\left(x_{1}, \cdots, x_{n}\right),$ where $x_{i}=X_{i}(s)$. It is a realization of the random sample $\mathbf{X}^{n}$. From the data set $\mathbf{x}^{n},$ we can compute an estimate $\bar{x}_{n}=\bar{X}_{n}(s)$ for $\mu$ and an estimate $s_{n}^{2}=S_{n}^{2}(s)$ for $\sigma^{2}$ Different outcomes s will yield different estimates for $\mu$ and $\sigma^{2}$ respectively

In fact, each basic outcome $s \in S$ will generate a sequence of real numbers $\left\{\bar{x}_{n}=\bar{X}_{n}(s), n=\right.$ $1,2, \cdots\}$ and a sequence of real numbers $\left\{s_{n}^{2}=S_{n}^{2}(s), n=1,2, \cdots\right\}$ respectively. These non-stochastic sequences are called a sample path for the sample mean $\bar{X}_{n}$ and a sample path for the sample variance $S_{n}^{2}$ respectively, when a basic outcome $s$ occurs. There are many such sample paths for both $\bar{X}_{n}$ and $S_{n}^{2}$ respectively, as illustrated in Figure 7.1 below. These sample paths correspond to different basic outcomes $s \in S .$ 

We need to develop some suitable con concepts and distance measures between $\bar{X}_{n}$ and $\mu$, or between $S_{n}^{2}$ and $\sigma^{2}$. Intuitively, a common feature of these different convergence concepts is that they define that most non-stochastic sequences $\left\{\bar{X}_{n}(s), n=1,2, \cdots\right\}$ and $\left\{S_{n}^{2}(s), n=1,2, \cdots\right\}$ converge to $\mu$ and $\sigma^{2}$ respectively

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20200506175117.png)

## 7.3 Convergence in Quadratic Mean and $L_{p}$-Convergence

【Convergence in Quadratic Mean】Let $\left\{Z_{n}, n=1,2, \cdots\right\}$ be a sequence of random variables and $Z$ be a random variable. Then the sequence $\{Z_{n}\}$ converges in quadratic mean (or converges in mean square) to $Z$ if

$$
E\left(Z_{n}-Z\right)^{2} \rightarrow 0 \text { as } n \rightarrow \infty
$$

or equivalently
$$
\lim _{n \rightarrow \infty} E\left(Z_{n}-Z\right)^{2}=0
$$

denoted by $z_{n} \stackrel{q m}{\longrightarrow} z$ or $z_{n}-z=o_{q m}(1)$.

Remarks:

- The convergence in quadratic mean means that the weighted average of the squared deviations between $Z_{n}(s)$ and $Z(s)$ vanishes to 0 as $n \rightarrow \infty,$ where the average is taken over all possible basic outcomes $\{s\}$ weighted by their probabilities of of curing.
- When $Z_{n}$ converge to $Z$ in quadratic mean, it's possible that there exist some sample paths for which $Z_{n}(s)$ does not converge to $Z(s)$. However, the quadratic deviations of these sample paths all together weighted by the probabilities of their occurings become negligible as $n$ becomes large.

【Example】Suppose $X^{n}=\left(X_{1}, \cdots, \cdots, X_{n}\right)$ is an IID random sample from a population with mean $\mu$ and variance $\sigma^{2}$. Define $Z_{n}=\bar{X}_{n}$. Show

$$
\bar{x}_{n}\stackrel{q m}{\longrightarrow} \mu
$$

Solution: It suffices to show $\lim _{n-\infty} E\left(\bar{X}_{n}-\mu\right)^{2}=0 .$ Note $E\left(\bar{X}_{n}\right)=\mu,$ we have

$$
\begin{aligned}
E\left(\bar{X}_{n}-\mu\right)^{2} &=\operatorname{var}\left(\bar{X}_{n}\right) \\
&=\operatorname{var}\left(\frac{1}{n} \sum_{i=1}^{n} X_{i}\right) \\
&=\frac{1}{n^{2}} \operatorname{var}\left(\sum_{i=1}^{n} X_{i}\right) \\
&=\frac{1}{n^{2}} \sum_{i=1}^{n} \operatorname{var}\left(X_{i}\right) \\
&=\frac{\sigma^{2}}{n} \\
& \rightarrow 0 \text { as } n \rightarrow \infty
\end{aligned}
$$

【$L_{p}-\text{convergence}$】Let $0<p<\infty,$ and let $\left\{Z_{n}, n=1,2, \cdots\right\}$ be a sequence of random variables with $E\left|Z_{n}\right|^{p}<\infty,$ and let $Z$ be a random variable with $E|Z|^{p}<\infty .$ Then $Z_{n}$ converges in $L_{p}$ to $Z$ if

$$
\lim _{n \rightarrow \infty} E\left|Z_{n}-Z\right|^{p}=0
$$

Remarks:

In connection with $L_{p}$ -convergence, the following inequalities are useful:

- Holder's inequality

$$
E|X Y| \leq\left(E|X|^{p}\right)^{1 / p}\left(E|Y|^{q}\right)^{1 / q}
$$

where $p>1$ and $1 / p+1 / q=1$

- Minkowski's inequality

$$
E|X+Y|^{p} \leq\left[\left(E|X|^{p}\right)^{1 / p}+\left(E|Y|^{p}\right)^{1 / p}\right]^{p}
$$

for $p \geq 1$

Question: 

What happens if $Z_{n}$ and $Z$ are $d \times 1$ random vectors, where $d$ is fixed (i.e., $d$ does not change as $n \rightarrow \infty) ?$

- A sequence of random vectors $\left\{Z_{n}\right\}$ converges to $Z$ in $L_{p},$ if each component of the vector $Z_{n}, Z_{i n} \stackrel{L_{p}}{\rightarrow} Z_{i}$ for $i=1, \cdots, d$. In other words, component-wise convergences ensure joint convergence of the entire vector $Z_{n},$ and vice versa


## 7.4 Convergence in Probability

### 7.4.1 Definition

【Convergence in Probability / weak Convergence】A sequence of random variables $\left\{Z_{n}, n=\right.$ $1,2, \cdots\}$ converges in probability to a random variable $Z$ if for every small constant $\epsilon>0$
$$
P\left[\left|Z_{n}-Z\right|>\epsilon\right] \rightarrow 0 \text { as } n \rightarrow \infty
$$
When $Z_{n}$ converges in probability to $Z$, we write
$$
\lim _{n \rightarrow \infty} P\left(\left|Z_{n}-Z\right|>\epsilon\right)=0
$$
for every $\epsilon>0,$ or $p \lim _{n \rightarrow \infty} Z_{n}=Z,$ or $Z_{n} \stackrel{p}{\rightarrow} Z,$ or $Z_{n}-Z=o_{P}(1),$ or $Z_{n}-Z \stackrel{p}{\rightarrow} 0$

Remarks:

- An alternative definition of convergence in probability: Given any $\epsilon>0$ and any $\delta>0,$ there exists a finite integer $N=N(\epsilon, \delta)$ such that for all $n>N,$ we have

$$
P\left(\left|Z_{n}-Z\right|>\epsilon\right)<\delta
$$

- Define $A_{n}(\epsilon)=\left\{s \in S:\left|Z_{n}(s)-Z(s)\right| \leq \epsilon\right\}$ as a subset of $S$ that consists of all basic outcomes $s \in S$ such that the difference $\left|Z_{n}(s)-Z(s)\right|$ is small. $Z_{n} \stackrel{p}{\rightarrow} Z$ means that the probability of "small deviations"

$$
P\left[\left|Z_{n}-Z\right|\right.\leq \epsilon]=P\left[A_{n}(\epsilon)\right] \rightarrow 1 \text { as } n \rightarrow \infty
$$

- For this reason, convergence in probability is also called **convergence with probability approaching 1**.

- When $Z_{n} \stackrel{p}{\rightarrow} b,$ where $b$ is a constant, we say that $Z_{n}$ is **consistent** for $b$.

【Order of Convergence in Probability】Suppose $\left\{f_{n}>0, n=1,2, \ldots\right\}$ is a sequence of real numbers

A sequence of random variables $\left\{Z_{n}, n=1,2, \ldots\right\}$ is of order smaller than $f_{n}$ in probability, denoted by $Z_{n}=o_{p}\left(f_{n}\right)$, if
$$
Z_{n} / f_{n} \stackrel{p}{\rightarrow} 0 \quad \text{as } n \rightarrow \infty
$$

A sequence of random variables $\left\{Z_{n}, n=1,2, \ldots\right\}$ is at most of order $f_{n}$ in probability, denoted by $Z_{n}=\mathcal{O}_{p}\left(f_{n}\right)$, if for every $\varepsilon>0,$ there exists a constant $M(\varepsilon)<\infty$ and an integer $N(\varepsilon)$ such that 
$$
P\left(\left|Z_{n} / f_{n}\right|>M(\varepsilon)\right)<\varepsilon
$$

for all $n \geqslant N(\varepsilon)$

Remark: 

- A sequence of random variables $\left\{Z_{n}, n=1,2, \ldots\right\}$ is **bounded in probability** if $Z_{n}=\mathcal{O}_{p}(1),$ that is, for any $\varepsilon>0,$ there exists a constant $M(\varepsilon)<\infty$ and an integer $N(\varepsilon)$ such that $P\left(\left|Z_{n}\right|>M(\varepsilon)\right)<\varepsilon$ for all $n \geqslant N(\varepsilon)$
- Let $\left\{X_{n}\right\}$ and $\left\{Y_{n}\right\}$ be two sequences of random variables. 
- If $X_{n}=\mathcal{O}_{p}\left(n^{\lambda}\right)$ and $Y_{n}=\mathcal{O}_{p}\left(n^{\tau}\right),$ then

$$
X_{n} Y_{n}=\mathcal{O}_{p}\left(n^{\lambda+\tau}\right), \quad X_{n}+Y_{n}=\mathcal{O}_{p}\left(n^{\max (\lambda, \tau)}\right)
$$

- If $X_{n}=o_{p}\left(n^{\lambda}\right)$ and $Y_{n}=o_{p}\left(n^{\tau}\right),$ then

$$
X_{n} Y_{n}=o_{p}\left(n^{\lambda+\tau}\right), \quad X_{n}+Y_{n}=o_{p}\left(n^{\max (\lambda, \tau)}\right)
$$

- If $X_{n}=\mathcal{O}_{p}\left(n^{\lambda}\right)$ and $Y_{n}=o_{p}\left(n^{\tau}\right),$ then

$$
X_{n} Y_{n}=o_{p}\left(n^{\lambda+\tau}\right), \quad X_{n}+Y_{n}=\left\{\begin{array}{ll}
\mathcal{O}_{p}\left(n^{\lambda}\right) & \text { if } \lambda \geqslant \tau \\
o_{p}\left(n^{\tau}\right) & \text { if } \lambda<\tau
\end{array}\right.
$$

### 7.4.2 Markov's Inequality

【Markov's Inequality】Suppose $X$ is a random variable and $g(X)$ is a nonnegative function. Then for any $\epsilon>0,$ and any $k>0,$ we have
$$
P[g(X) \geq \epsilon] \leq \frac{E\left[g(X)^{k}\right]}{\epsilon^{k}}
$$

**Proof:**

Let $\mathbf{1}(\cdot)$ be the indicator function that takes values 1 and $0,$ depending on whether the statement is true. Then
$$
\begin{aligned}
P[g(X)>\epsilon] 
&=\int_{\{x : g(x)>\varepsilon\}} d F_{X}(x) \\
&=\int_{-\infty}^{\infty} 1[g(x)>\epsilon] d F_{X}(x) \\
& \leq \int_{-\infty}^{\infty} 1[g(x)>\epsilon] \frac{g(x)^{k}}{\epsilon^{k}} d F_{X}(x) \\
& \leq \int_{-\infty}^{\infty} \frac{g(x)^{k}}{\epsilon^{k}} d F_{X}(x) \\
&=\frac{1}{\epsilon^{k}} E\left[g(X)^{k}\right]
\end{aligned}
$$

Remarks:

- Markov's inequality bounds the **tail probability by a moment condition**. The thickness of the tail probability depends on the magnitude of moments of the distribution.
- When $k = 2$ and $g(X) = |X-\mu|$; Markov's inequality is called Chebyshev's inequality, i.e.

$$
P[|X-\mu|\geq \epsilon] \leq \frac{E\left[(X-\mu)^{2}\right]}{\epsilon^{k}}
$$

### 7.4.3 WLLN

【Weak Law of Large Numbers(WLLN)】Let $\mathrm{X}^{n}=\left(X_{1}, \cdots, X_{n}\right)$ be an IID random sample with $E\left(X_{i}\right)=\mu$ and var $\left(X_{i}\right)=\sigma^{2}<\infty .$ Define $\bar{X}_{n}=n^{-1} \sum_{i=1}^{n} X_{i} .$ Then 
$$
\bar{X}_{n} \stackrel{p}{\rightarrow} \mu
$$
**Proof:**

First, noting $E\left(\bar{X}_{n}\right)=\mu$ and $\operatorname{var}\left(\bar{X}_{n}\right)=\sigma^{2} / n,$ we have by Chebychev's inequality
$$
\begin{aligned}
P\left[\left|\bar{X}_{n}-\mu\right|\right.&>\epsilon] \\
& \leq \frac{E\left(\bar{X}_{n}-\mu\right)^{2}}{\epsilon^{2}} \\
&=\frac{\operatorname{var}\left(\bar{X}_{n}\right)}{\epsilon^{2}} \\
&=\frac{\sigma^{2}}{n \epsilon^{2}}
\end{aligned}
$$

It follows that
$$
\begin{aligned}
P\left[\left|\bar{X}_{n}-\mu\right|\right.&\leq \epsilon] \\
&=1-P\left[\left|\bar{X}_{n}-\mu\right|>\epsilon\right] \\
& \geq 1-\frac{\sigma^{2}}{n \epsilon^{2}} \\
& \rightarrow 1
\end{aligned}
$$

as $n \rightarrow \infty .$ Therefore, $\bar{X}_{n} \stackrel{p}{\rightarrow} \mu$

Remarks: 

- In the WLLN theorem, we have assumed a finite variance. Although such an assumption is true and desirable in most applications, it is, in fact, a stronger assumption than is needed. The only moment condition needed is that $E\left|X_{i}\right|<\infty$ (see Resnik 1999, Chapter 7 , or Billingsley $1995,$ Section 22 )

- To appreciate why a **moment condition** is needed for the WLLN, one can consider the example of an IID random sample from the Cauchy distribution whose moments do not exist.

### 7.4.4 Relationships Between Convergence

【Lemma 2】Suppose $Z_{n} \rightarrow Z$ in $L_{p}$ as $n \rightarrow \infty .$ Then $Z_{n} \stackrel{p}{\rightarrow} Z$ as $n \rightarrow \infty$.

**Proof:** 

By Markov's inequality, we have for all $\epsilon>0$
$$
P\left[\left|Z_{n}-Z\right|\right.>\epsilon] \leq \frac{E\left|Z_{n}-Z\right|^{p}}{\epsilon^{p}} \rightarrow 0
$$

if $\lim _{n-\infty} E\left|Z_{n}-Z\right|^{p}=0$.

【Example】Suppose $\mathbf{X}^{n}=\left(X_{1}, \cdots, X_{n}\right)$ is and IID $N \left(\mu, \sigma^{2}\right)$ random sample. Show
$$
S_{n}^{2} \stackrel{p}{\longrightarrow} \sigma^{2}
$$
**Solution:**

We have shown that under a normal random sample $\mathrm{X}^{n}$
$$
\frac{(n-1) S_{n}^{2}}{\sigma^{2}} \sim \chi_{n-1}^{2}
$$

for all $n>1$

It follows that $E\left(S_{n}^{2}\right)=\sigma^{2}$ and $\operatorname{var}\left(S_{n}^{2}\right)=2 \sigma^{4} /(n-1) .$ 

Hence, we have
$$
\begin{aligned}
E\left(S_{n}^{2}-\sigma^{2}\right)^{2} &=\operatorname{var}\left(S_{n}^{2}\right) \\
&=\frac{2 \sigma^{4}}{n-1} \\
& \rightarrow 0 \text { as } n \rightarrow \infty
\end{aligned}
$$

Thus $S^2_n \rightarrow \sigma^2$ in $L_{p}$. It follows that $S_{n}^{2} \stackrel{p}{\rightarrow} \sigma^{2}$ as $n \rightarrow \infty$

But $Z_{n} \stackrel{p}{\rightarrow} Z$ does not imply $Z_{n} \rightarrow Z$ in $L_{p}$.

【Example】Suppose a sequence of binary random variables $\left\{Z_{n}\right\}$ is defined as
$$
\begin{array}{c|cc}
Z_{n} & \frac{1}{n} & n \\
\hline f_{Z_{n}}\left(z_{n}\right) & 1-\frac{1}{n} & \frac{1}{n}
\end{array}
$$

(1) Does $Z_{n}$ converge in quadratic mean to 0? Give your reasoning clearly
(2) Does $Z_{n}$ converge in probability to o? Give your reasoning clearly.

**Solution:**
(1) $Z_{n}$ does not converge in quadratic mean to 0 because
$$
\begin{aligned}
E\left(Z_{n}-0\right)^{2} &=\sum\left(z_{n}-0\right)^{2} f_{Z_{n}}\left(z_{n}\right) \\
&=n^{-2}\left(1-n^{-1}\right)+n^{2}\left(n^{-1}\right)\\
&>n \to \infty
\end{aligned}
$$

(2) Given any $\epsilon>0,$ for all $n>N(\epsilon)=\left[\epsilon^{-1}\right]+1\left(\text { so } n^{-1}<\epsilon\right)$
$$
\begin{aligned}
P\left(\left|Z_{n}-0\right|\right.\leq \epsilon) &=P\left(Z_{n}=n^{-1}\right) \\
&=1-n^{-1} \\
& \rightarrow 1 \text { as } n \rightarrow \infty
\end{aligned}
$$

Therefore, $Z_{n}$ converges to 0 in probability

Remarks:

- The square of $Z_{n}$ grows to infinity at a rate of $n^{2}$, which is faster than the rate $n^{-1}$ of the vanishing probability $P\left(Z_{n}=n\right) .$ As a result, the second moment does not exist.
- More generally, convergence in probability implies that there may exist a set of outcomes in sample space with "large deviations" and the probability of this set shrinks to zero as $n \rightarrow \infty$ but it is nonzero for a finite $n$. Some large deviations may be even explosive at a rate faster than their shrinking probabilities. As a result, the $L_{p}$ -convergence may not exist.

### 7.4.5 Continuous Mapping Theorem

【Continuous Mapping Theorem】Suppose $g$ (.) is a continuous function, and $Z_{n}\stackrel{p}{\rightarrow} Z$. Then $g\left(Z_{n}\right)\stackrel{p}{\rightarrow}g(Z)$. Or equivalently $p \lim g\left(Z_{n}\right)=g\left(p \lim Z_{n}\right)$.

**Proof:**
By continuity of function $g(\cdot):$ given any $\epsilon>0,$ there exists a constant $\delta=\delta(\epsilon),$ such that whenever $\left|Z_{n}-Z\right| \leq \delta$, we have
$$
\left|g\left(Z_{n}\right)-g(Z)\right|<\epsilon
$$

Now, define two events
$$
\begin{array}{l}
A_{n}(\delta) \equiv\left\{s \in S:\left|Z_{n}(s)-Z(s)\right| \leq \delta\right\} \\
B_{n}(\epsilon) \equiv\left\{s \in S:\left|g\left[Z_{n}(s)\right]-g[Z(s)]\right| \leq \epsilon\right\}
\end{array}
$$

Then continuity of $g(\cdot)$ implies $A_{n}(\delta) \subseteq B_{n}(\epsilon),$ that is, $A_{n}(\delta)$ is a subset of $B_{n}(\epsilon)$

It follows that $P\left[A_{n}(\delta)\right] \leq P\left[B_{n}(\epsilon)\right],$ and so
$$
P\left[B_{n}(\epsilon)^{c}\right] \leq P\left[A_{n}(\delta)^{c}\right] \rightarrow 0 \text { as } n \rightarrow \infty
$$

where $B_{n}(\epsilon)^{c}$ and $A_{n}(\delta)^{c}$ are the complements of $B_{n}(\epsilon)$ and $A_{n}(\delta)$ respectively. Because $\epsilon$ is arbitrary, so is $\delta .$ It follows that $g\left(Z_{n}\right) \stackrel{p}{\rightarrow} g(Z)$ as $n \rightarrow \infty$.

**Remarks:**

- The $p \text{ lim}$ operator passes through nonlinear functions, provided they are continuous.
- This is analogous to the well-known result in calculus that the limit of a continuous function is equal to the function of the limit.

- The expectation operator $E(\cdot),$ which is used in the $L_{p}$-convergence, does not have this feature, and this makes finite sample analysis difficult for many statistics.

### 7.4.6 Continuity

【Lemma 7.3】**Continuity**: Suppose $ a_{n} \stackrel{p}{\rightarrow} $ a and $ b_{n} \stackrel{p}{\rightarrow} b, $ and $ g(\cdot) $ and $ h(\cdot) $ are continuous functions. Then
$$
\begin{array}{l}g\left(a_{n}\right)+h\left(b_{n}\right) \stackrel{p}{\rightarrow} g(a)+h(b), \text { and } \\ g\left(a_{n}\right) h\left(b_{n}\right) \stackrel{p}{\rightarrow} g(a) h(b)\end{array}
$$

## 7.5 Almost Sure Convergence

### 7.5.1 Definition

【Almost Sure Convergence / strong convergence】A sequence of random variables $\left\{Z_{n}\right\}$ converges almost surely to a random variable $Z$ if for every given constant $\epsilon>0$
$$
P\left[\lim _{n \rightarrow \infty}\left|Z_{n}-Z\right|>\epsilon\right]=0
$$

or
$$
P\left[s \in S: \lim _{n \rightarrow \infty}\left|Z_{n}(s)-Z(s)\right| \leq \epsilon\right]=1
$$

or equivalently
$$
P\left[s \in S: \lim _{n \rightarrow \infty}Z_{n}(s)=Z(s)\right]=1
$$
where $S$ is the sample space. When $Z_{n}$ converges almost surely to $Z$, we write $Z_{n} \stackrel{\text { a.s. }}{\rightarrow} Z$ or $Z_{n}-Z=o_{a . s .}(1),$ or $Z_{n}-Z \stackrel{\text { a.s. }}{\rightarrow} 0$

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20200507102251.png)

**Remarks:**

Almost sure convergence can also be expressed as
$$
P\left(\lim _{n \rightarrow \infty}\left|Z_{n}-Z\right|=0\right)=1
$$

For this reason, almost sure convergence is also called convergence with probability 1

When $Z=b,$ a constant, we say that $Z_{n}$ is **strongly consistent** for $b$ if $Z_{n}$ converges to $b$ with probability one.

Define a set
$$
\begin{aligned}
A(\epsilon) &=\left\{s \in S: \lim _{n \rightarrow \infty}\left|Z_{n}(s)-Z(s)\right| \leq \epsilon\right\} \\
&=\left\{s \in S:\left|Z_{n}(s)-Z(s)\right| \leq \epsilon \text { for all } n>N(\epsilon, s)\right\}
\end{aligned}
$$

i.e. $A(\epsilon)$ is a subset of $S$ that consists of all basic outcomes $s \in S$ such that $\lim _{n \rightarrow \infty}\left|Z_{n}(s)-Z(s)\right|<\epsilon .$ Intuitively $A(\epsilon)$ is a convergence set in $S$ in the sense that for each $s \in A(\epsilon),$ the sample path $\left\{Z_{n}(s)\right\}$ converges to $Z(s)$ as $n \rightarrow \infty$. Then
$$
\begin{aligned}
P\left(\lim _{n \rightarrow \infty}\left|Z_{n}-Z\right| \leq \epsilon\right) &=P[A(\epsilon)] \\
&=1
\end{aligned}
$$

【Example】Suppose the probability space is $(S, \mathcal{B}, P),$ where the sample space $S=[0,1], \mathcal{B}$ contains all the Borel sets $B$ such that $B \subset S, P$ is the Lebesgue measure on $S$, i.e. $P(\{s \in[a, b]\})=b-a$ for any $0 \leqslant a \leqslant b \leqslant 1 .$ Then $Z_{n}(s)=s+s^{n}, n=1,2, \ldots$ and $Z(s)=s$ are random variables.
- For $0 \leqslant s<1, Z_{n}(s) \rightarrow Z(s)$ as $n \rightarrow \infty$
- For $s=1, \lim _{n \rightarrow \infty} Z_{n}(s)=2 \neq Z(s)$
- $P\left(\left\{s \in S: Z_{n}(s) \rightarrow Z(s)\right\}\right)=P([0,1))=1,$ so $Z_{n} \stackrel{a . s,}{\longrightarrow} Z$

### 7.5.2 Order of Almost sure convergence

【Almost sure convergence with order $n^{\alpha}$】, where $\alpha$ can be positive or negative:

The sequence of random variables $\left\{Z_{n}\right\}$ is said to be of order smaller than $n^{\alpha}$ with probability one if
$$
Z_{n} / n^{\alpha} \stackrel{a. s.}{\longrightarrow} 0 \text { as } n \rightarrow \infty
$$

This is denoted as $Z_{n}=o_{a, s}\left(n^{\alpha}\right)$

The sequence of random variables $\left\{Z_{n}\right\}$ is said to be at most of order $n^{\alpha}$ with probability one if there exists some constant $M<\infty$ such that
$$
P\left(\lim _{n \rightarrow \infty}\left|Z_{n} / n^{\alpha}\right|>M\right)=0
$$

This is denoted as $Z_{n}=O_{a, s}\left(n^{\alpha}\right)$

Remarks:
$Z_{n}=O_{a, s}(1)$ implies that with probability one, $Z_{n}$ is bounded by some large constant for all $n$ sufficiently large.

### 7.5.3 Relationships Between Convergence

【Lemma 7.4】Suppose $ Z_n  \stackrel{a. s.}{\longrightarrow} Z $, then $Z_n  \stackrel{p.}{\rightarrow} Z$.

$Z_{n} \stackrel{\text {a.s.}}{\longrightarrow} Z$ means for any $\varepsilon>0$
$$
\begin{array}{l}P\left(\left\{s \in S: \lim _{n \rightarrow \infty} Z_{n}(s)=Z(s)\right\}\right) \\\quad=P\left(\left\{s \in S:\left|Z_{n}(s)-Z(s)\right| \leqslant \varepsilon \text { for all } n>N(\varepsilon, s)\right\}\right)=1\end{array}
$$

$Z_{n} \stackrel{p}{\rightarrow} Z$ means for any $\varepsilon>0$
$$
P\left(\left\{s \in S:\left|Z_{n}(s)-Z(s)\right| \leqslant \varepsilon\right\}\right) \rightarrow 1
$$

We first compare the difference in notations between almost sure convergence
$$
P\left[\lim _{n \rightarrow \infty}\left|Z_{n}-Z\right|>\epsilon\right]=0
$$

and convergence in probability
$$
\lim _{n \rightarrow \infty} P\left[\left|Z_{n}-Z\right|>\epsilon\right]=0
$$

The fact that the set $A_{n}(\epsilon)$ of "large" differences for $\left|Z_{n}-Z\right|$ may have a nonzero probability for any finite $n$ implies that convergence in probability is weaker than almost sure convergence.

If for some $s \in S, Z_{n}(s) \rightarrow Z(s)$ as $n \rightarrow \infty,$ then the difference $\left|Z_{n}(s)-Z(s)\right|$ will eventually become "small" (i.e., smaller than $\epsilon$ ) for all $n$ sufficiently large. Hence, almost sure convergence implies convergence in probability.

While almost sure convergence implies convergence in probability, the converse may not be true.
$$
Z_{n} \stackrel{p}{\rightarrow} Z \nRightarrow Z_{n} \stackrel{a . s .}{\longrightarrow} Z
$$
【Example】Suppose the probability space is $(S, \mathcal{B}, P),$ where the sample space $S=[0,1], \mathcal{B}$ contains all the Borel sets $B$ such that $B \subset S, P$ is the Lebesgue measure on $S$. Let $Z(s)=0$ and $Z_{1}(s)=1(0<s \leqslant 1)$ where $1(\cdot)$ is the indicator function $Z_{2}(s)=1\left(0<s \leqslant \frac{1}{2}\right), Z_{3}(s)=1\left(\frac{1}{2}<s \leqslant 1\right)$
$Z_{n}(s)=1\left(\frac{n-4}{4}<s \leqslant \frac{n-3}{4}\right), n=4, \ldots, 7$
$Z_{n}(s)=1\left(\frac{n-8}{8}<s \leqslant \frac{n-7}{8}\right), n=8, \ldots, 15, \cdots$

- For every $\varepsilon>0, P\left(\left|Z_{n}-Z\right|>\varepsilon\right) \rightarrow 0$ as $n \rightarrow \infty,$ so $Z_{n} \stackrel{p}{\rightarrow} Z$
- $E\left|Z_{n}-Z\right|^{p}=1 / 2^{k} \rightarrow 0$ as $n \rightarrow \infty,$ where $k=\left[\log _{2} n\right],$ so
    $Z_{n} \rightarrow Z$ in $L_{p}$

- For any $s>0, \lim _{n \rightarrow \infty} Z_{n}$ does not exist, so $Z_{n}$ does not converge to $Z$ almost surely.

Almost sure convergence does not imply $L_p$-convergence and $L_p$-convergence does not imply almost sure convergence.

$$ in $$ in $L_{p}$
$$
Z_{n} \rightarrow Z \text{ in }L_{p} \nRightarrow Z_{n} \stackrel{\text { a.s. }}{\longrightarrow} Z, \quad Z_{n} \stackrel{\text { a.s. }}{\longrightarrow} Z \neq Z_{n} \rightarrow Z \text{ in }L_{p} 
$$
【Example】Suppose the probability space is $(S, \mathcal{B}, P),$ where the sample space $S=[0,1], \mathcal{B}$ contains all the Borel sets $B$ such that $B \subset S, P$ is the Lebesgue measure on $S$. Let $Z(s)=0$ and
$$
Z_{n}(s)=\left\{\begin{array}{ll}
0 & \text { if } \frac{1}{n}<s \leqslant 1 \\
e^{n} & \text { if } 0 \leqslant s \leqslant \frac{1}{n}
\end{array}\right.
$$

- For every $\varepsilon>0, P\left(\left|Z_{n}(s)-Z(s)\right|>\varepsilon\right) \leqslant 1 / n$ as $n \rightarrow \infty,$ so
    $Z_{n} \stackrel{p}{\rightarrow} Z$

- For any $0<s \leq 1, \lim _{n \rightarrow \infty} Z_{n}(s)=0,$ so
$P\left(\left\{s \in S: Z_{n}(s) \rightarrow Z(s)\right\}\right)=P((0,1])=1$ and $Z_{n} \stackrel{\text { a.s. }}{\longrightarrow} Z$

- $E\left|Z_{n}-Z\right|^{p}=\frac{1}{n} e^{n p} \rightarrow \infty,$ so $Z_{n}$ does not converge to $Z$ in $L_{p}$

### 7.5.4 Continuous Mapping Theorem

【Continuous Mapping Theorem】Suppose $g(\cdot)$  is a continuous function, and $Z_{n}\stackrel{a.s.}{\rightarrow} Z$. Then $g\left(Z_{n}\right)\stackrel{a.s.}{\rightarrow}g(Z)$ as $n \to \infty$

**Proof:**

The proof is similar to the proof of convergence in probability of a continuous function.

Let $s \in S$ be a basic outcome. since $Z_{n}(s) \rightarrow Z(s)$ as $n \rightarrow \infty$ implies $g\left[Z_{n}(s)\right] \rightarrow$ $g[Z(s)]$ as $n \rightarrow \infty$ by the continuity of $g(\cdot),$ we have
$$
\left\{s \in S: Z_{n}(s) \rightarrow Z(s)\right\} \subseteq\left\{s \in S: g\left[Z_{n}(s)\right] \rightarrow g[Z(s)]\right\}
$$

Hence
$$
P\left[s \in S: g\left[Z_{n}(s)\right] \rightarrow g[Z(s)]\right] \geq P\left[s \in S: Z_{n}(s) \rightarrow Z(s)\right] \rightarrow 1
$$

It follows that $g\left(Z_{n}\right) \stackrel{\text {a.s.}}{\rightarrow} g(Z)$

### 7.5.5 Continuity

【Lemma 7.3】**Continuity**: Suppose $ a_{n} \stackrel{a.s.}{\rightarrow} $ a and $ b_{n} \stackrel{a.s.}{\rightarrow} b, $ and $ g(\cdot) $ and $ h(\cdot) $ are continuous functions. Then
$$
\begin{array}{l}g\left(a_{n}\right)+h\left(b_{n}\right) \stackrel{a.s.}{\rightarrow} g(a)+h(b), \text { and } \\ g\left(a_{n}\right) h\left(b_{n}\right) \stackrel{a.s.}{\rightarrow} g(a) h(b)\end{array}
$$

### 7.5.6 SLLN & USLLN

【Kolmogorov's Strong Law of Large Numbers(SLLN)】Suppose $\mathrm{X}^{n}=$ $\left(X_{1}, \cdots, X_{n}\right)$ be an IID random sample with $E\left(X_{i}\right)=\mu$ and $E\left|X_{i}\right|<\infty .$ Define $\bar{X}_{n}=n^{-1} \sum_{i=1}^{n} X_{i}$, then
$$
\bar{X}_{n} \stackrel{\text {a.s.}}{\longrightarrow} \mu \text { as } n \rightarrow \infty
$$

Proof: See (e.g.) Gallant (1997, pp.132-135).

【Uniform Strong Law of Large Numbers (USLLN)】Suppose

1. $ \mathrm{X}^{n}=\left(X_{1}, \cdots, X_{n}\right)$ is an IID random sample
2. function $g(x, \theta)$ is continuous over $\Omega \times \Theta$ where $\Omega$ is the support of $X_{i}$ and $\Theta$ is a compact set in $\mathbb{R}^{d}$ with $d$ finite and fixed;
3. $ E\left[\text { sup }_{\theta \in \Theta} \left| g\left(X_{i}, \theta\right)\right|\right]<\infty$, where the expectation $E(\cdot)$ is taken over the population distribution of $X_{i}$. 

Then as $n \to \infty$,

$$
\sup_{\theta \in \Theta}\left| n^{-1} \sum_{i=1}^{n} g\left(X_{i}, \theta\right)-E\left[g\left(X_{i}, \theta\right)\right] \right|\rightarrow 0 \text { almost surely.}
$$

Moreover, $E\left[g\left(X_{i}, \theta\right)\right]$ is a continuous function of $\theta$ over $\Theta$.

**Remarks:**

Different from the SLLN, $g\left(X_{i}, \theta\right)$ depends on both random variable $X_{i}$ and parameter $\theta$.

USLLN says that, for each $n$, the worst deviation of the sample average $n^{-1} \sum_{i=1}^{n} g\left(X_{i}, \theta\right)$ from the population mean $E\left[g\left(X_{i}, \theta\right)\right]$ that one could find over all possible values in $\Theta$ converges to zero almost surely. The uniform convergence is with respect to the parameter space $\Theta$.

USLLN is rather useful when investigating the asymptotic behavior of nonlinear metric estimators

## 7.6 Convergence in Distribution

### 7.6.1 Definition

【Convergence in Distribution】Let $\left\{Z_{n}\right\}$ be a sequence of random variables with a sequence of corresponding CDF$\left\{F_{n}(z)\right\},$ and let $Z$ be a random variable with $\operatorname{CDF} F(z) .$ Then $Z_{n}$ converges in distribution to $Z$ as $n \rightarrow \infty$ if the CDF $ F_{n}(z)$ converges to $F(z)$ at every continuity point $t \in(-\infty, \infty),$ namely
$$
\lim _{n \rightarrow \infty} F_{n}(z)=F(z)
$$

at every point $z$ where $F_{n}(z)$ is continuous. Here, $F(z)$ is called a **limiting or asymptotic distribution** of the sequence of random variables $\left\{Z_{n}\right\}$. Convergence in distribution is denoted
as $Z_{n} \stackrel{d}{\longrightarrow} Z$ as $n \rightarrow \infty$.

Remarks:
- Although we refer to a sequence of random variables, $\left\{Z_{n}\right\}$, converging in distribution to a random variable $Z$, it is actually the sequence of CDF's $\left\{F_{n}(\cdot), n=1,2, \ldots\right\}$ that converges to the CDF $F(\cdot)$. In other words, convergence in distribution means that their CDF's converge, not the random variables $\left\{Z_{n}\right\}$ themselves.
- This is different from the concepts of convergence in $L_{p},$ convergence in probability and almost sure convergence. The latter all characterize the convergence or closeness of the random variables $\left\{Z_{n}\right\}$ to random variable $Z$.
- The distribution $F(z)$ is called the limiting distribution or asymptotic distribution of $Z_{n} .$ Suppose $F(\cdot)$ has mean $\mu$ and variance $\sigma^{2} .$ Then they will be called, respectively, the asymptotic mean and asymptotic variance of the distribution $F_{n}(\cdot) .$ since $F(\cdot)$ is not the limit of $F_{n}(\cdot),$ the asymptotic mean and variance may not be the limits of the mean and variances of the distribution $F_{n}(\cdot)$ respectively, even if the latter exist.

It should be emphasized that the limiting distribution $F(\cdot)$ might not be obtained by taking the limit of $F_{n}(\cdot) .$ For example, suppose $Z_{n} \sim N\left(0, \frac{1}{n}\right) .$ Then it has the distribution function
$$
\begin{aligned}
F_{n}(z) &=\int_{-\infty}^{z} \frac{1}{\sqrt{1 / n} \sqrt{2 \pi}} e^{-n u^{2} / 2} d u \\
&=\int_{-\infty}^{\sqrt{n} z} \frac{1}{\sqrt{2 \pi}} e^{-v^{2} / 2} d v \\
&=\Phi(\sqrt{n} z)
\end{aligned}
$$

where $\Phi(\cdot)$ is the $N(0,1)$ CDF. Obviously, we have
$$
\lim _{n \rightarrow \infty} F_{n}(z)=\left\{\begin{array}{ll}
0 & \text { if } z<0 \\
\frac{1}{2} & \text { if } z=0 \\
1 & \text { if } z>0
\end{array}\right.
$$

Now define the function
$$
F(z)=\left\{\begin{array}{ll}
0 & \text { if } z<0 \\
1 & \text { if } z \geq 0
\end{array}\right.
$$

Then $F(z)$ is a CDF and $\lim _{n \rightarrow \infty} F_{n}(z)=F(z)$ at every continuity point of $F(z)$. (The function $F(z)$ is not continuous at point $z=0 .$ ) Therefore, $F(\cdot)$ is a limiting distribution of $Z_{n} .$ However $F(\cdot)$ cannot be obtained by taking the limit of $F_{n}(\cdot),$ because $\lim _{n \rightarrow \infty} F_{n}(0) \neq F(0)$ at zero. Note that in this example $\lim _{n \rightarrow \infty} F_{n}(z)$ is not a CDF because it is not right-continuous.

【Example】Suppose $\mathrm{X}^{n}=\left(X_{1}, \cdots, X_{n}\right)$ is an IID $U[0, \theta]$ random sample, where $\theta$ is an unknown parameter. Let $Z_{n}=\max _{1 \leq i \leq n}\left(X_{i}\right)$ be an estimator of $\theta$. Derive the limiting distribution of $n\left(\theta-Z_{n}\right)$

**solution:**

For any given $u \geq 0,$ we have
$$
\begin{aligned}
P\left[n\left(\theta-Z_{n}\right)>u\right] &=P\left(Z_{n}<\theta-\frac{u}{n}\right) \\
&=P\left(X_{1}<\theta-\frac{u}{n}, \cdots, X_{n}<\theta-\frac{u}{n}\right) \\
&=\prod_{i=1}^{n} P\left(X_{i}<\theta-\frac{u}{n}\right) \\
&=\left(1-\frac{u}{n \theta}\right)^{n} \\
& \rightarrow e^{-u / \theta}
\end{aligned}
$$

as $n \rightarrow \infty,$ where we have used the formula $\left(1-\frac{a}{n}\right)^{n} \rightarrow e^{-a}$ as $n \rightarrow \infty .$ It follows that, for $u \geq 0$
$$
\begin{aligned}
F_{n}(u) &=1-P\left[n\left(\theta-Z_{n}\right)>u\right] \\
& \rightarrow 1-e^{-u / \theta}
\end{aligned}
$$

as $n \rightarrow \infty$. This implies that $n\left(\theta-Z_{n}\right)$ converges in distribution to the exponential distribution with parameter $\theta,$ denoted as $\operatorname{EXP}(\theta)$.

### 7.6.2 Relationships Between Convergence

【Lemma 7.5】Let $Z_{n}$ be a random variable with CDF $ F_{n}(\cdot),$ and let $Z$ be a random variable with a continuous $\operatorname{CDF} F(\cdot) .$ If $Z_{n} \stackrel{d}{\rightarrow} Z$ as $n \rightarrow \infty,$ then $Z_{n}=O_{P}(1)$

**Proof:**

For any given constant $\epsilon>0,$ let $M=M(\epsilon)$ be a (large) constant such that $P(|Z|>$ $M)<\epsilon .$ Let $F_{n}(z)$ be the CDF of $Z_{n} .$ Given $Z_{n} \stackrel{d}{\rightarrow} Z,$ and $F(z)$ is continuous everywhere, we have $\left|F_{n}(z)-F(z)\right| \leq \epsilon$ for any point $z \in(-\infty, \infty)$ and for all $n$ sufficiently large. This implies that for all $n$ sufficiently large, we have
$$
\begin{array}{l}
P\left(Z_{n}>M\right)-P(Z>M) \leq \epsilon \\
P\left(Z_{n} \leq-M\right)-P(Z \leq-M) \leq \epsilon
\end{array}
$$

It follows that
$$
\begin{aligned}
P\left(Z_{n}>M\right)+P\left(Z_{n}<-M\right) & \leq P\left(Z_{n}>M\right)+P\left(Z_{n} \leq-M\right) \\
&<P(Z>M)+P(Z \leq-M)+2 \epsilon \\
&=P(Z>M)+P(Z<-M)+2 \epsilon
\end{aligned}
$$

where $P(Z=-M)=0$ given that $Z$ follows a continuous distribution. Therefore,
$$
P\left(\left|Z_{n}\right|>M\right)<P(|Z|>M)+2 \epsilon<3 \epsilon \equiv \delta
$$

Because $\epsilon$ is arbitrary, so is $\delta,$ and therefore $Z_{n}=O_{P}(1)$

**Remarks:**

- If the probability distribution of $Z_{n}$ converges to a well-defined continuous probability distribution as $n \rightarrow \infty,$ then $Z_{n}$ is bounded in probability.

【Example】Recall from that for $Z_{n}=\max _{1 \leq i \leq n}\left(X_{i}\right),$ where $\mathrm{X}^{n}=\left(X_{1}, \cdots\right.,X_{n}$ ) is an IID random sample from a $U[0, \theta]$ distribution, we have shown that $n\left(\theta-Z_{n}\right) \stackrel{d}{\to} EXP(\theta)$ as $n \rightarrow \infty$. Therefore, $n\left(\theta-Z_{n}\right)=O_{P}(1)$ and $Z_{n}-\theta=O_{P}\left(n^{-1}\right) .$ This implies that the convergence rate of $Z_{n}$ to $\theta$ in probability is at most $n^{-1}$, which is rather rapid.

Remarks:

- The observations of $n$ random variables $\left\{X_{i}\right\}_{i=1}^{n}$ will be more or less equally spread over the interval $[0, \theta]$. Therefore, the maximal observation of $\left\{X_{i}\right\}_{i=1}^{n}$ will approach the upper bound of $\theta$ at a rate equal to $n$

【Lemma 7.6】If $Z_{n} \stackrel{p}{\longrightarrow} Z$ as $n \rightarrow \infty,$ then $Z_{n} \stackrel{d}{\rightarrow} Z$ as $n \rightarrow \infty$

**Proof:**

We want to show for every continuous point $z$ of $F_{Z}$ and every $\varepsilon>0,$ there is an $N$ such that $\left|F_{n}(z)-F_{Z}(z)\right|<\varepsilon$ for all $n \geqslant N$.

Because $F_{Z}(z)$ is continuous at $z,$ we can find $\delta>0$ such that $\left|F_{Z}(z+\delta)-F_{Z}(z)\right|<\varepsilon / 2$ and $\left|F_{Z}(z-\delta)-F_{Z}(z)\right|<\varepsilon / 2$
$$
\begin{aligned}
F_{Z}(z-\delta) &=P(Z \leqslant z-\delta) \\
&=P\left(Z \leqslant z-\delta, Z_{n} \leqslant z\right)+P\left(Z \leqslant z-\delta, Z_{n}>z\right) \\
& \leqslant P\left(Z_{n} \leqslant z\right)+P\left(Z_{n}-Z>\delta\right) \\
& \leqslant F_{n}(z)+P\left(\left|Z_{n}-Z\right|>\delta\right)
\end{aligned}
$$

So we have $F_{Z}(z-\delta)-P\left(\left|Z_{n}-Z\right|>\delta\right) \leqslant F_{n}(z)$

Similarly, $F_{n}(z) \leqslant F_{Z}(z+\delta)+P\left(\left|Z_{n}-Z\right|>\delta\right) .$ Therefore,
$$
F_{Z}(z-\delta)-P\left(\left|Z_{n}-Z\right|>\delta\right) \leqslant F_{n}(z) \leqslant F_{Z}(z+\delta)+P\left(\left|Z_{n}-Z\right|>\delta\right)
$$

and then
$$
\left|F_{n}(z)-F_{Z}(z)\right|<\frac{\varepsilon}{2}+P\left(\left|Z_{n}-Z\right|>\delta\right)
$$

since $Z_{n} \stackrel{P}{\rightarrow} Z,$ we can find $N$ large enough such that $P\left(\left|Z_{n}-Z\right|>\delta\right)<\varepsilon / 2$ for all $n \geqslant N$

Then $\left|F_{n}(z)-F_{Z}(z)\right|<\varepsilon$ for all $n \geqslant N$. Hence $Z_{n} \stackrel{d}{\rightarrow} Z$

**Remarks:**

- When $Z_{n}$ converges to $Z$ in probability as $n \rightarrow \infty,$ the random variable $Z_{n}$ will be arbitrarily close to random variable $Z$ for $n$ sufficiently large. Therefore, the probability law of $Z_{n}$ will be arbitrarily close to the probability law of $Z$ for $n$ sufficiently large.

【Lemma 7.7】**Asymptotic Equivalence**: If $Y_{n}-Z_{n} \stackrel{p}{\rightarrow} 0$ and $Z_{n} \stackrel{d}{\to}Z $ as $n \rightarrow \infty$, then $Y_{n} \stackrel{d}{\to} Z$.

Remarks:

- If two random variables $Y_{n}$ and $Z_{n}$ are very close with probability approaching one as $n \rightarrow \infty,$ they will follow the same large sample distribution
- This lemma is very useful when one is interested in deriving the asymptotic distribution of $Y_{n}$.

【Degenerate Distribution】A random variable $Z$ is said to have $a$ degenerate distribution if $P(Z=c)=1$ for some constant $c$.

【Theorem】Let $F_{n}(z)$ be the CDF of a random variable $Z_{n}$ whose distribution depends on the positive integer $n$. Let c denote a constant which does not depend upon $n$. The sequence  $Z_n \stackrel{p}{\to} c$ if and only if the limiting distribution of $Z_{n}$ is degenerate at $z=c$.

**Proof:**

$(1)[\text { Necessity }]$ : First, Suppose $\lim _{n \rightarrow \infty} P\left(\left|Z_{n}-c\right|<\epsilon\right)=1$ for any given $\epsilon>0$. Then we shall show
$$
\lim _{n \rightarrow \infty} F_{n}(z)=\left\{\begin{array}{l}
0 \text { if } z<c \\
1 \text { if } z>c
\end{array}\right.
$$

from which we can define an asymptotic (i.e., limiting) distribution
$$
F(z)=\left\{\begin{array}{l}
0 \text { if } z<c \\
1 \text { if } z \geq c
\end{array}\right.
$$

We first observe that
$$
P\left(\left|Z_{n}-c\right| \leq \epsilon\right)=F_{n}(c+\epsilon)-F_{n}(c-\epsilon)+P\left(Z_{n}=c-\epsilon\right)
$$

Because $0 \leq F_{n}(z) \leq 1$ and $\lim _{n-\infty} \operatorname{Pr}\left(\left|Z_{n}-c\right|<\epsilon\right)=1,$ we must have that for all
$\epsilon>0$
$$
\begin{aligned}
\lim _{n \rightarrow \infty} F_{n}(c+\epsilon) &=1 \\
\lim _{n \rightarrow \infty} F_{n}(c-\epsilon) &=0 \\
\lim _{n \rightarrow \infty} P\left(Z_{n}=c-\epsilon\right) &=0
\end{aligned}
$$

It follows that
$$
\lim _{n \rightarrow \infty} F_{n}(z)=\left\{\begin{array}{l}
0, \text { if } z<c \\
1, \text { if } z>c
\end{array}\right.
$$

Thus, we can define an asymptotic (i.e., limiting) distribution as follows:
$$
F(z)=\left\{\begin{array}{ll}
0, & \text { if } z<c \\
1, & \text { if } z \geq c
\end{array}\right.
$$
which is the CDF for $Z$ such that $P(Z=c)=1 .$ since $\lim _{n \rightarrow \infty} F_{n}(z)=F(z)$ at all continuity points $\{z\}$ on the real line (only $z=0$ is not a continuity point), we have $z_{n} \stackrel{d}{\longrightarrow} c$ as $n \rightarrow \infty$

$(2)[\text { Sufficiency }]:$ To complete the proof of the theorem, now suppose
$$
\lim _{n \rightarrow \infty} F_{n}(z)=\left\{\begin{array}{l}
0 \text { if } z<c \\
1 \text { if } z>c
\end{array}\right.
$$

We shall prove that $\lim _{n \rightarrow \infty} \operatorname{Pr}\left(\left|Z_{n}-c\right| \leq \epsilon\right)=1$ for all $\epsilon>0$. Because
$$
\begin{array}{l}
1 \geq \operatorname{Pr}\left(\left|Z_{n}-c\right| \leq \epsilon\right)=F_{n}(c+\epsilon)-F_{n}(c-\epsilon)+P\left(Z_{n}=c-\epsilon\right) \\
\quad \rightarrow 1-0+0=1 \text { as } n \rightarrow \infty
\end{array}
$$

for all $\epsilon>0,$ we then obtain the desired result immediately

### 7.6.3 Continuous Mapping Theorem

【Continuous Mapping Theorem】 Suppose a sequence of $k \times 1$ random vectors $Z_{n} \stackrel{d}{\rightarrow} Z$ as $n \rightarrow \infty$ and $g: \mathbb{R}^{k} \rightarrow \mathbb{R}^{l}$ is a continuous vector-valued function. Then $g\left(Z_{n}\right) \stackrel{d}{\rightarrow} g(Z)$.

**Remarks:**

- Once we know the limiting distribution of $Z_{n}$, we can find the limiting distribution of many interesting functions of $Z_{n}$. This is particularly useful for deriving the limiting distributions of statistic $T\left(Z_{n}\right)$ once the limiting distribution of $Z_{n}$ is known.

## 7.7 Central Limit Theorems

### 7.7.1 Lindeberg-Levy's CLT

【Lindeberg-Levy's Central Limit Theorem (CLT)】$\operatorname{Let} \mathrm{X}^{n}=\left(X_{1}, \cdots\right.$ $\left., X_{n}\right)^{\prime}$ be an IID random sample from a population with mean $\mu$ and variance $0<\sigma^{2}<\infty$ Define $\bar{X}_{n}=n^{-1} \sum_{i=1}^{n} X_{i} .$ Then the standardized sample mean
$$
\begin{aligned}
Z_{n} &=\frac{\bar{X}_{n}-E\left(\bar{X}_{n}\right)}{\sqrt{\operatorname{var}\left(\bar{X}_{n}\right)}} \\
&=\frac{\bar{X}_{n}-\mu}{\sigma / \sqrt{n}} \\
&=\frac{\sqrt{n}\left(\bar{X}_{n}-\mu\right)}{\sigma} \\
& \stackrel{d}{\rightarrow} N(0,1) \text { as } n \rightarrow \infty
\end{aligned}
$$

**Proof:**

Define a standardized random variable
$$
Y_{i}=\frac{X_{i}-\mu}{\sigma}, \quad i=1, \cdots, n
$$
with characteristic function $\varphi_{Y}(t)=E\left(e^{\mathrm{it} Y_{i}}\right),$ where $\mathrm{i}=\sqrt{-1}$. Then $Y_{i}$ has zero mean and unit variance.

It follows that
$$
\begin{array}{l}
\varphi_{Y}^{\prime}(0)=\mathbf{i} \cdot 0=0 \\
\varphi_{Y}^{\prime \prime}(0)=\mathbf{i}^{2} \cdot \sigma_{Y}^{2}=-1
\end{array}
$$

We now write the standardized sample mean
$$
\begin{aligned}
Z_{n} &=\frac{\bar{X}_{n}-\mu}{\sigma / \sqrt{n}} \\
&=\sqrt{n} \frac{\bar{X}_{n}-\mu}{\sigma} \\
&=\sqrt{n}\left(n^{-1} \sum_{i=1}^{n} \frac{X_{i}-\mu}{\sigma}\right) \\
&=\sqrt{n} \bar{Y}_{n} \\
&=\frac{1}{\sqrt{n}} \sum_{i=1}^{n} Y_{i}
\end{aligned}
$$

Since $X_{i}$ may not have a well-defined MGF, we take a characteristic function approach, that is, we shall show that $\varphi_{n}(t) \rightarrow e^{-\frac{1}{2} t^{2}}$ as $n \rightarrow \infty,$ where $\varphi_{n}(t)=E\left(e^{\mathrm{i} t Z_{n}}\right)$ is the characteristic function of $Z_{n}$ and $e^{-\frac{1}{2} t^{2}}$ is the characteristic function of $N(0,1)$.

The characteristic function of $Z_{n}$
$$
\begin{aligned}
\varphi_{n}(t) &=E\left(e^{\mathrm{i} t \sqrt{n} \bar{Y}_{n}}\right) \\
&=\left[E\left(e^{\mathrm{i} t Y_{1} / \sqrt{n}}\right)\right]^{n} \\
&=\left[\varphi_{Y}(t / \sqrt{n})\right]^{n} \\
&=\left[\varphi_{Y}(0)+\varphi_{Y}^{\prime}(0) \frac{t}{\sqrt{n}}+\frac{1}{2} \varphi_{Y}^{\prime \prime}(0)\left(\frac{t}{\sqrt{n}}\right)^{2}+r\left(\frac{t}{\sqrt{n}}\right)\right]^{n} \\
&=\left(1-\frac{t^{2}}{2 n}+o\left(n^{-1}\right)\right)^{n} \\
& \rightarrow e^{-t^{2} / 2}
\end{aligned}
$$

where $r(t / \sqrt{n})$ denotes a reminder term, and we have used the formula $\left(1+\frac{a}{n}\right)^{n} \rightarrow e^{a}$ as $n \rightarrow \infty$.

Therefore, $Z_{n} \stackrel{d}{\rightarrow} N(0,1)$ as $n \rightarrow \infty$

**Remarks:**

- Let $Z \sim N(0,1) .$ The $N(0,1)$ CDF is denoted as

$$
\Phi(z)=\int_{-\infty}^{z} \frac{1}{\sqrt{2 \pi}} e^{-x^{2} / 2} d x
$$

- CLT says that when $n \rightarrow \infty, Z_{n} \stackrel{d}{\rightarrow} Z,$ i.e.

$$
F_{n}(z) \equiv P\left(Z_{n} \leq z\right) \rightarrow \Phi(z)
$$

- for all $z \in(-\infty, \infty)$


- Therefore, for each sufficiently large (**but finite**) integer $n,$ the distribution of $\bar{X}_{n}$ will be approximately a $N\left(\mu, \sigma^{2} / n\right)$ or equivalently, the distribution of the sum $\sum_{i=1}^{n} X_{i}$ will be approximately a $N\left(n \mu, n \sigma^{2}\right)$ distribution.
- Sometimes CLT is interpreted incorrectly as implying that the distribution of $\bar{X}_{n}$ approaches a normal distribution as $n \rightarrow \infty .$ This is incorrect because $\operatorname{var}\left(\bar{X}_{n}\right) \rightarrow 0$ and $\bar{X}_{n}$ converges to a degenerate distribution $F(\cdot)$ such that $F(x)=0$ if $x<\mu$ and $F(x)=1$ if $x \geq \mu$.
- Historically, CLT was first established for a random sample from a Bernoulli distribution by A. de Moivre in the early eighteenth century. The proof for a random sample from an arbitrary distribution was given independently by J.W. Lindeberg and P. Levy in the early 1920s.
- CLT occupies a central position in statistical inference. Although CLT provides a simple and useful general approximation, there is no automatic way of knowing how good the approximation is in general. In fact, the goodness of the approximation is a function of the sample size n and the original population distribution, and differs case by case.

【Normal Approximation for the Binomial Distribution $B(n, p)$ When $n$ is Large】For a binomial $B(n, p)$ random variable $Z_{n},$ we can write $Z_{n}=\sum_{i=1}^{n}$ where $\mathbf{X}^{n}=\left(X_{1}, \cdots, X_{n}\right)$ is an IID random sample from a Bernoulli(p) distribution with 
$P(X_i=1)=p \in(0,1) .$ 

By CLT$,$ we have that as $n \rightarrow \infty,$ the standardized random variable
$$
\begin{aligned}
\frac{Z_{n}-E\left(Z_{n}\right)}{\sqrt{\operatorname{var}\left(Z_{n}\right)}} &=\frac{Z_{n}-n p}{\sqrt{n p(1-p)}} \\
& \stackrel{d}{\rightarrow} N(0,1)
\end{aligned}
$$
Although this result applies only when $n \rightarrow \infty,$ the normal distribution is often used to approximate binomial probabilities in practice even when $n$ is fairly small. 

Remarks:

- In Chapter 4, we approximate the Binomial$(n, p)$ distribution by a Poisson distribution, which is called the law of small numbers. Here we use a normal approximation due to the law of large numbers.
- A Poisson approximation is better when $p$ is small, while a normal approximation works better when both $n p$ and $n(1-p)$ are both larger than 5.

【Normal Approximation of $\chi_{n}^{2}$ When $n$ is Large】Suppose $\mathrm{X}^{n}$ is an IID $N(0,1)$ random sample. Put $Y=X_{i}^{2}, i=1, \cdots, n$. Then as $n \rightarrow \infty,$ the standardized random variable

$$
\begin{aligned}
\frac{\sum_{i=1}^{n} Y_{i}-n \mu_{Y}}{\sqrt{n \sigma_{Y}^{2}}} &=\frac{\sum_{i=1}^{n} X_{i}^{2}-n}{\sqrt{2 n}} \\
& \stackrel{d}{\longrightarrow} N(0,1)
\end{aligned}
$$

**Proof:**

Put $\bar{Y}_{n}=n^{-1} \sum_{i=1}^{n} Y_{i} .$ since $E\left(Y_{i}\right)=1$ and $\operatorname{var}\left(Y_{i}\right)=2,$ we have from CLT that as $n \rightarrow \infty,$ the standardized sample mean
$$
\begin{aligned}
\frac{\bar{Y}_{n}-\mu_{Y}}{\sigma_{Y} / \sqrt{n}}=& \frac{\bar{Y}_{n}-1}{\sqrt{2} / \sqrt{n}} \\
& \stackrel{d}{\rightarrow} N(0,1)
\end{aligned}
$$

where
$$
\begin{aligned}
\frac{\bar{Y}_{n}-1}{\sqrt{2} / \sqrt{n}} &=\frac{\sqrt{n}\left(\bar{Y}_{n}-1\right)}{\sqrt{2}} \\
&=\frac{\frac{1}{\sqrt{n}} \sum_{i=1}^{n}\left(Y_{i}-1\right)}{\sqrt{2}} \\
&=\frac{\sum_{i=1}^{n} Y_{i}-n}{\sqrt{2 n}} \\
&=\frac{\sum_{i=1}^{n} X_{i}^{2}-n}{\sqrt{2 n}}
\end{aligned}
$$

Noting $\sum_{i=1}^{n} X_{i}^{2} \sim \chi_{n}^{2},$ we can approximate a $\chi_{n}^{2}$ distribution by a $N(n, 2 n)$ distribution when the degree $n$ of freedom is large. As an example, when $\mathbf{X}^{n}$ is an IID $N\left(\mu, \sigma^{2}\right)$ random sample, we have $(n-1) S_{n}^{2} / \sigma^{2} \sim \chi_{n-1}^{2}$ for all $n>1 .$ It follows that as $n \rightarrow \infty$
$$
\begin{array}{l}
{\left[\frac{(n-1) S_{n}^{2}}{\sigma^{2}}-(n-1)\right] / \sqrt{2(n-1)}} \\
\stackrel{d}{\rightarrow} N(0,1)
\end{array}
$$

### 7.7.2 Relaxing Assumptions

The assumption of a finite variance is essentially necessary for CLIT. It implies that we always obtain an approximate normality from the sum of "small" (finite variance) independent disturbances. Although the finite variance assumption can be relaxed somewhat, it cannot be eliminated.	

【Example】Suppose $X_n$ is an IID random sample from the Cauchy(0, 1) distribution. Then it can be shown by characteristic function that $\bar{X}_n \sim Cauchy(0, 1)$ for all $n \geq 1$.

On the other hand, the identical distribution assumption can be relaxed. In other words, CLT continues to hold when there exist certain degrees of heterogeneity in observations

【Liapounov (1901) CLT for Independent Random Variables】 Suppose the random variables $X_{1}, \cdots, X_{n}$ are jointly independent and $E\left|X_{i}-\mu_{i}\right|^{3}<\infty$ for $i=1, \cdots, n$ where $E\left(X_{i}\right)=\mu_{i} .$ Also, suppose
$$
\lim _{n \rightarrow \infty} \frac{\sum_{i=1}^{n} E\left|X_{i}-\mu_{i}\right|^{3}}{\left(\sum_{i=1}^{n} \sigma_{i}^{2}\right)^{3 / 2}}=0
$$

Then as $n \rightarrow \infty,$ we have the standardized random variable
$$
\begin{aligned}
Z_{n}=& \frac{\sum_{i=1}^{n} X_{i}-\sum_{i=1}^{n} \mu_{i}}{\left(\sum_{i=1}^{n} \sigma_{i}^{2}\right)^{1 / 2}} \\
& \stackrel{d}{\rightarrow} N(0,1)
\end{aligned}
$$

**Remarks:**

- CLT also holds when there exists certain degree of dependence among the random variables $X_{1}, \cdots, X_{n}$
- Although the dependence allowed cannot be too strong (see what happens if $X_{1}=X_{2}=\cdots=X_{n}$ in an extreme case), one can relax the independence assumption to some extent. See, for example, Billingsley (1995, section 27). Also see White (1999) for CLT for dependent random samples. This allows application of CLT to some time series data.

### 7.7.3 Slutsky's Theorem

【Slutsky's Theorem】Suppose $X_{n} \stackrel{d}{\rightarrow} X$ and $c_{n} \stackrel{p}{\longrightarrow} c,$ a constant. Then
	(1) $X_{n}+c_{n} \stackrel{d}{\longrightarrow} X+c$
	(2) $X_{n}-c_{n} \stackrel{d}{\rightarrow} X-c$
	(3) $X_{n} c_{n} \stackrel{d}{\longrightarrow} c X$
	(4) $\frac{X_{n}}{c_{n}} \stackrel{d}{\rightarrow} \frac{X}{c},$ for $c \neq 0$
【Example】Suppose the standardized sample mean
$$
\frac{\sqrt{n}\left(\bar{X}_{n}-\mu\right)}{\sigma} \stackrel{d}{\rightarrow} N(0,1)
$$
and $S_{n}^{2} \stackrel{p}{\rightarrow} \sigma^{2}$ as $n \rightarrow \infty$ (so $S_{n} \stackrel{p}{\rightarrow} \sigma$ by the continuity of the square root function). Then by the Slutsky theorem
$$
\begin{aligned}
\frac{\sqrt{n}\left(\bar{X}_{n}-\mu\right)}{S_{n}}=& \frac{\sigma}{S_{n}} \frac{\sqrt{n}\left(\bar{X}_{n}-\mu\right)}{\sigma} \\
& \stackrel{d}{\rightarrow} N(0,1)
\end{aligned}
$$

【Example】Suppose $X_{n} \stackrel{d}{\rightarrow} X$ and $Y_{n} \stackrel{d}{\rightarrow} Y$ as $n \rightarrow \infty .$ Do we have the following results? Give your reasoning:
	(1) $X_{n} \pm Y_{n} \stackrel{\text { d }}{\longrightarrow} X \pm Y$ as $n \rightarrow \infty$
	(2) $X_{n} Y_{n} \stackrel{d}{\longrightarrow} X Y$ as $n \rightarrow \infty$

**Solution:**

The answer is generally no, because the dependence between $X_{n}$ and $Y_{n}$ is not taken into account.

In other words, convergence in marginal distribution does not imply convergence in joint distribution. This is different from other convergence concepts, such as convergence in quadratic mean, convergence in probability and almost sure convergence. For these convergences, element-by-element convergences are equivalent to joint convergence.

### 7.7.4 Delta Method

【Delta Method】Suppose $\sqrt{n}\left(\bar{X}_{n}-\mu\right) / \sigma \stackrel{d}{\rightarrow} N(0,1)$ as $n \rightarrow \infty,$ and function $g(\cdot)$ is continuously differentiable with $g^{\prime}(\mu) \neq 0 .$ Then as $n \rightarrow \infty$
$$
\sqrt{n}\left[g\left(\bar{X}_{n}\right)-g(\mu)\right] \stackrel{d}{\rightarrow} N\left(0, \sigma^{2}\left[g^{\prime}(\mu)\right]^{2}\right)
$$

and
$$
\frac{\sqrt{n}\left[g\left(\bar{X}_{n}\right)-g(\mu)\right]}{\sigma g^{\prime}(\mu)} \stackrel{d}{\rightarrow} N(0,1)
$$
**Proof:**
First, by Lemma 4,$\sqrt{n}\left(\bar{X}_{n}-\mu\right) / \sigma \stackrel{d}{\rightarrow} N(0,1)$ implies $\sqrt{n}\left(\bar{X}_{n}-\mu\right) / \sigma=O_{P}(1)$. Therefore, we have $\bar{X}_{n}-\mu=O_{P}\left(n^{-1 / 2}\right)=o_{P}(1)$.

Next, by the mean value theorem, we have
$$
Y_{n}=g\left(\bar{X}_{n}\right)=g(\mu)+g^{\prime}\left(\bar{\mu}_{n}\right)\left(\bar{X}_{n}-\mu\right)
$$

where $\bar{\mu}_{n}=\lambda \mu+(1-\lambda) \bar{X}_{n}$ for some $\lambda \in[0,1] .$ Note that $\left|\bar{\mu}_{n}-\mu\right|=\left|(1-\lambda)\left(\bar{X}_{n}-\mu\right)\right| \leq \left|\bar{X}_{n}-\mu\right|=o_{P}(1)$

It follows by the Slutsky theorem that
$$
\begin{aligned}
\sqrt{n}\left[\frac{g\left(\bar{X}_{n}\right)-g(\mu)}{\sigma}\right] &=g^{\prime}\left(\bar{\mu}_{n}\right) \sqrt{n} \frac{\bar{X}_{n}-\mu}{\sigma} \\
& \stackrel{d}{\rightarrow} N\left[0, g^{\prime}(\mu)^{2}\right]
\end{aligned}
$$

where $g^{\prime}\left(\bar{\mu}_{n}\right) \stackrel{p}{\rightarrow} g^{\prime}(\mu)$  given $\bar{\mu}_{n} \stackrel{p}{\rightarrow} \mu$ and the continuity of the first derivative $g^{\prime}(\cdot)$.

By the Slutsky theorem again, we have
$$
\frac{\sqrt{n}\left[g\left(\bar{X}_{n}\right)-g(\mu)\right]}{\sigma g^{\prime}\left(\bar{X}_{n}\right)} \stackrel{d}{\rightarrow} N(0,1)
$$
Remarks:

- The Delta method can be viewed as a Taylor series approximation in a statistical context. It linearizes a smooth (i.e., continuously differentiable) nonlinear statistic so that CLT can be applied to the linearized statistic. Therefore, it can be viewed as a generalization of CLT.

Question: To apply the delta method, it is required that $g^{\prime}(\mu) \neq 0 .$ What happens to the delta method if $g^{\prime}(\mu)=0 ?$

【Second Order Delta Method】Suppose random variables  $\sqrt{n}(\bar{X}_{n}-\mu) / \sigma \stackrel{d}{\longrightarrow} N(0,1),$ and function $g(\cdot)$ is twice continuously differentiable such that $g^{\prime}(\mu)=0$ and
$g^{\prime \prime}(\mu) \neq 0 .$ Then as $n \rightarrow \infty$
$$
\frac{n\left[g\left(\bar{X}_{n}\right)-g(\mu)\right]}{\sigma^{2}} \stackrel{d}{\to} \frac{g^{\prime \prime}(\mu)}{2} \chi_{1}^{2}
$$

**Question:** We have considered the sequence of scalar random variables $\left\{\bar{X}_{n}, n=1,2, \cdots\right\}$ How can one derive the asymptotic distribution for a random vector $Z_{n} ?$

- The following Cramer-Wold device will allow us to derive the asymptotic distribution of a sequence of random vectors.

【Cramer-Wold Device】A sequence of random vector $Z_{n}=\left(Z_{1 n}, \ldots, Z_{d n}\right)$ converges in distribution to a random vector $Z$ if an only if
$$
a^{\prime} Z_{n} \stackrel{d}{\rightarrow} a^{\prime} Z
$$

for every constant vector $a \neq 0$ in $\mathbb{R}^{d}$.

【Example 7.33】Suppose $ Z_{n} \stackrel{d}{\rightarrow} Z \sim N(0, \Sigma), $ where $ Z $ is an $ m \times 1 $ random vector and $ \Sigma $ is an $ m \times m $ nonsingular matrix, where the dimension $ m $ is fixed. If $ \Sigma_{n} \stackrel{p}{\rightarrow} \Sigma $ as $ n \rightarrow \infty, $ then the quadratic form

$$
Z_{n}^{\prime} \Sigma_{n}^{-1} Z_{n} \stackrel{d}{\rightarrow} Z^{\prime} \Sigma^{-1} Z \sim \chi_{m}^{2}
$$

**Proof:**

First, by the Cramer-Wold device and the Slutsky‘s theorem, we can show that
$$
\Sigma^{-\frac{1}{2}} Z_{n} \stackrel{d}{\rightarrow} \Sigma^{-1 / 2} Z \sim N\left(0, I_{m}\right) \text { as } n \rightarrow \infty
$$

where $ I_{m} $ is an $ m \times m $ identity matrix.

It follows from the continuous mapping theorem that
$$
\begin{aligned}
\left(\hat{\Sigma}^{-\frac{1}{2}} Z_{n}\right)^{\prime}\left(\hat{\Sigma}^{-\frac{1}{2}} Z_{n}\right) &=Z_{n}^{\prime} \hat{\Sigma}^{-1} Z_{n} \\
& \stackrel{d}{\rightarrow} Z^{\prime} \Sigma^{-1} Z \sim \chi_{m}^{2}
\end{aligned}
$$