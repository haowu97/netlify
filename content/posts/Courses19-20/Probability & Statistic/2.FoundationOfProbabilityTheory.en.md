---
title: "2. Foundation of Probability Theory"
date: 2021-05-03T17:38:51+08:00
lastmod: 2021-05-04T17:38:51+08:00

draft: false

description: ""
upd: ""

tags: ['Notes']
categories: ['Probability and Statistic']

math: true
---



## 2.1 Random Experiments

**Definition (Random Experiment)**: A random experiment is a process that generates at least two possible outcomes. One and only one outcome will occur, but there is uncertainty associated with which one will occur.

Two essential elements of a random experiment:

- The set of all possible outcomes——**sample space**;
- The likelihood with which each outcome will occur——**probability function**.

Fundamental axioms of modern econometrics:

- An economic system can be viewed as a random experiment governed by some probability distribution or probability law.
- Any economic phenomena (often in form of data) can be viewed as an outcome of this random experiment.

## 2.2 Basic Concepts of Probability

**Definition (Sample Space)**: The possible outcomes of the random experiment are called **basic outcomes**, and the set of all basic outcomes is called the sample space, denoted by $ S $.

When an experiment is performed, the realization of the experiment is one outcome in the sample space.

**Example: Finite sample space**

| Tossing a coin          | $S=\{\text { Head, Tail }\}$      |
| ----------------------- | --------------------------------- |
| Rolling a die           | $S=\{1,2,3,4,5,6\}$               |
| Playing a football game | $ S=\{\text { Win, Lose, Tie}\} $ |

**Example: Infinite discrete sample space**

Consider the number of accidents that occur at a given intersection within a month. The sample space is the set of all nonnegative integers, $ S=\{0,1,2, \ldots\} $

**Example: Continuous sample space**

When recording the lifetime of a light bulb, the outcome is the time until the bulb burns out. Therefore the sample space is the set of all nonnegative real number, $ S=\mathbb{R}_{+}=\{t: t \in \mathbb{R}, t \geqslant 0\} $.

**Definition (Event)**: An event $ A $ is a subset of basic outcomes from the sample space $ S $. The event $ A $ is said to occur if the random experiment gives rise to one of the constituent basic outcomes in $ A $. That is, an event occurs if any of its basic outcomes has occurred.

**Example**

A die is rolled. The sample space $ S=\{1,2,3,4,5,6\} . $ Event $ A $ is defined as "number resulting is even", then $ A=\{2,4,6\} . $ Event $ B $ is defined as "number resulting is 4", then $ B=\{4\} $.

Remarks:

- The words "set" and "event" are interchangeable.
- Basic outcome $ \in $ sample space; event $ \subseteq $ sample space.

## 2.3 Review of Set Theory

**Definition (Containment)**: The event $ A $ is contained in the event $ B $, or $ B $ contains $ A, $ if every sample point of $ A $ is also a sample point of $ B $. If so, we write $ A \subseteq B, $ or equivalently, $ B \supseteq A $.

**Definition (Equality)**: Two events $ A $ and $ B $ are said to be equal, $ A=B, $ if $ A \subseteq B $ and $ B \subseteq A $.

**Definition (Empty Set)**: The set containing no elements is called the empty set and is denoted by $ \varnothing . $ The event corresponding to $ \varnothing $ is called a null (or impossible) event.

**Definition (Complement)**: The complement of $ A, $ denoted by $ A^{c}, $ is the set of basic outcomes of a random experiment belonging to $ S $ but not to $ A $.

**Definition (Union)**: The union of $ A $ and $ B $, denoted by $ A \cup B, $ is the set of all basic outcomes in $ S $ that belong to either $ A $ or $ B $. The union of $ A $ and $ B $ occurs if and only if either $ A $ or $ B $ (or both) occurs.

**Definition (Intersection)**: The intersection of $ A $ and $ B, $ denoted by $ A \cap B, $ is the set of basic outcomes in $ S $ that belong to both $ A $ and $ B $. The intersection occurs if and only if both events $ A $ and $ B $ occur.

**Definition (Difference)**: The difference of $ A $ and $ B $, denoted by $ A \backslash B $ or $ A-B, $ is the set of basic outcomes in $ S $ that belong to $ A $ but not to $ B $, i.e. $ A \backslash B=A \cap B^{c} $.

**Definition (Exclusiveness)**: If $ A $ and $ B $ have no common basic outcomes, they are called mutually exclusive (or disjoint). Their intersection is empty set, i.e. $ A \cap B=\varnothing $.

**Definition (Collectively Exhaustiveness)**: Suppose $ A_{1}, A_{2}, \ldots, A_{n} $ are $ n $ events in the sample space $ S, $ where $ n $ is any positive integer. If $ \bigcup_{i=1}^{n} A_{i}=S, $ then these $ n $ events are said to be collectively exhaustive.

**Definition (Partition)**: A class of events $ \mathcal{H}=\left\{A_{1}, A_{2}, \ldots, A_{n}\right\} $ forms a partition of the sample space $ S $ if these events satisfy

1. $ A_{i} \cap A_{j}=\varnothing $ for all $ i \neq j $ (mutually exclusive), and
2. $ A_{1} \cup A_{2} \cup \cdots \cup A_{n}=S $ (collectively exhaustive).

Complementation
$$
\left(A^{c}\right)^{c}=A, \quad \varnothing^{c}=S, \quad S^{c}=\varnothing
$$

Commutativity

$$
A \cup B=B \cup A, \quad A \cap B=B \cap A
$$

Associativity

$$
A \cup(B \cup C)=(A \cup B) \cup C, \quad A \cap(B \cap C)=(A \cap B) \cap C
$$

Distributivity

$$
A \cap(B \cup C)=(A \cap B) \cup(A \cap C), \quad A \cup(B \cap C)=(A \cup B) \cap(A \cup C)
$$

More generally, for $ n \geqslant 1 $,

$$
B \cap\left(\bigcup_{i=1}^{n} A_{i}\right)=\bigcup_{i=1}^{n}\left(B \cap A_{i}\right), \quad B \cup\left(\bigcap_{i=1}^{n} A_{i}\right)=\bigcap_{i=1}^{n}\left(B \cup A_{i}\right)
$$

De Morgan's Laws

$$
(A \cup B)^{c}=A^{c} \cap B^{c}, \quad(A \cap B)^{c}=A^{c} \cup B^{c}
$$

More generally, for $ n \geqslant 1 $,

$$
\left(\bigcup_{i=1}^{n} A_{i}\right)^{c}=\bigcap_{i=1}^{n}\left(A_{i}\right)^{c}, \quad\left(\bigcap_{i=1}^{n} A_{i}\right)^{c}=\bigcup_{i=1}^{n}\left(A_{i}\right)^{c}
$$

**Definition (Countable Set)**: A set $ S $ is called countable if the set has one-to-one correspondence with a subset of the natural numbers $ \mathbb{N} $.

**Remarks:**

- A countable set is either a finite set or a countably infinite set.
- We can use a finite or infinite sequence to index all elements in a countable set.

- The set of natural numbers $ \mathbb{N} $, the set of integers $ \mathbb{Z} $, and the
set of rational numbers $ \mathbb{Q} $ are countable sets.
- The set of real numbers $ \mathbb{R} $ is uncountable. The set of all real
numbers in interval $ (a, b) $ is uncountable.

Unions and intersections of infinitely countable collection of
events:

$$
\begin{array}{l}
\bigcup_{i=1}^{\infty} A_{i}=\left\{x \in S: x \in A_{i} \text { for some } i\right\} \\
\bigcap_{i=1}^{\infty} A_{i}=\left\{x \in S: x \in A_{i} \text { for all } i\right\}
\end{array}
$$

Unions and intersections of (infinitely) uncountable collection of events: Let $ \Gamma $ be an uncountable index set, then

$$
\begin{array}{l}
\bigcup_{a \in \Gamma} A_{a}=\left\{x \in S: x \in A_{a} \text { for some } a\right\} \\
\bigcap_{a \in \Gamma} A_{a}=\left\{x \in S: x \in A_{a} \text { for all } a\right\}
\end{array}
$$

## 2.4 Fundamental Probability Laws: $ \sigma $-Algebra

**Definition $\sigma$-algebra**: A $ \sigma $ -algebra (or $ \sigma $ -field), denoted by $ \mathcal{B}, $ is a collection of subsets of $ S $ that satisfies

1. $ \varnothing \in \mathcal{B} $ (the empty set is contained in $ \mathcal{B} $ );
2. If $ A \in \mathcal{B}, $ then $ A^{c} \in \mathcal{B}(\mathcal{B} $ is closed under complement); and
3. If $ A_{1}, A_{2}, \ldots \in \mathcal{B}, $ then $ \bigcup_{i=1}^{\infty} A_{i} \in \mathcal{B}(\mathcal{B} $ is closed under
    countable unions).

Remarks:

- A $ \sigma $ -algebra is a set of sets.
- (1) and (2) imply that $ S \in \mathcal{B} $.
- (2) and (3) imply that $ \bigcap_{i=1}^{\infty} A_{i} \in \mathcal{B} $
- For a given $ S, $ we can construct many different $ \sigma $ -algebras.
- $ \{\varnothing, S\} $ is a $ \sigma $ -algebra, usually called the trivial $ \sigma $ -algebra.
- The collection of all possible subsets of $ S $ is a $ \sigma $ -algebra.
- For any event $ A,\left\{\varnothing, A, A^{c}, S\right\} $ is a $ \sigma $ -algebra.
- Intersection of $ \sigma $ -algebras is also a $ \sigma $ -algebras.

**Theorem**: For any non-empty collection $ \mathcal{K} $ of subsets of sample space $ S, $ there exists a unique smallest $ \sigma $ -algebra $ \mathcal{B} $ containing $ \mathcal{K} $. It is called the $ \sigma $ -algebra generated by $ \mathcal{K} $.

- Smallest: If $ \mathcal{B}^{\prime} $ is a $ \sigma $ -algebra containing $ \mathcal{K}, $ then $ \mathcal{B} \subseteq \mathcal{B}^{\prime} $
- Unique: If $ \mathcal{B}^{\prime} $ is another smallest $ \sigma $ -algebra containing $ \mathcal{K}, $ then $ \mathcal{B}^{\prime}=\mathcal{B} $

**Example**: Suppose $ S=\{1,2,3\} . $ Let $ \mathcal{K}_{1}=\{\{1\}\} $ and $ \mathcal{K}_{2}=\{\{1\},\{1,2\}\} $.

- $ \{\varnothing,\{1\},\{2,3\}, S\} $ is a $ \sigma $ -algebra containing $ \mathcal{K}_{1} $. (Smallest?)
- $ \{\varnothing,\{1\},\{2\},\{3\},\{1,2\},\{2,3\},\{1,3\}, S\} $ is the smallest $ \sigma $-algebra containing $ \mathcal{K}_{2} $

**Example (Borel algebra)**: Let $ S=\mathbb{R}=(-\infty,+\infty), $ the real line. Let $ \mathcal{B} $ be the collection of all sets that can be formed by taking complements, countable unions, and countable intersections of $ [a, b],[a, b),(a, b],(a, b) $ for any real numbers a and $ b $. Then $ \mathcal{B} $ is the smallest $ \sigma $ -algebra containing all the intervals. This $ \sigma $ -algebra is called Borel algebra, and any set in $ \mathcal{B} $ is called Borel set.

**Definition (Probability Function)**: Suppose a random experiment has a sample space $ S $ and an associated $ \sigma $ -algebra $ \mathcal{B} $. The probability function $ P: \mathcal{B} \rightarrow[0,1] $ is a mapping that satisfies

1. $ 0 \leqslant P(A) \leqslant 1 $ for any event $ A \in \mathcal{B} $
2. $ P(S)=1 $
3. If countable number of events $ A_{1}, A_{2}, \ldots \in \mathcal{B} $ are mutually exclusive (pairwise disjoint), then $ P\left(\bigcup_{i=1}^{\infty} A_{i}\right)=\sum_{i=1}^{\infty} P\left(A_{i}\right) $ (countable additivity)

**Remark:** For a given measurable space $ (S, \mathcal{B}), $ many different probability functions can be defined.

**Definition (Probability Space)**: A probability space is a triple $ (S, \mathcal{B}, P), $ where

1. $ S $ is the sample space corresponding to outcomes of the underlying random experiment;
2. $ \mathcal{B} $ is an associated $ \sigma $ -algebra of $ S $, and elements of $ \mathcal{B} $ are called events;
3. $ P $ is a probability measure (or probability function).

Properties of probability function:

- $ P(\varnothing)=0 $
- $ P\left(A^{c}\right)=1-P(A) $
- If $ A \subseteq B, $ then $ P(A) \leqslant P(B) $
- If $ A_{1}, A_{2}, \ldots \in \mathcal{B} $ form a partition of $ S $ (i.e. mutually exclusive and collectively exhaustive), and $ A $ is an event in $ S, $ then

$$
P(A)=\sum_{i=1}^{\infty} P\left(A \cap A_{i}\right)
$$

- Sub-additivity: For any sequence of events $ A_{1}, A_{2}, \ldots \in \mathcal{B} $

$$
P\left(\bigcup_{i=1}^{\infty} A_{i}\right) \leqslant \sum_{i=1}^{\infty} P\left(A_{i}\right)
$$

**Theorem**: For any $ n $ events $ A_{1}, A_{2}, \ldots, A_{n} $

$$
\begin{aligned}
P\left(\bigcup_{i=1}^{n} A_{i}\right)=&\sum_{1 \leqslant i \leqslant n} P\left(A_{i}\right)-\sum_{1 \leqslant i_{1}<i_{2} \leqslant n} P\left(A_{i_{1}} \cap A_{i_{2}}\right) & \\
&+\sum_{1 \leqslant i_{1}<i_{2}<i_{3} \leqslant n} P\left(A_{i_{1}} \cap A_{i_{2}} \cap A_{i_{3}}\right) \\
&-\cdots+(-1)^{n+1} P\left(A_{1} \cap A_{2} \cap \cdots \cap A_{n}\right)
\end{aligned}
$$

**Remark**: For $ n=2, P\left(A_{1} \cup A_{2}\right)=P\left(A_{1}\right)+P\left(A_{2}\right)-P\left(A_{1} \cap A_{2}\right) $

For the so-called **classical interpretation of probability**, we assume:

- Sample space $ S $ contains a finite number $ n $ of outcomes, say $ S=\left\{a_{1}, \ldots, a_{n}\right\} $.

- All of the outcomes are equally probable.
- The associated $ \sigma $ -algebra is the collection of all possible subsets of $ S $

The probability of any single outcomes is $ 1 / n . $ For every event $ A $

$$
P(A)=\sum_{a_{i} \in A} P\left(\left\{a_{i}\right\}\right)=\frac{\text { number of outcomes in } A}{n}
$$

How to determine the number of total outcomes in the sample
space and in various events in $ S $?

## 2.5 Methods of Counting

**Theorem (Fundamental Theorem of Counting)**: If a random experiment consists of $ k $ separated tasks, the $ i $-th of which can be done in $ n_{i} $ ways, $ i=1,2, \ldots, k, $ then the entire job can be done in $ n_{1} \times n_{2} \times \cdots \times n_{k}=\prod_{i=1}^{k} n_{i} $ ways.

We consider two important counting methods: permutation and combination.

### 2.5.1 Permutation

**Example**: Suppose we will choose choose two letters out of $ \{\mathrm{A}, \mathrm{B}, \mathrm{C}, \mathrm{D}\} $ in different orders, with each letter being used at most once each time. How many possible orders could we obtain?

12 ways: $ \mathrm{AB}, \mathrm{AC}, \mathrm{AD}, \mathrm{BA}, \mathrm{BC}, \mathrm{BD}, \mathrm{CA}, \mathrm{CB}, \mathrm{CD}, \mathrm{DA}, \mathrm{DB}, \mathrm{DC} $

Suppose there are $ k $ boxes in a row and there are $ n $ objects, where $ n \geqslant k . $ If we choose $ k $ from the $ n $ objects to fill in the $ k $ boxes, how many possible different ordered sequences could we obtain?

1. One object is selected to fill in box $ 1, $ so there are $ n $ ways;

2. A second object is selected from the remaining $ n-1 $ objects to fill in box $ 2, $ so there are $ n-1 $ ways;

  ... k. To fill the last box (box $ k $ ), there are $ n-(k-1) $ ways since $ n-(k-1) $ objects remain. 

The total number of different ways to fill boxes $ 1, \ldots, k $ is
$$
n \times(n-1) \times \cdots \times[n-(k-1)]
$$

The experiment is equivalent to selecting $ k $ objects out of the $ n $ objects first, then arrange the selected $ k $ objects in a sequence.

Each different arrangement of the sequence is called a **permutation**.

The number of permutations of choosing $ k $ out of $ n, $ denoted by $ P_{n}^{k}, $ is
$$
P_{n}^{k}=\frac{n !}{(n-k) !}=n \times(n-1) \times \cdots \times[n-(k-1)]
$$

where $ n !=n \times(n-1) \times \cdots \times 2 \times 1 $.

Convention: $ 0 !=1 $

**Example (Birthday Problem)**: What is the probability that at least two persons in a group of $ k $ people $ (2 \leqslant k \leqslant 365) $ have the same birthday (i.e. the same day of the same month)? Assume no one is born on Feb 29 and each of the 365 days is equally likely to be the birthday of anyone.

Define event $ A $ as "at least two persons have the same birthday".
- How many possible ways in which the $ k $ people could be born? $ 365^{k}(\text { total numbers of outcomes in sample space } S) $
- Event $ A^{c} $ is " $ k $ people have different birthdays". How many ways that all can have different birthdays? $ P_{365}^{k} $
- So

$$
P(A)=1-P\left(A^{c}\right)=1-\frac{P_{365}^{k}}{365^{k}}
$$

### 2.5.2 Combination

**Example**: Suppose we will choose two letters out of $ \{\mathrm{A}, \mathrm{B}, \mathrm{C}, \mathrm{D}\} $ without ordering. If each letter is used at most once each time, how many possible pairs could we obtain?

6 pairs: $ (\mathrm{A}, \mathrm{B}),(\mathrm{A}, \mathrm{C}),(\mathrm{A}, \mathrm{D}),(\mathrm{B}, \mathrm{C}),(\mathrm{B}, \mathrm{D}),(\mathrm{C}, \mathrm{D}) $

Choose a subset of $ k $ elements without replacement from a set of $ n $ distinct elements.

- The order of the elements is irrelevant. For example, the subsets $ \{a, b\} $ and $ \{b, a\} $ are identical.
- Each subset is called a **combination**.
- The number of combinations of choosing $ k $ out of $ n $ is denoted by $ C_{n}^{k} . $ We have

$$
\begin{aligned}
C_{n}^{k} &=\frac{\text { number of choosing } k \text { out of } n \text { with ordering }}{\text { number of ordering } k \text { elements }} \\
&=\frac{P_{n}^{k}}{k !}=\frac{n !}{k !(n-k) !}
\end{aligned}
$$

$ C_{n}^{k} $ is also denoted by $ \left(\begin{array}{l}n \\ k\end{array}\right) . $ This is also called a binomial coefficient because of its appearance in the binomial theorem

$$
(x+y)^{n}=\sum_{k=0}^{n}\left(\begin{array}{l}
n \\
k
\end{array}\right) x^{k} y^{n-k}
$$

Properties of the binomial coefficients
$$
\begin{array}{c}
\sum_{k=0}^{n} C_{n}^{k}=2^{n}, \quad \sum_{k=0}^{n}(-1)^{k} C_{n}^{k}=0 \\
\sum_{k=0}^{m} C_{n}^{k} C_{n}^{m-k}=C_{2 n}^{m} \\
C_{n}^{k}+C_{n}^{k-1}=C_{n+1}^{k}
\end{array}
$$

**Example**: Select 10 students randomly from a class containing 15 boys and 20 girls. What is the probability that exactly 3 boys are selected?

- The number of combinations of 10 students out of 35 students is $ C_{35}^{10} $
- The number of combinations of 3 boys out of 15 boys is $ C_{15}^{3} $
- The number of combinations of 7 girls out of 20 girls is $ C_{20}^{7} $

Thus the probability is $ \left(C_{15}^{3} \times C_{20}^{7}\right) / C_{35}^{10} $.

**Matching Problem**: There are $ n $ letters and $ n $ envelopes with the corresponding addresses. If we place the $ n $ letters in the $ n $ envelopes in a random manner, what is the probability that at least one letter will be placed in the correct envelope?

Let $ A $ be the event "at least one letter is in the correct envelope".

- Let $ A_{i}, i=1, \ldots, n, $ be the event "letter $ i $ is placed in the correct envelope". We shall determine $ P\left(\bigcup_{i=1}^{n} A_{i}\right) $ 
- Use formula

$$
\begin{aligned}
P\left(\bigcup_{i=1}^{n} A_{i}\right)=&\sum_{i} P\left(A_{i}\right)-\sum_{i_{1}<i_{2}} P\left(A_{i_{1}} \cap A_{i_{2}}\right)\\
&+\sum_{i_{1}<i_{2}<i_{3}} P\left(A_{i_{1}} \cap A_{i_{2}} \cap A_{i_{3}}\right) \\
&-\cdots+(-1)^{n+1} P\left(A_{1} \cap A_{2} \cap \cdots \cap A_{n}\right)
\end{aligned}
$$

- For a given $ 1 \leqslant k \leqslant n, $ for any $ 1 \leqslant i_{1}<\cdots<i_{k} \leqslant n $

$$
P\left(A_{i_{1}} \cap \cdots \cap A_{i_{k}}\right)=(n-k) ! / n !
$$

- $ \sum_{i_{1}<\cdots<i_{k}} P\left(A_{i_{1}} \cap \cdots \cap A_{i_{k}}\right)=C_{n}^{k} \times(n-k) ! / n !=1 / k ! $. Thus,

$$
P\left(\bigcup_{i=1}^{n} A_{i}\right)=1-\frac{1}{2 !}+\frac{1}{3 !}-\cdots+(-1)^{n+1} \frac{1}{n !}=\sum_{k=1}^{n}(-1)^{k+1} \frac{1}{k !}
$$

**Example**: Three types of fruit: apple, banana, and orange. How many ways to load a pack of 5 pieces?

Combination with replacement Number of ways to select subsets of size $ k $ from a set of $ n $ distinguishable elements:

|                     | Ordered       | Unordered         |
| ------------------- | ------------- | ----------------- |
| Without replacement | $ P_{n}^{k} $ | $ C_{n}^{k} $     |
| With replacement    | $ n^{k} $     | $ C_{n+k-1}^{k} $ |

## 2.6 Conditional Probability

Different economic events are generally related to each other. Because of the connection, the occurrence of event $ B $ may
affect or contain the information about the probability that
event $ A $ will occur.

**Example (Financial Contagion)**: A large drop of the price in one market can cause a large drop of the price in another market, given the speculations and reactions of market participants.

When an event has occurred, some uncertainty is eliminated as new information arrives. We want to update probability calculations based on new information.

**Definition (Conditional Probability)**: Let $ A $ and $ B $ be two events in probability space $ (S, \mathcal{B}, P) . $ The conditional probability of event $ A $ given event $ B $, denoted as $ P(A \mid B), $ is defined as
$$
P(A \mid B)=\frac{P(A \cap B)}{P(B)}
$$

provided that $ P(B)>0 $.

Properties of conditional probability:

- $ P(A)=P(A \mid S) $
- $ P(A \mid B)=1-P\left(A^{c} \mid B\right) $
- Multiplication rule

$$
P(A \cap B)=P(A) P(B \mid A)=P(B) P(A \mid B)
$$

**Example**: Suppose two balls are randomly selected, without replacement, from a box containing $ a $ red balls and $ b $ blue balls. What is the probability that the first is red and the second is blue?

Let $ A=\{\text { the first is red }\} $ and $ B=\{\text { the second is blue }\} . $ Then
$$
P(A \cap B)=P(A) P(B \mid A)=\frac{a}{a+b} \times \frac{b}{a+b-1}
$$

**Theorem (Chain Rule of Joint Probability)**: For any events $ A_{1}, A_{2}, \ldots, A_{n} $ in $ \mathcal{B}, $ the joint probability of $ n $ events
$$
\begin{aligned} 
P\left(A_{1} \cap A_{2} \cap \cdots \cap A_{n}\right)=&P\left(A_{1}\right) \times P\left(A_{2} \mid A_{1}\right) \times P\left(A_{3} \mid A_{1} \cap A_{2}\right) \\ 
&\times \cdots \times P\left(A_{n} \mid A_{1} \cap \cdots \cap A_{n-1}\right) 
\end{aligned}
$$
is provided that $ P\left(A_{1} \cap \cdots \cap A_{n-1}\right)>0 $

**Remark:** $ P\left(A_{1} \cap \cdots \cap A_{n-1}\right)>0 $ implies $ P\left(A_{1}\right)>0, P\left(A_{1} \cap A_{2}\right)>0 $, $ \ldots, P\left(A_{1} \cap \cdots \cap A_{n-2}\right)>0 $

**Theorem (Rule of Total Probability)**: Let $ \left\{A_{i}, i=1,2, \ldots\right\} $ be a **partition** (i.e. mutually exclusive and collectively exhaustive) of sample space $ S, $ with $ P\left(A_{i}\right)>0 $ for all $ i \geqslant 1 . $ Then for any event $ B $ in $ \mathcal{B} $
$$
P(B)=\sum_{i=1}^{\infty} P\left(B \mid A_{i}\right) P\left(A_{i}\right)
$$

**Example**: Suppose events $ B_{1}, B_{2} $ and $ B_{3} $ are mutually exclusive. If $ P\left(B_{i}\right)=\frac{1}{3} $ and $ P\left(A \mid B_{i}\right)=\frac{i}{6} $ for $ i=1,2,3 . $ What is $ P(A) ? $ (Hint: $ B_{1}, B_{2} $ and $ B_{3} $ are also collectively exhaustive.)

## 2.7 Bayes' Formula

**Theorem (Bayes' Theorem)**: Suppose $ P(A)>0 $ and $ P(B)>0 . $ Then
$$
P(A \mid B)=\frac{P(B \mid A) P(A)}{P(B \mid A) P(A)+P\left(B \mid A^{c}\right) P\left(A^{c}\right)}
$$

**Remarks:**

- We consider $ P(A) $ as the prior probability about event $ A $.
- $ P(A \mid B) $ is the posterior probability given that $ B $ has occurred.
    ayes' Formula

**Theorem (Alternative Statement of Bayes' Theorem)**: Suppose $ A_{1}, \ldots, A_{n} $ are $ n $ mutually exclusive and collectively exhaustive events in the sample space $ S, $ and $ B $ is an event with $ P(B)>0 . $ Then the conditional probability of $ A_{i} $ given $ B $ is
$$
P\left(A_{i} \mid B\right)=\frac{P\left(B \mid A_{i}\right) P\left(A_{i}\right)}{P(B)}=\frac{P\left(B \mid A_{i}\right) P\left(A_{i}\right)}{\sum_{j=1}^{n} P\left(B \mid A_{j}\right) P\left(A_{j}\right)}
$$

**Example (Medical Test)**: Suppose there is a certain disease randomly found in 0.5% of the general population. Some blood test will yield a positive result in 99% of the case where the disease is present; but it also yields false-positive results in 5% of the case where a person is not infected. What is the probability that a person gets the disease if his test result is positive?

Let $ A $ and $ B $ denote the events "disease is present" and "positive
test result", respectively. Then

$$
\begin{aligned}
P(A \mid B) &=\frac{P(B \mid A) P(A)}{P(B \mid A) P(A)+P\left(B \mid A^{c}\right) P\left(A^{c}\right)} \\
&=\frac{0.99 \times 0.005}{0.99 \times 0.005+0.05 \times(1-0.005)} \approx 0.0905
\end{aligned}
$$

**Example (Monty Hall Problem)**: In a TV game there are three doors $ A, B, $ and $ C, $ of which two hide nothing while behind the third there is a prize. The prize is won if it is guessed correctly that which door hides it.

- You choose one of the door first, say A.
- Before door $ A $ is opened to reveal what is behind it, the game
    host open one of the other two doors, say $ \mathrm{B}, $ and shows that
    there is nothing behind it.
- He then offers you the option to change your decision (from door $ A $ to door $ C $ ).

Should you stick to your original choice or change to $ \mathrm{C} ? $

Let $ A, B, $ and $ C $ be the events "prize is behind door A" (respectively, B and C). Assume $ P(A)=P(B)=P(C)=\frac{1}{3} . $ Let $ B^{*} $ be the event "host shows that nothing is behind door $ \mathrm{B} $". Then,

$$
P\left(A \mid B^{*}\right)=\frac{P\left(B^{*} \mid A\right) P(A)}{P\left(B^{*} \mid A\right) P(A)+P\left(B^{*} \mid B\right) P(B)+P\left(B^{*} \mid C\right) P(C)}
$$

If the prize is behind door A, the hosts randomly open door B or door $ \mathrm{C}, $ so $ P\left(B^{*} \mid A\right)=\frac{1}{2} $
- If the prize is behind door B, then $ P\left(B^{*} \mid B\right)=0 $.
- If the prize is behind door $ C $, then $ P\left(B^{*} \mid C\right)=1 $

Therefore, $ P\left(A \mid B^{*}\right)=\frac{1}{3}, $ and similarly, $ P\left(C \mid B^{*}\right)=\frac{2}{3} $.

## 2.8 Independence

**Definition (Independence)**: Events $ A $ and $ B $ are said to be statistically independent if $ P(A \cap B)=P(A) P(B) $.

**Remarks:**

By definition,
$$
P(A \mid B)=\frac{P(A \cap B)}{P(B)}=\frac{P(A) P(B)}{P(B)}=P(A)
$$

Similarly, $ P(B \mid A)=P(B) $. Therefore, the occurrence of either one does not affect the probability of the occurrence of the other; that is, the knowledge of $ B $ does not help in predicting $ A $.

Mutual exclusiveness does not necessarily imply independence.

If $ P(A)=0, $ then any event $ B $ is independent of $ A $.

**Theorem**: Let $ A $ and $ B $ be two independent events. Then (1) $ A $ and $ B^{c},(2) $ $ A^{c} $ and $ B, $ (3) $ A^{c} $ and $ B^{c} $ are all independent.

**Definition (Independence among Several Events)**: Events $ A_{1}, A_{2}, \ldots, A_{n} $ are (jointly) independent if, for every possible subset $ \left\{A_{i_{1}}, \ldots, A_{i_{k}}\right\} $ where $ i_k=2, \ldots, n $
$$
P\left(A_{i_{1}} \cap \cdots \cap A_{i_{k}}\right)=P\left(A_{i_{1}}\right) \times \cdots \times P\left(A_{i_{k}}\right)
$$

**Remark:**

There are $ \left(2^{n}-n-1\right) $ conditions to be verified. For example, three events $ A, B $ and $ C $ are independent if
$$
\begin{aligned}
P(A \cap B) &=P(A) P(B) \\
P(A \cap C) &=P(A) P(C) \\
P(B \cap C) &=P(B) P(C) \\
P(A \cap B \cap C) &=P(A) P(B) P(C)
\end{aligned}
$$

It is possible to find that three events are pairwise independent but not jointly independent. 

**Example**: Suppose $ S=\{a, b, c, d\} $ and each basic outcome is equally likely to occur. Let $ A_{1}=\{a, b\}, A_{2}=\{b, c\} $ and $ A_{3}=\{a, c\} . $ Then,
$$
\begin{array}{c}
P\left(A_{1}\right)=P\left(A_{2}\right)=P\left(A_{3}\right)=\frac{1}{2} \\
P\left(A_{1} \cap A_{2}\right)=P\left(A_{1} \cap A_{3}\right)=P\left(A_{2} \cap A_{3}\right)=\frac{1}{4}
\end{array}
$$

but

$$
P\left(A_{1} \cap A_{2} \cap A_{3}\right)=0 \neq \frac{1}{8}
$$

It is also possible to find three events $ A, B $, $ C $ that satisfy $ P(A \cap B \cap C)=P(A) P(B) P(C) $ but not independent.

**Example**: Suppose $ S=\{a, b, c, d, e, f, g, h\} $ and each basic outcome is equally likely to occur. Let $ A_{1}=A_{2}=\{a, b, c, d\} $ and $ A_{3}=\{a, e, f, g\} . $ Then,
$$
P\left(A_{1}\right)=P\left(A_{2}\right)=P\left(A_{3}\right)=\frac{1}{2}, \quad P\left(A_{1} \cap A_{2} \cap A_{3}\right)=\frac{1}{8}
$$

but

$$
P\left(A_{1} \cap A_{2}\right)=\frac{1}{2} \neq P\left(A_{1}\right) P\left(A_{2}\right)
$$

**Theorem**: If events $ A_{1}, A_{2}, \ldots, A_{n} $ are independent, then
$$
P\left(A_{1} \cup \cdots \cup A_{n}\right)=1-\left\{\left[1-P\left(A_{1}\right)\right] \times \cdots \times\left[1-P\left(A_{n}\right)\right]\right\}
$$