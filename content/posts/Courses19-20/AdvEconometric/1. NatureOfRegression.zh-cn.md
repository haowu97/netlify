---
title: "经典回归模型到底在干嘛？"
date: 2021-03-21T21:17:53+08:00
draft: false

description: "All models are wrong, but some are useful. – George Box"
upd: "All models are wrong, but some are useful. – George Box"

tags: ['笔记']
categories: ['计量经济学']
---

<!--more-->

**回归**(Regression)最早由高尔顿（1886）提出，他发现，子辈的平均身高是其父辈平均身高以及他们所处族群平均身高的加权平均和，即身高具有均值回归的倾向。这个均值本质上是条件均值(给定父辈和种群平均身高，子代身高的均值)，**经典回归模型事实上就是在估计条件均值**。

在经典回归模型中，我们希望用解释变量(regressand)$X$的函数$g(X)$来预测被解释变量(regressor)$Y$。此时需要一个标准来测度$g(X)$与$Y$的接近程度，**均方误(mean squared error, MSE)准则**最常被使用，MSE是预测误差（预测值$g(X)$与目标$Y$之差）的平方的期望，表达式如下

$$
\operatorname{MSE}(g)=E[Y-g(X)]^{2} = \int\int[y-g(x)]^2f_{XY}(x,y)\mathrm{d} x\mathrm{d} y
$$

其中，$f_{XY}(x,y)$是变量$X$和$Y$的联合概率分布。

显然，MSE越小，$g(X)$对$Y$的预测能力越强。因此现在的问题转换为，**求解使MSE最小的函数$g(·)$**，注意到MSE是函数$g(·)$的函数。

事实上，**条件均值$E(Y|X)$就是使MSE最小的函数$g_0(X)$**，可以用求微分和方差分解两种方法证明（证明见文末附录）。

需要注意的是，条件均值$E(Y|X)$是$X$而非$Y$的函数，例如在高尔顿的例子中，子代身高的条件均值，取决于父辈和种群的平均身高，也即父辈和种群的平均身高的函数。

MSE是衡量$g(X)$对$Y$的预测能力的准则之一，但非唯一准则。例如，平均绝对误差(mean absolute error, MAE)，

$$
\operatorname{MAE}(g)=E|Y-g(X)|
$$

此时，**使MAE最小的函数$g(X)$是条件中位数**，分位数回归采用的正是该准则。

相比MAE，MSE具有连续可导的优良性质。

此外，令$Y=E(Y | X)+\varepsilon$，其中$\varepsilon$被称为回归扰动项，则有

$$
\begin{aligned} E(\varepsilon | X) &=E\{[Y-E(Y | X)] | X\} 
\\ &=E(Y | X)-E\left[g_{o}(X) | X\right] 
\\ &=E(Y | X)-g_{o}(X) 
\\ &=0 \end{aligned}
$$

**$E(\varepsilon|X) = 0$意味着$\varepsilon$不包含可用于预测$Y$的期望值的任何有关$X$的信息。换句话说，可用于预测$Y$的所有$X$的信息被包含在$E(Y|X)$中**。

在很多经济问题中，一阶条件矩即条件均值也是关注的焦点。

基于以上诸多原因，回归等式被设定为$Y=E(Y | X)+\varepsilon$，经典回归模型就是在估计$E(Y|X)$。常用的建模方法就是将$E(Y|X)$设定为某种有已知的函数形式，但包含少数未知参数，然后估计未知参数即可。

例如，线性回归模型假定

$$
E(Y|X)=\beta_{0}+\sum_{j=1}^{k} \beta_{j} X_{j}, \beta_{j} \in \mathbb{R}
$$

又如，Logistic回归模型假定
$$
E(Y|X)=\frac{1}{1+\exp (-\beta_{0}-\sum_{j=1}^{k} \beta_{j} X_{j})}
$$

最终经典回归问题被转换为熟悉的参数估计。

### 参考文献

[1] Francis, Galton. *Regression Towards Mediocrity in Hereditary Stature*[J]. The Journal of the Anthropological Institute of Great Britain and Ireland, 1886.

[2] Hong Y. *Advanced Econometrics*, Higher Education Press, 2011:18-28.

### 附录

**引理：重复期望法则**(Law of Iterated Expectations, LIE)，对给定可测函数$G(X,Y)$，假设期望$E[G(X,Y)]$存在，则

$$
E[G(X, Y)]=E\{E[G(X, Y) | X]\}
$$

证明：仅考虑$\left(Y,X^{\prime}\right)^{\prime}$是连续随机向量的情形，有

$$
\begin{aligned} E[G(X, Y)] &=\iint_{-\infty}^{\infty} G(x, y) f_{X Y}(x, y) \mathrm{d} x \mathrm{d} y \\ &=\iint_{-\infty}^{\infty} G(x, y) f_{Y | X}(y | x) f_{X}(x) \mathrm{d} x \mathrm{d} y \\ &=\int\left[\int_{-\infty}^{\infty} G(x, y) f_{Y | X}(y | x) \mathrm{d} y\right] f_{X}(x) \mathrm{d} x \\ &=\int E[G(X, Y) | X=x] f_{X}(x) \mathrm{d} x \\ &=E\{E[G(X, Y) | X]\} \end{aligned}
$$

**定理**：条件均值$E(Y|X)$是下列问题的最优解

$$
\begin{aligned} E(Y | X) &=\arg \min _{g \in \mathbb{F}} M S E(g) \\ &=\arg \min _{g \in \mathbb{F}} E[Y-g(X)]^{2} \end{aligned}
$$

其中$\mathbb{F}$是所有可测和平方可积函数的集合，即

$$
\mathbb{F}=\left\{g: \mathbb{R}^{k+1} \rightarrow \mathbb{R} | \int g^{2}(x) f_{X}(x) \mathrm{d} x<\infty\right\}
$$

**法一**：方差分解

令$g_{0}(X) = E(Y | X)$，则

$$
\begin{aligned} \operatorname{MSE}(g) &=E\left[Y-g_{0}(X)+g_{0}(X)-g(X)\right]^{2} \\ &=E\left[Y-g_{0}(X)\right]^{2}+E\left[g_{0}(X)-g(X)\right]^{2}+2 E\left\{\left[Y-g_{0}(X)\right]\left[g_{0}(X)-g(X)\right]\right\}  \end{aligned}
$$

根据重复期望法则

$$
\begin{aligned} E\left\{\left[Y-g_{0}(X)\right]\left[g_{0}(X)-g(X)\right]\right\}
&=E\left\{E\left(\left[Y-g_{0}(X)\right]\left[g_{0}(X)-g(X)\right]|X\right)\right\}
\\ &=E\left\{\left[g_{0}(X)-g(X)\right]E\left(\left[Y-g_{0}(X)\right]|X\right)\right\}
\\ &=E\left\{\left[g_{0}(X)-g(X)\right][E(Y|X)-g_{0}(X)]\right\}
\\ &=E\left\{\left[g_{0}(X)-g(X)\right]·0\right\}
\\ &=0
 \end{aligned}
$$

$$
\implies MSE(g) =E\left[Y-g_{0}(X)\right]^{2}+E\left[g_{0}(X)-g(X)\right]^{2}
$$

$$
\implies \arg \min _{g \in \mathbb{F}} M S E(g) = g_0(X) = E(Y|X)
$$

**法二**：求微分法

$$
\operatorname{MSE}(g)=E[Y-g(X)]^{2} = \int\int[y-g(x)]^2f_{XY}(x,y)\mathrm{d} x\mathrm{d} y
$$

根据一阶条件，MSE对$g(X)$的导数为0
$$
\frac{\delta MSE(g)}{\delta g(x)}=-2\int[y-g(x)] f_{XY}(x,y) \mathrm{d} y=0
$$

$$
\implies \int g(x)f_{XY}(x,y) \mathrm{d}y = \int yf_{XY}(x,y) \mathrm{d}y
$$

$$
\implies g(x)\int f_{XY}(x,y) \mathrm{d}y = \int yf_{XY}(x,y) \mathrm{d}y
$$

$$
\implies g(x) f_X(x) = \int yf_{XY}(x,y) \mathrm{d}y
$$

$$
\implies g(x)  = \int y\frac{f_{XY}(x,y)}{f_X(x)} \mathrm{d}y
$$

$$
\implies g(x)  = \int yf_{Y|X}(y|x) \mathrm{d}y=E(Y|X)
$$

