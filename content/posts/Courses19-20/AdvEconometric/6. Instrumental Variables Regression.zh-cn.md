---
title: "Instrumental Variable Regression"
date: 2021-03-21T21:17:53+08:00
draft: false

description: "Endogeneity, Two-Stage Least Squares (2SLS) Estimation, Hausman's Test."
upd: "Endogeneity, Two-Stage Least Squares (2SLS) Estimation, Hausman's Test."

tags: ['笔记']
categories: ['计量经济学']
---

<!--more-->

## 6.1 Motivation

So far, we've discussed what happens to OLS without the normality assumption.

- Asymptotic Theory with IID.
- Asymptotic Theory with Ergodic Stationary MDS
- Asymptotic Theory with Ergodic Stationary non-MDS.

Next, we will discuss what happens to OLS if $ E\left(\varepsilon_{t} | X_{t}\right) \neq 0 $.

- When $ E\left(\varepsilon_{t} | X_{t}\right) \neq 0, $ OLS estimator is inconsistent!

There are many cases with $ E\left(\varepsilon_{t} | X_{t}\right) \neq 0 $

- Measurement Errors in variables.

- omitted Important Variables.

- Simultaneous Cases.

If $ E\left(\varepsilon_{t} | X_{t}\right) \neq 0, $ what can we do?

- Give up OLS, and use alternative estimator; **two-stage least squares (2SLS) estimator**.

2SLS requires an instrumental variable (IV) satisfying certain conditions.

- Instrument relevance: $ \operatorname{corr}\left(Z_{t}, X_{t}\right) \neq 0 $
- Instrument exogeneity: $ \operatorname{corr}\left(Z_{t}, \varepsilon_{t}\right)=0 $.

We will investigate its statistical properties and construct hypothesis test statistics.

- When $ \varepsilon_{t} $ is an MDS with conditional homoskedasticity
- When $ \varepsilon_{t} $ is an MDS with conditional heteroskedasticity,
- When $ \varepsilon_{t} $ is a non-MDS process.

Note that the $ t $-test and $ F $-test obtained from the second stage regression estimation cannot be used even for large samples.

**Questions:** When may the condition $ E\left(\varepsilon_{t} | X_{t}\right)=0 $ fail? When $ E\left(\varepsilon_{t} | X_{t}\right) \neq 0, X_{t} $ is called 'endogenous', or $ X_{t} $ has the endogeneity problem.

【Example 6.2】**Errors of Measurements in Dependent Variable**: Now we consider a data generating process (DGP) given by
$$
Y_{t}^{*}=\beta_{0}^{\circ}+\beta_{1}^{\circ} X_{t}+\varepsilon_{t}
$$
where $ X_{t} $ is the income, $ Y_{t}^{*} $ is the consumption, and $ \left\{\varepsilon_{t}\right\} $ is i.i.d. $ \left(0, \sigma_{u}^{2}\right) $ and is independent of $ \left\{X_{t}\right\} $.

We don't observe $ Y_{t}^{*}, $ but $ Y_{t}=Y_{t}^{*}+w_{t}, $ where $ \left\{w_{t}\right\} $ is i.i.d. $ \left(0, \sigma_{w}^{2}\right) $ measurement errors independent of $ \left\{X_{t}\right\} $ and $ \left\{Y_{t}^{*}\right\} . $ 

Assume that $ \left\{w_{t}\right\} $ and $ \left\{\varepsilon_{t}\right\} $ are mutually independent.

Because we only observe $ \left(X_{t}, Y_{t}\right), $ we are forced to estimate the following model
$$
Y_{t}=Y_{t}^{*}+w_{t}=\beta_{0}^{o}+\beta_{1}^{o} X_{t}+u_{t}
$$
where $ u_{t}=w_{t}+\varepsilon_{t} . $ since $ E\left(X_{t} u_{t}\right)=0, $ **OLS is consistent**!

The measurement error in $ Y_{t} $ can be regarded as part of the true regression disturbance.

It increases the asymptotic variance of $ \sqrt{n}\left(\hat{\beta}-\beta^{\circ}\right), $ so **renders the estimation of $ \beta^{\circ} $ less precise**.

【Example 6.3】**Errors in Expectations/ Regressors**: Consider a linear regression model
$$
Y_{t}=\beta_{0}+\beta_{1} X_{t}^{*}+\varepsilon_{t}
$$

where $ X_{t}^{*} $ is the economic agent's conditional expectation of $ X_{t} $ at time $ t-1, $ and $ \left\{\varepsilon_{t}\right\} $ is an $ I I D\left(0, \sigma^{2}\right) $ sequence with $ E\left(\varepsilon_{t} | X_{t}^{*}\right)=0 . $ The conditional expectation $ X_{t}^{*} $ is a latent variable. **One such example is the Phillips curve's based inflation rate model in macroeconomics**.

When the economic agent follows rational expectations, then $ X_{t}^{*}=E\left(X_{t} | I_{t-1}\right) $ and we have

$$
X_{t}=X_{t}^{*}+v_{t}
$$

where

$$
E\left(v_{t} | I_{t-1}\right)=0
$$

where $ I_{t-1} $ is the information available to the economic agent at time $ t-1 . $ Assume that two error series $ \left\{\varepsilon_{t}\right\} $ and $ \left\{v_{t}\right\} $ are independent of each other.

Because we only observe $ \left(X_{t}, Y_{t}\right), $ we are forced to estimate the following model
$$
Y_{t}=\beta_{0}^{o}+\beta_{1}^{o}\left(X_{t}-v_{t}\right)+\varepsilon_{t}=\beta_{0}^{o}+\beta_{1}^{o} X_{t}+u_{t}
$$
where $ u_{t}=\varepsilon_{t}-\beta_{1}^{\circ} v_{t} $
since
$$
E\left(X_{t} u_{t}\right)=E\left[\left(X_{t}^{*}+v_{t}\right)\left(\varepsilon_{t}-\beta_{1}^{o} v_{t}\right)\right]=-\beta_{1}^{o} \sigma_{v}^{2} \neq 0
$$
provided $ \beta_{1}^{\circ} \neq 0, $ the $ \mathrm{OLS} $ estimator is not consistent for $ \beta_{1}^{\circ} $.

It may be noted that forecast variables obtained from survey data have been used for $ X_{t}^{*} $ in empirical studies.

【Example 6.4】**Endogeneity due to Omitted variables**: Consider an earning data generating process
$$
Y_{t}=X_{t}^{\prime} \beta^{\circ}+\gamma A_{t}+u_{t}
$$

where $ Y_{t} $ is the earning, $ X_{t} $ is working experience and schooling, and $ A_{t} $ is ability which is unobservable. Suppose the disturbance $ u_{t} $ satisfies the condition that $ E\left(u_{t} | X_{t}, A_{t}\right)=0 $.

Because $ A_{t} $ is not observed, one is forced to consider
$$
Y_{t}=X_{t}^{\prime} \beta^{\circ}+\varepsilon_{t}
$$

and is interested in knowing $ \beta^{\circ} $.

However, we have $ E\left(X_{t} \varepsilon_{t}\right) \neq 0 $ because $ \varepsilon_{t}=\gamma A_{t}+u_{t} $ and $ A_{t} $ is usually correlated with $ X_{t} $.

【Example 6.7】**Simultaneous Equation Bias**: We consider the following simple model of **national income determination**:
$$
\begin{aligned}
C_{t} &=\beta_{0}^{o}+\beta_{1}^{o} I_{t}+\varepsilon_{t} \qquad &(6.1)\\
I_{t} &=C_{t}+D_{t} \qquad &(6.2)
\end{aligned}
$$

where $ I_{t} $ is the income, $ C_{t} $ is the consumption expenditure, and $ D_{t} $ is the non-consumption expenditure. These models are called simultaneous equations, as they affect each other.

The variables $ I_t $ and $ C_t $ are called **endogenous variables**, as they are determined by the two-equation model. The variable $ D_{t} $ is called an **exogenous variable**, because it is determined outside the model (or the system considered).

We assume that $ \left\{D_{t}\right\} $ and $ \left\{\varepsilon_{t}\right\} $ are mutually independent, and $ \left\{\varepsilon_{t}\right\} $ is i.i.d. $ \left(0, \sigma^{2}\right) $

Solving for above equation simultaneously, we can obtain the "reduced forms":
$$
\begin{aligned}
C_{t} &=\beta_{0}^{o}+\beta_{1}^{o}\left(C_{t}+D_{t}\right)+\varepsilon_{t} \\
 &=\frac{\beta_{0}^{\circ}}{1-\beta_{1}^{\circ}}+\frac{\beta_{1}^{\circ}}{1-\beta_{1}^{\circ}} D_{t}+\frac{1}{1-\beta_{1}^{\circ}} \varepsilon_{t} 
\end{aligned}
$$
and
$$
\begin{aligned}
I_{t} &=\beta_{0}^{\circ}+\beta_{1}^{\circ} I_{t}+\varepsilon_{t}+D_{t} \\
 &=\frac{\beta_{0}^{\circ}}{1-\beta_{1}^{\circ}}+\frac{1}{1-\beta_{1}^{\circ}} D_{t}+\frac{1}{1-\beta_{1}^{\circ}} \varepsilon_{t}
\end{aligned}
$$

If $ \varepsilon_{t} $ increases, then $ C_{t} $ and $ I_{t} $ both simultaneously increase!

Obviously, $ I_{t} $ is positively correlated with $ \varepsilon_{t}$ (i.e., $ E\left(I_{t} \varepsilon_{t}\right) \neq 0 $). Thus, the OLS estimator for the regression of $ C_t $ on $ I_t $ in Eq. (6.1) will not be consistent for $ \beta_{1}^{\circ} $, the parameter for marginal propensity to consume .Generally speaking, the OLS estimator for the reduced form is consistent for new parameters, which are functions of original parameters.

## 6.2 Framework and Assumptions

### 6.2.1 Assumptions

【Assumption 7.1】**Ergodic Stationarity**: $ \left\{Y_{t}, X_{t}^{\prime}, Z_{t}^{\prime}\right\}_{t=1}^{\prime n} $ is an ergodic stationary stochastic process, where $ X_{t} $ is a $ K \times 1 $ vector, $ Z_{t} $ is a $ l \times 1 $ vector, and $ l \geq K $.

Assumption 7.1 allows for IID observations and stationary time series observations.

【Assumption 7.2】**Linearity**:
$$
Y_{t}=X_{t}^{\prime} \beta^{\circ}+\varepsilon_{t}, \quad t=1, \ldots, n
$$
for some unknown parameter $ \beta^{\circ} $ and some unobservable disturbance $ \varepsilon_{t} $.

【Assumption 7.3】**Nonsingularity**: The $ K \times K $ matrix
$$
Q_{x x}=E\left(X_{t} X_{t}^{\prime}\right)
$$
is nonsingular and finite.

These three assumptions are similar to those in previous chapters.

【Assumption 7.4】**IV Conditions**:

1. $ E\left(\varepsilon_{t} | X_t\right) \neq 0 $
2. $ E\left(\varepsilon_{t} | Z_{t}\right)=0 $
3. The $ l \times l $ matrix

$$
Q_{z z}=E\left(Z_{t} Z_{t}^{\prime}\right)
$$
is finite and nonsingular, and the $ l \times K $ matrix
$$
Q_{z x}=E\left(Z_{t} X_{t}^{\prime}\right)
$$
is finite and of full rank (**with A. 7.1 rank=K**).

【Assumption 7.5】**CLT**: 
$$
 n^{-1 / 2} \sum_{t=1}^{n} Z_{t} \varepsilon_{t} \stackrel{d}{\rightarrow} N(0, V) 
$$
for some $ K \times K $ symmetric matrix $ V \equiv \operatorname{avar}\left(n^{-1 / 2} \sum_{t=1}^{n} Z_{t} \varepsilon_{t}\right) $ finite and nonsingular.

Assumption 7.5 directly assumes that the CLT holds. This is often called a "high level assumption." It covers three cases: IID, MDS and non-MDS for $ \left\{X_{t} \varepsilon_{t}\right\} $.

- For an IID or MDS sequence $ \left\{Z_{t} \varepsilon_{t}\right\}, $ we have $ V=\operatorname{var}\left(Z_{t} \varepsilon_{t}\right)=E\left(Z_{t} Z_{t}^{\prime} \varepsilon_{t}^{2}\right) $
- For a non-MDS process $ \left\{Z_{t} \varepsilon_{t}\right\} $, $ V=\Sigma_{j=-\infty}^{\infty} \operatorname{cov}\left(Z_{t} \varepsilon_{t} , Z_{t-j} \varepsilon_{t-j}\right) $ is a long-run variance-covariance matrix.

### 6.2.2 Instrumental Variable

The random vector $ Z_{t} $ that satisfies assumption 7.4 is called instruments. The condition that $ l \geq K $ in  assumption 7.1 implies that **the number of instruments $ Z_{t} $ is larger than or at least equal to the number of regressors $ X_{t} $.**

When $ E \left(\varepsilon_{t} | X_{t}\right) \neq 0, $ we usually (but not always) have $ E\left(X_{t} \varepsilon_{t}\right) \neq 0 . $ As a result, the OLS estimator is not consistent for $ \beta^{\circ} . $ Now suppose we have an instrument vector $ Z_{t} $ with $ E\left(\varepsilon_{t} | Z_{t}\right)= 0 $, which implies $ E\left(Z_{t} \varepsilon_{t}\right)=0 . $ Then we can first project $ X_{t} $ onto $ Z_{t} $ and then run a regression of $ Y_{t} $ on the projection. This will deliver consistent estimation of $ \beta^{\circ} $.

Intuitively, IV is used when $ X_{t} $ is correlated to the error term. A valid IV induces changes in $ X_{t}, $ but has no direct effect on $ Y_{t} $.

**Question**: How to choose instruments $ Z_{t} $ in practice?

**All exogenous variables in $ X_t $:** First of all, one should analyze which explanatory variables in $ X_t $ are endogenous or exogenous. If an explanatory variable is exogenous, then this variable should be included in $ Z_t $, the set of instruments. For example, the constant term should always be included, because a constant is uncorrelated with any random variables. If $ k_{0} $ of $ K $ regressors are endogenous, at least $ k_{0} $ additional instruments should be included in $ Z_{t} $.

Most importantly, **we should choose an IV which is closely related to $ X_{t} $.** As we will see below, the strength of the correlation between $ Z_{t} $ and $ X_{t} $ affects the magnitude of the asymptotic variance of the 2SLS estimator for $ \beta_{0}, $ although it does not affect the consistency provided the correlation between $ Z_t $ and $ X_t $ is a non-zero constant.

## 6.3 Two-Stage Least Squares (2SLS) Estimation

**Question**: Because $ E\left(\varepsilon_{t} | X_{t}\right) \neq 0, $ the OLS estimator $ \hat{\beta} $ is not consistent for $ \beta^{\circ} . $ How to obtain consistent estimators for $ \beta^{\circ} ? $

It should be pointed out that when $ E\left(\varepsilon_{t} | X_{t}\right) \neq 0, $ endogeneity arises due to various reasons including model misspecification. However, **it still makes sense to find out the expected marginal effect of explanatory variables $ X_{t} $ on the dependent variable $ Y_{t} $, even if the linear regression model is misspecified**. This requires consistent estimation of $ \beta^{o} $. See For example, although Example 7.4 suffers from an omitted variable problem, one may be still interested in knowing the expected marginal effect of education on income.

### 6.3.1 Stage 1

**Stage 1**: Regress $ X_{t} $ on $ Z_{t} $ via $ \mathrm{OLS} $ and save the predicted value $ \hat{X}_{t} $.

Here, the artificial linear regression model is
$$
X_{t}=\gamma^{\prime} Z_{t}+v_{t}, \quad t=1, \ldots, n
$$
where $ \gamma $ is a $ I \times K $ parameter matrix, and $ v_{t} $ is a $ K \times 1 $ regression error.

From the result in chapter 1, we have $ E\left(Z_{t} v_{t}\right)=0 $ if and only if
$$
\gamma=\left[E\left(Z_{t} Z_{t}^{\prime}\right)\right]^{-1} E\left(Z_{t} X_{t}^{\prime}\right)
$$
The OLS estimator for $ \gamma $ is $ \hat{\gamma}=\left(Z^{\prime} Z\right)^{-1} Z^{\prime} X, $ and the predicted value of $ X_{t} $ on $ Z_{t} $ is $ \hat{X}_{t}=\hat{\gamma}^{\prime} Z_{t} . $ In matrix form
$$
\mathbf{\hat{X}}=\mathbf{Z} \hat{\gamma}=\mathbf{Z}\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime} \mathbf{X} \equiv P_Z \mathbf{X}
$$

where $ P_Z $ is symmetric and idempotent.

### 6.3.2 Stage 2

**Stage 2**: Use the predicted value $ \hat{X}_{t} $ as regressors for $ Y_{t} . $ Regress $ Y_{t} $ on $ \hat{X}_{t} $ and the resulting OLS estimator is called the 2SLS estimator, denoted as $ \hat{\beta}_{2 s l s} $.

**Question:** Why use the fitted value $ \hat{X}_{t}=\hat{\gamma}^{\prime} Z_{t} $ as regressors?

Because $ E\left(Z_{t} \varepsilon_{t}\right)=0, $ the population projection $ \gamma^{\prime} Z_{t} $ is orthogonal to $ \varepsilon $.

In general, $ v_{t}=X_{t}-\gamma^{\prime} Z_{t}, $ which is orthogonal to $ Z_{t}, $ is correlated with $ \varepsilon_{t} $ The stage 1 decomposes $ X_{t} $ into two components: $ \gamma^{\prime} Z_{t} $ and $ v_{t}, $ where $ \gamma^{\prime} Z_{t} $ is orthogonal to $ \varepsilon_{t}, $ and $ v_{t} $ is correlated with $ \varepsilon_{t} $.

The regression model in the second stage can be written as
$$
Y_{t}=\hat{X}_{t}^{\prime} \beta^{\circ}+\tilde{\varepsilon}_{t}
$$
or in matrix form
$$
Y=\hat{X} \beta^{\circ}+\tilde{\varepsilon}
$$
Note that the disturbance $ \tilde{\varepsilon}_{t} $ is not $ \varepsilon_{t} $ because $ \hat{X}_{t} $ is not $ X_{t} $.

Using $ \hat{X}=Z \hat{\gamma}=Z\left(Z^{\prime} Z\right)^{-1} Z^{\prime} X, $ we can write the $ 2 S L S $ estimator as follows:
$$
\begin{aligned}
\hat{\beta}_{2 s l s} &=\left(\hat{\mathbf{X}}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} Y=\left[(\mathbf{Z} \hat{\gamma})^{\prime}(\mathbf{Z} \hat{\gamma})\right]^{-1}(\mathbf{Z} \hat{\gamma})^{\prime} Y \\
&=\left\{\left[\mathbf{Z}\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime} \mathbf{X}\right]^{\prime}\left[\mathbf{Z}\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime} \mathbf{X}\right]\right\}^{-1}\left[\mathbf{Z}\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime} \mathbf{X}\right]^{\prime} Y \\
&=\left[\mathbf{X}^{\prime} \mathbf{Z}\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime} \mathbf{Z}\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime} \mathbf{X}\right]^{-1} \mathbf{X}^{\prime} \mathbf{Z}\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime} Y \\
&=\left[\mathbf{X}^{\prime} \mathbf{Z}\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime} \mathbf{X}\right]^{-1} \mathbf{X}^{\prime} \mathbf{Z}\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime} Y \\
&=\left[\frac{\mathbf{X}^{\prime} \mathbf{Z}}{n}\left(\frac{\mathbf{Z}^{\prime} \mathbf{Z}}{n}\right)^{-1} \frac{\mathbf{Z}^{\prime} \mathbf{X}}{n}\right]^{-1} \frac{\mathbf{X}^{\prime} \mathbf{Z}}{n}\left(\frac{\mathbf{Z}^{\prime} \mathbf{Z}}{n}\right)^{-1} \frac{\mathbf{Z}^{\prime} Y}{n}
\end{aligned}
$$

or use **the simple form**

$$
\begin{aligned}
\hat{\beta}_{2sls}&=\left(\hat{X}^{\prime} \hat{X}\right)^{-1} \hat{X}^{\prime} Y \\
&=\left(X^{\prime} \mathbb{P}_{z} X\right)^{-1} X^{\prime} \mathbb{P}_{z} Y \\
&=\left[X^{\prime} Z\left(Z^{\prime} Z\right)^{-1} Z^{\prime} X \right]^{-1} X^{\prime} Z\left(Z^{\prime} Z\right)^{-1} Z^{\prime} Y
\end{aligned}
$$

Using the expression $ Y=\mathrm{X} \beta^{\circ}+\varepsilon $ from assumption 7.2 we have
$$
\begin{aligned}
\hat{\beta}_{2 s l s}-\beta^{\circ} &=\left[\frac{\mathbf{X}^{\prime} \mathbf{Z}}{n}\left(\frac{\mathbf{Z}^{\prime} \mathbf{Z}}{n}\right)^{-1} \frac{\mathbf{Z}^{\prime} \mathbf{X}}{n}\right]^{-1} \frac{\mathbf{X}^{\prime} \mathbf{Z}}{n}\left(\frac{\mathbf{Z}^{\prime} \mathbf{Z}}{n}\right)^{-1} \frac{\mathbf{Z}^{\prime} \varepsilon}{n} \\
&=\left[\begin{array}{c}
\hat{Q}_{x z} \hat{Q}_{z z}^{-1} \hat{Q}_{z x}
\end{array}\right]^{-1} \hat{Q}_{x z} \hat{Q}_{z z}^{-1} \frac{Z^{\prime} \varepsilon}{n}
\end{aligned}
$$
where
$$
\begin{aligned}
\hat{Q}_{z z} &=\frac{\mathbf{Z}^{\prime} \mathbf{Z}}{n}=n^{-1} \sum_{t=1}^{n} Z_{t} Z_{t}^{\prime} \\
\hat{Q}_{x z} &=\frac{\mathbf{X}^{\prime} \mathbf{Z}}{n}=n^{-1} \sum_{t=1}^{n} X_{t} Z_{t}^{\prime} \\
Q_{z x} &=\frac{\mathbf{Z}^{\prime} \mathbf{X}}{n}=n^{-1} \sum_{t=1}^{n} Z_{t} X_{t}^{\prime}=\hat{Q}_{x z}^{\prime}
\end{aligned}
$$
![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20200613145124.png)

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20200613151631.png)

## 6.4 Consistency of 2SLS

**Question**: What are the statistical properties of $ \hat{\beta}_{2 s l s} $?

【Theorem 7.1】**Consistency of 2SLS**: Under Assumptions 7.1-7.4,  as $ n \rightarrow \infty $
$$
\hat{\beta}_{2 s l s} \stackrel{P}{\rightarrow} \beta^{\circ}
$$

**Proof:**

By the WLLN for e.s.p (ergodic stationary process), we have
$$
\begin{array}{c}
\hat{Q}_{z z} \stackrel{p}{\rightarrow} Q_{z z}, \quad l \times l \\
\hat{Q}_{x z} \stackrel{p}{\rightarrow} Q_{x z}, \quad K \times l \\
\frac{Z^{\prime} \varepsilon}{n} \stackrel{p}{\rightarrow} E\left(Z_{t} \varepsilon_{t}\right)=0, \quad l \times 1
\end{array}
$$
Also, $ Q_{x z} Q_{z z}^{-1} Q_{z x} $ is a $ K \times K $ symmetric and nonsingular matrix because $ Q_{x z} $ is of full rank, $ Q_{z z} $ is nonsingular, and $ l \geq K $. It follows from continuity that
$$
\left[\hat{Q}_{x z} \hat{Q}_{z z}^{-1} \hat{Q}_{z x}\right]^{-1} \stackrel{p}{\rightarrow}\left[Q_{x z} Q_{z z}^{-1} Q_{z x}\right]^{-1}
$$
Consequently, we have
$$
\hat{\beta}_{2 s l s}-\beta^{\circ} \stackrel{P}{\rightarrow}\left[Q_{x z} Q_{z z}^{-1} Q_{z x}\right]^{-1} Q_{x z} Q_{z z}^{-1} \cdot 0=0
$$

## 6.5 Asymptotic Normality of 2SLS

### 6.5.1 Asymptotic distribution of 2SLS

We now derive the asymptotic distribution of $ \hat{\beta}_{2 \text {sls}} . $ Write
$$
\sqrt{n}\left(\hat{\beta}_{2 s l s}-\beta^{o}\right)=\left[\hat{Q}_{x z} \hat{Q}_{z z}^{-1} \hat{Q}_{z x}\right]^{-1} \hat{Q}_{x z} \hat{Q}_{z z}^{-1} \frac{z^{\prime} \varepsilon}{\sqrt{n}}=\hat{A} \cdot \frac{Z^{\prime} \varepsilon}{\sqrt{n}}
$$
where the $ K \times l $ matrix
$$
\hat{A}=\left[\hat{Q}_{x z} \hat{Q}_{z z}^{-1} \hat{Q}_{z x}\right]^{-1} \hat{Q}_{x z} \hat{Q}_{z z}^{-1}
$$
By the CLT assumption (A.7.5), we have
$$
\frac{Z^{\prime} \varepsilon}{\sqrt{n}}=n^{-\frac{1}{2}} \sum_{t=1}^{n} Z_{t} \varepsilon_{t} \stackrel{d}{\rightarrow} N(0, V)
$$
where $ V $ is a finite and nonsingular $ I \times I $ matrix.

Then by the Slutsky theorem, we have
$$
\qquad \sqrt{n}\left(\hat{\beta}_{2 s / s}-\beta^{o}\right) \stackrel{d}{\rightarrow}\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\right)^{-1} Q_{x z} Q_{z z}^{-1} \cdot N(0, V) \sim N\left(0, A V A^{\prime}\right) 
$$
where $ A=\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\right)^{-1} Q_{x z} Q_{z z}^{-1} $.

The asymptotic variance of $ \sqrt{n}\left(\hat{\beta}_{2 s l s}-\beta^{\circ}\right) $
$$
\begin{aligned}
& \operatorname{avar}\left(\sqrt{n} \hat{\beta}_{2 s l s}\right) \equiv \Omega=A V A^{\prime} \\
=&\left\{\left[Q_{x z} Q_{z z}^{-1} Q_{z x}\right]^{-1} Q_{x z} Q_{z z}^{-1}\right\} V\left\{\left[Q_{x z} Q_{z z}^{-1} Q_{z x}\right]^{-1} Q_{x z} Q_{z z}^{-1}\right\}^{\prime} \\
=&\left[Q_{x z} Q_{z z}^{-1} Q_{z x}\right]^{-1} Q_{x z} Q_{z z}^{-1} V Q_{z z}^{-1} Q_{z x}\left[Q_{x z} Q_{z z}^{-1} Q_{z x}\right]^{-1}
\end{aligned}
$$
【Theorem 7.2】**Asymptotic Normality of 2SLS**: Under Assumptions 7.1-7.5, as $ n \rightarrow \infty $,
$$
\qquad \sqrt{n}\left(\hat{\beta}_{2 s l s}-\beta^{\circ}\right) \stackrel{d}{\rightarrow} N(0, \Omega)
$$

The estimation of $ V $ depends on whether $ \left\{Z_{t} \varepsilon_{t}\right\} $ is an MDS. We first consider the case where $ \left\{Z_{t} \varepsilon_{t}\right\} $ is an MDS process. In this case, $ V=E\left(Z_{t} Z_{t}^{\prime} \varepsilon_{t}^{2}\right) $ and so we need not estimate the long-run variance-covariance matrix.

### 6.5.2 Form of Asymptotic Variance

Now we discuss the form of $ \Omega $.

Note that IID case is the same as stationary ergodic MDS case, here we only discuss the latter case

**Case 1**: $ \left\{Z_{t} \varepsilon_{t}\right\} $ is a Stationary Ergodic MDS

【Assumption 7.6】**MDS**:

1. $ \left\{Z_{t} \varepsilon_{t}\right\} $ is an MDS;

2. $ \operatorname{var}\left(Z_{t} \varepsilon_{t}\right)=E\left(Z_{t} Z_{t}^{\prime} \varepsilon_{t}^{2}\right) $ is finite and nonsingular.

【Corollary 7.3】Under Assumptions 7.1-7.4 and 7.6, we have as $ n \rightarrow \infty $
$$
\sqrt{n}\left(\hat{\beta}_{2 s l s}-\beta^{o}\right) \stackrel{d}{\rightarrow} N(0, \Omega)
$$
where $ \Omega $ is defined as above with $ V=E\left(Z_{t} Z_{t}^{\prime} \varepsilon_{t}^{2}\right) $

**Special Case**: Conditional Homoskedasticity

When $ \left\{Z_{t} \varepsilon_{t}\right\} $ is an MDS with conditional homoskedasticity, the asymptotic variance $ \Omega $ can be greatly simplified.

【Assumption 7.7】**Conditional Homoskedasticity**: $ E\left(\varepsilon_{t}^{2} | Z_{t}\right)=\sigma^{2} $.

Under this assumption, by the law of iterated expectations, we obtain
$$
V=E\left(Z_{t} Z_{t}^{\prime} \varepsilon_{t}^{2}\right)=E\left[Z_{t} Z_{t}^{\prime} E\left(\varepsilon_{t}^{2} | Z_{t}\right)\right]=\sigma^{2} E\left(Z_{t} Z_{t}^{\prime}\right)=\sigma^{2} Q_{z z}
$$
It follows that
$$
\begin{aligned}
\Omega &=\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\right)^{-1} Q_{x z} Q_{z z}^{-1} \sigma^{2} Q_{z z} Q_{z z}^{-1} Q_{z x}\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\right)^{-1} \\
&=\sigma^{2}\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\right)^{-1}
\end{aligned}
$$
【Corollary 7.4】**Asymptotic Normality of 2 SLS under MDS with Conditional Homoskedasticity**: Under Assumptions 7.1-7.4, 7.6 and 7.7, we have as $ n \rightarrow \infty $
$$
\sqrt{n}\left(\hat{\beta}_{2 s l s}-\beta^{o}\right) \stackrel{d}{\rightarrow} N(0, \Omega)
$$
where
$$
\Omega=\sigma^{2}\left[Q_{x z} Q_{z z}^{-1} Q_{z x}\right]^{-1}
$$

**Case 2**: $ \left\{Z_{t} \varepsilon_{t}\right\} $ is a Stationary Ergodic non-MDS

In this general case, we have
$$
V \equiv \operatorname{avar}\left(n^{-1 / 2} \sum_{t=1}^{n} Z_{t} \varepsilon_{t}\right)=\sum_{j=-
\infty}^{\infty} \Gamma(j)
$$
where $ \Gamma(j)=\operatorname{cov}\left(Z_{t} \varepsilon_{t}, Z_{t-j} \varepsilon_{t-j}\right) $.

We need to use a long-run variance-covariance matrix estimator for $ V $. When $ \{Z_t \varepsilon_t \}$ is not a MDS, there is no need (and in fact there is no way) to consider conditional homoskedasticity and conditional heteroskedasticity separately.

## 6.6 Interpretation and Estimation of Asymptotic Variance of 2SLS

Let us revisit the stage 2 regression model
$$
Y_{t}=\hat{X}_{t}^{\prime} \beta^{\circ}+\tilde{\varepsilon}_{t}
$$
where the regressor
$$
\hat{X}_{t}=\hat{\gamma}^{\prime} Z_{t}
$$
and the disturbance $ \tilde{\varepsilon}_{t}=Y_{t}-\hat{X}_{t}^{\prime} \beta^{\circ} $. Note that $ \tilde{\varepsilon}_{t} \neq \varepsilon_{t} $ because $ \hat{X}_{t} \neq X_{t} $.

From A. 7.2, we have
$$
\tilde{\varepsilon}_{t}=Y_{t}-\hat{X}_{t}^{\prime} \beta^{\circ}=\varepsilon_{t}+\left(X_{t}-\hat{X}_{t}\right)^{\prime} \beta^{\circ}=\varepsilon_{t}+\hat{v}_{t}^{\prime} \beta^{\circ}
$$
where $ \hat{v}_{t} \equiv X_{t}-\hat{X}_{t}=X_{t}-\hat{\gamma}^{\prime} Z_{t} $.

Since $ \hat{v}_{t} $ is the estimated residual from the stage 1 OLS regression $ \mathrm{X}=\mathrm{Z} \gamma+v $ we have the following FOC holds:
$$
\mathbf{Z}^{\prime}(\mathbf{X}-\hat{\mathbf{X}})=\mathbf{Z}^{\prime} \hat{v}=0
$$
It follows that the 2SLS estimator
$$
\begin{array}{l}
\hat{\beta}_{2 s l s}&=\left(\hat{\mathbf{X}}^{\prime} \hat{\mathbf{X}}\right)^{-1} \hat{\mathbf{X}}^{\prime} Y \\
&=\left(\hat{\mathbf{X}}^{\prime} \hat{\mathbf{X}}\right)^{-1} \hat{\mathbf{X}}^{\prime}\left(\hat{\mathbf{X}} \beta^{\circ}+\tilde{\varepsilon}\right) \\
&=\beta^{\circ}+\left(\hat{\mathbf{X}}^{\prime} \hat{\mathbf{X}}\right)^{-1} \hat{\mathbf{X}}^{\prime}\left[\varepsilon+\hat{v} \beta^{\circ}\right] \\
&=\beta^{\circ}+\left(\hat{\mathbf{X}}^{\prime} \hat{\mathbf{X}}\right)^{-1} \hat{\mathbf{X}}^{\prime} \varepsilon
\end{array}
$$
because $ \hat{\mathrm{X}}^{\prime} \hat{v}=0 $. Therefore, the asymptotic properties of $ \hat{\beta}_{2SLS} $ are determined by
$$
\hat{\beta}_{2 s l s}-\beta^{\circ}=\left(\hat{\mathbf{X}}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \varepsilon=\left(\frac{\hat{\mathbf{X}}^{\prime} \mathbf{X}}{n}\right)^{-1} \frac{\hat{\mathbf{X}}^{\prime} \varepsilon}{n}
$$
The estimated residual $ \hat{v}=\mathbf{X}-\hat{\mathbf{X}} $ from the first stage has no impact on the statistical properties of $ \hat{\beta}_{2 s l s} $. **We can proceed as if we were estimating $ Y=\hat{\mathbf{X}} \beta^{\circ}+\varepsilon $ by OLS.**

Note that we have by the WLLN
$$
\hat{\gamma}=\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime} \mathbf{X} \stackrel{P}{\rightarrow} Q_{z z}^{-1} Q_{z x}=\gamma
$$

We will consider the following artificial regression model
$$
Y_{t}=\tilde{X}_{t}^{\prime} \beta^{\circ}+\varepsilon_{t}
$$
where
$$
\tilde{\beta}=\left(\tilde{X}^{\prime} \tilde{X}\right)^{-1} \tilde{X}^{\prime} Y
$$
The asymptotic properties of $ \hat{\beta}_{2 s l s} $ are the same as those of the infeasible OLS estimator $ \tilde{\beta} $. This helps a lot in understanding the variance-covariance structure of $ \hat{\beta}_{2 s l s} $. It is just a convenient way to understand the nature of $ \hat{\beta}_{2 s l s} $.

**We now show that the asymptotic properties of $ \hat{\beta}_{2 s l s} $ are the same as the asymptotic properties of $ \tilde{\beta} $.** For the asymptotic normality, observe that
$$
\sqrt{n}\left(\tilde{\beta}-\beta^{o}\right)=\hat{Q}_{\tilde{x} \tilde{x}}^{-1} \frac{\tilde{X}^{\prime} \varepsilon}{\sqrt{n}} \stackrel{d}{\rightarrow} Q_{\tilde{x} \tilde{x}}^{-1} \cdot N(0, \tilde{V}) \sim N\left(0, Q_{\tilde{x} \tilde{x}}^{-1} \tilde{V} Q_{\tilde{x} \tilde{x}}^{-1}\right)
$$
where
$$
Q_{\tilde{x} \tilde{x}} \equiv E\left(\tilde{X}_{t} \tilde{X}_{t}^{\prime}\right), \text { and } \tilde{V} \equiv \operatorname{avar}\left(n^{-1 / 2} \sum_{t=1}^{n} \tilde{X}_{t} \varepsilon_{t}\right)
$$
**Case I**: MDS with Conditional Homoskedasticity

Suppose $ \left\{\tilde{X}_{t} \varepsilon_{t}\right\} $ is MDS, and $ E\left(\varepsilon_{t}^{2} | \tilde{X}_{t}\right)=\sigma^{2} $. Then we have
$$
\tilde{V}=E\left(\tilde{X}_{t} \tilde{X}_{t}^{\prime} \varepsilon_{t}^{2}\right)=\sigma^{2} Q_{\tilde{x}\tilde{x}}
$$
by the law of iterated expectations (LIE). It follows that

$$
\sqrt{n}\left(\tilde{\beta}-\beta^{o}\right) \stackrel{d}{\rightarrow} N\left(0, \sigma^{2} Q_{\tilde{X} \tilde{X}}^{-1}\right)
$$
Because $ \tilde{X}_{t}=\gamma^{\prime} Z_{t}, \gamma=Q_{z z}^{-1} Q_{z x}, $ we have
$$
\begin{aligned}
Q_{\tilde{x} \tilde{x}} &=E\left(\tilde{X}_{t} \tilde{X}_{t}^{\prime}\right)=\gamma^{\prime} E\left(Z_{t} Z_{t}^{\prime}\right) \gamma=\gamma^{\prime} Q_{z z} \gamma \\
&=Q_{x z} Q_{z z}^{-1} Q_{z z} Q_{z z}^{-1} Q_{z x}=Q_{x z} Q_{z z}^{-1} Q_{z x}
\end{aligned}
$$

Therefore,

$$
\sigma^{2} Q_{\tilde{x} \tilde{x}}^{-1}=\sigma^{2}\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\right)^{-1} =\Omega\equiv \operatorname{avar}\left(\sqrt{n} \hat{\beta}_{2 s l s}\right)
$$

This implies that the asymptotic distribution of $ \tilde{\beta} $  is indeed the same as the asymptotic distribution of $ \hat{\beta}_{2 s l s} $ under the MDS with conditional homoskedasticity.

The asymptotic variance formula

$$
\operatorname{avar}\left(\sqrt{n} \hat{\beta}_{2 S L S}\right)=\sigma^{2} Q_{\bar{X} \bar{X}}^{-1}=\sigma^{2}\left(\gamma^{\prime} Q_{Z Z} \gamma\right)^{-1}
$$

indicates that the asymptotic variance of $ \sqrt{n} \hat{\beta}_{2 S L S} $ will be large if the correlation between $ Z_{t} $ and $ X_{t}, $ as measured by $ \gamma, $ is weak. **Thus, more precise estimation of $ \beta^{o} $ will be obtained if one chooses the instrument vector $ Z_{t} $ such that $ Z_{t} $ is highly correlated with $ X_{t} $.**

**Question:** How to estimate $ \Omega $ under the MDS disturbances with conditional homoskedasticity?

Consider the asymptotic variance estimator
$$
\hat{\Omega}=\hat{s}^{2} \hat{Q}_{\hat{x} \hat{x}}^{-1}=\hat{s}^{2}\left(\hat{Q}_{x z} \hat{Q}_{z z}^{-1} \hat{Q}_{z x}\right)^{-1}
$$
where
$$
\hat{s}^{2}=\hat{e}^{\prime} \hat{e} /(n-K), \hat{e}=Y-X \hat{\beta}_{2 s l s}, \text { and } \hat{Q}_{\hat{x} \hat{x}}=n^{-1} \sum_{t=1}^{n} \hat{X}_{t} \hat{X}_{t}^{\prime}
$$
It should be emphasized that $ \hat{e} $ is not the residual from the second stage.

This implies that even under conditional homoskedasticity, the conventional $ t $ -statistic in the second stage does not converge to $ N(0,1) $ in distribution, and $ J \cdot \hat{F} $ does not converge to $ \chi_{J}^{2} $ where $ \hat{F} $ is the $ F $ -statistic.

To show $ \hat{\Omega} \stackrel{p}{\rightarrow} \Omega, $ we shall show: $ (\mathrm{a}) \hat{Q}_{\dot{X} \hat{X}}^{-1} \stackrel{p}{\rightarrow} Q_{\bar{X} \tilde{X}}^{-1}, $ and $ (\mathrm{b}) \hat{s}^{2} \stackrel{p}{\rightarrow} \sigma^{2} $ as $ n \rightarrow \infty $.

We first show (a). There are two methods for proving this.

**Method 1**: We shall show $ \hat{Q}_{\hat{X} \hat{X}}^{-1} \stackrel{p}{\rightarrow} Q_{\tilde{X} \tilde{X}}^{-1} $ as $ n \rightarrow \infty . $ Because $ \hat{X}_{t}=\hat{\gamma}^{\prime} Z_{t} $ and $ \hat{\gamma} \stackrel{p}{\rightarrow} \gamma, $ we have
$$
\begin{aligned}
\hat{Q}_{\hat{X} \hat{X}} &=n^{-1} \sum_{t=1}^{n} \hat{X}_{t} \hat{X}_{t}^{\prime} \\
&=\hat{\gamma}^{\prime}\left(n^{-1} \sum_{t=1}^{n} Z_{t} Z_{t}^{\prime}\right) \hat{\gamma} \\
&=\hat{\gamma}^{\prime} \hat{Q}_{Z Z} \hat{\gamma} \\
& \stackrel{p}{\rightarrow} \gamma^{\prime} Q_{Z Z} \gamma \\
&=E\left[\left(\gamma^{\prime} Z_{t}\right)\left(Z_{t}^{\prime} \gamma\right)\right] \\
&=E\left(\tilde{X}_{t} \tilde{X}_{t}^{\prime}\right) \\
&=Q_{\tilde{X} \tilde{X}}
\end{aligned}
$$
**Method 2**: We shall show $ \left(\hat{Q}_{X Z} \hat{Q}_{Z Z}^{-1} \hat{Q}_{Z X}\right)^{-1} \stackrel{p}{\rightarrow}\left(Q_{X Z} Q_{Z Z}^{-1} Q_{Z X}\right)^{-1} $ as $ n \rightarrow \infty, $ which follows immediately from $ \hat{Q}_{X Z} \stackrel{p}{\rightarrow} Q_{X Z} $ and $ \hat{Q}_{Z Z} \stackrel{p}{\rightarrow} Q_{Z Z} $ by the WLLN. This method is more straightforward but is less intuitive than the first method.

Next, we shall show (b) $ \hat{s}^{2} \stackrel{p}{\rightarrow} \sigma^{2} $ as $ n \rightarrow \infty . $ We decompose
$$
\begin{aligned}
\hat{s}^{2} &=\frac{\hat{e}^{\prime} \hat{e}}{n-K} \\
&=\frac{1}{n-K} \sum_{t=1}^{n}\left(Y_{t}-X_{t}^{\prime} \hat{\beta}_{2 S L S}\right)^{2} \\
&=\frac{1}{n-K} \sum_{t=1}^{n}\left[\varepsilon_{t}-X_{t}^{\prime}\left(\hat{\beta}_{2 S L S}-\beta^{o}\right)\right]^{2} \\
&=\frac{1}{n-K} \sum_{t=1}^{n} \varepsilon_{t}^{2} \\
&+\left(\hat{\beta}_{2 S L S}-\beta^{o}\right)^{\prime} \frac{1}{n-K} \sum_{t=1}^{n} X_{t} X_{t}^{\prime}\left(\hat{\beta}_{2 S L S}-\beta^{o}\right) \\
&-2\left(\hat{\beta}_{2 S L S}-\beta^{o}\right)^{\prime} \frac{1}{n-K} \sum_{t=1}^{n} X_{t} \varepsilon_{t} \\
& \stackrel{p}{\rightarrow} \sigma^{2}+0 \cdot Q_{x x} \cdot 0-2 \cdot 0 \cdot E\left(X_{t} \varepsilon_{t}\right) \\
&=\sigma^{2}
\end{aligned}
$$
Note that although $ E\left(X_{t} \varepsilon_{t}\right) \neq 0, $ the last term still vanishes to zero in probability, because $ \hat{\beta}_{2 S L S}-\beta^{o} \stackrel{p}{\rightarrow} 0 $

We have proved the following theorem.

【Theorem 7.5】**Consistency of $ \hat{\Omega} $ under MDS with Conditional Homoskedasticity**: Under Assumptions 7.1-7.4, 7.6  and 7.7, we have as $ n \rightarrow \infty $,
$$
\Omega=\hat{s}^{2} \hat{Q}_{\hat{x} \hat{x}}^{-1} \stackrel{p}{\rightarrow} \Omega=\sigma^{2} Q_{\tilde{x} \tilde{x}}^{-1}=\sigma^{2}\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\right)^{-1}
$$
**Case II**: $ \left\{Z_{t} \varepsilon_{t}\right\} $ is an MDS with Conditional Heteroskedasticity

The following asymptotic distribution holds, for $ \tilde{V}=E\left(\tilde{X}_{t} \tilde{X}_{t}^{\prime} \varepsilon_{t}^{2}\right) $
$$
\sqrt{n}\left(\tilde{\beta}-\beta^{o}\right) \stackrel{d}{\rightarrow} N\left(0, Q_{\tilde{x} \tilde{x}}^{-1} \tilde{V} Q_{\tilde{x} \tilde{x}}^{-1}\right)
$$
Given $ \tilde{X}_{t}=\gamma^{\prime} Z_{t}, \gamma=Q_{Z Z}^{-1} Q_{Z X}, Q_{\tilde{X} \bar{X}}=\gamma^{\prime} Q_{Z Z} \gamma, $ and $ \tilde{V}=\gamma^{\prime} E\left(Z_{t} Z_{t}^{\prime} \varepsilon_{t}^{2}\right) \gamma=\gamma^{\prime} V \gamma, $ where $ V=E\left(Z_{t} Z_{t}^{\prime} \varepsilon_{t}^{2}\right) $, under the MDS assumption with conditional heteroskedasticity, we have
$$
\begin{aligned}
\operatorname{avar}(\sqrt{n} \tilde{\beta}) &=Q_{\tilde{x} \tilde{x}}^{-1} \tilde{V} Q_{\tilde{x} \tilde{x}}^{-1}\\
&=\left[E\left(\tilde{X}_{t} \tilde{X}_{t}^{\prime}\right)\right]^{-1} E\left[\tilde{X}_{t} \tilde{X}_{t}^{\prime} \varepsilon_{t}^{2}\right]\left[E\left(\tilde{X}_{t} \tilde{X}_{t}^{\prime}\right)\right]^{-1} \\
&=\left[\gamma^{\prime} E\left(Z_{t} Z_{t}^{\prime}\right) \gamma\right]^{-1} \gamma^{\prime} E\left(Z_{t} Z_{t}^{\prime} \varepsilon_{t}^{2}\right) \gamma\left[\gamma^{\prime} E\left(Z_{t} Z_{t}^{\prime}\right) \gamma\right]^{-1} \\
&=\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\right)^{-1} Q_{x z} Q_{z z}^{-1} V Q_{z z}^{-1} Q_{z x}\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\right)^{-1} \\
&=\Omega \equiv \operatorname{avar}\left(\sqrt{n} \hat{\beta}_{2 s l s}\right)
\end{aligned}
$$

This implies that the asymptotic distribution of the infeasible OLS estimator $ \tilde{\beta} $  is the same as the asymptotic distribution of $ \hat{\beta}_{2sls} $ under MDS with conditional heteroskedasticity. Therefore, the estimator for $ \Omega $ is
$$
\hat{\Omega}=\hat{Q}_{\hat{x} \hat{x}}^{-1} \hat{V}_{\hat{x} \hat{x}} \hat{Q}_{\hat{x} \hat{x}}^{-1}
$$
where
$$
\hat{V}_{\hat{x} \hat{x}}=n^{-1} \sum_{t=1}^{n} \hat{X}_{t} \hat{X}_{t}^{\prime} \hat{e}_{t}^{2}=\hat{\gamma}^{\prime}\left(n^{-1} \sum_{t=1}^{n} Z_{t} Z_{t}^{\prime} \hat{e}_{t}^{2}\right) \hat{\gamma}
$$
where $ \hat{\gamma}=\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime} \mathbf{X}=\hat{Q}_{z z}^{-1} \hat{Q}_{z \times} $ and $ \hat{e}_{t}=Y_{t}-X_{t}^{\prime} \hat{\beta}_{2 s / s} $.This is a White's (1980) heteroskedasticity consistent variance-covariance matrix estimator for $ \hat{\beta}_{2SLS} $.

【Assumption 7.8】a) $E\left(Z_{j t}^{4}\right) \leq C $ for some constant $ C<\infty $ and for all $ 0 \leq j \leq l ; $ b) $ E\left(\varepsilon_{t}^{4}\right)<\infty $.

Again, there are two methods to show $ \hat{\Omega} \stackrel{p}{\rightarrow} \Omega $ here.

Given that
$$
\hat{\Omega}=\left(\hat{Q}_{x z} \hat{Q}_{z z}^{-1} \hat{Q}_{z x}\right)^{-1} \hat{Q}_{x z} \hat{Q}_{z z}^{-1} \hat{V} \hat{Q}_{z z}^{-1} \hat{Q}_{z x}\left(\hat{Q}_{x z} \hat{Q}_{z z}^{-1} \hat{Q}_{z x}\right)^{-1}
$$
it suffices to show $ \hat{Q}_{x z} \stackrel{p}{\longrightarrow} Q_{x z}, \hat{Q}_{z z} \stackrel{p}{\rightarrow} Q_{z z} $ and $ \hat{V} \stackrel{p}{\rightarrow} V . $ The first two results immediately follow by the WLLN. The last result follows by using a similar reasoning of the consistency proof for $ n^{-1} \sum_{t=1}^{n} X_{t} X_{t}^{\prime} e_{t}^{2} $ in Chapter 4 or 5.

【Theorem 7.6】**Consistency of $ \hat{\Omega} $ under MDS with Conditional Heteroskedasticity**: Under Assumptions 7.1-7.4, 7.6 and 7.8, we have as $ n \rightarrow \infty $,
$$
\begin{aligned}
\hat{\Omega} &=\hat{Q}_{\hat{\chi} \hat{x}}^{-1} \hat{V}_{\hat{x} \hat{x}} \hat{Q}_{\hat{x} \hat{x}}^{-1} \stackrel{p}{\rightarrow} \Omega=Q_{\tilde{x} \tilde{x}}^{-1} \tilde{V} Q_{\tilde{x} \tilde{x}}^{-1} \\
&=\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\right)^{-1} Q_{x z} Q_{z z}^{-1} V Q_{z z}^{-1} Q_{z x}\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\right)^{-1}
\end{aligned}
$$
where $ \tilde{V}= E\left(\tilde{X}_{t} \tilde{X}_{t}^{\prime} \varepsilon_{t}^{2}\right) \text { and } V=E\left(Z_{t} Z_{t}^{\prime} \varepsilon_{t}^{2}\right) $.

Case Ⅲ: $ \left\{Z_{t} \varepsilon_{t}\right\} $ is a Stationary Ergodic non-MDS In this case, we have $ \sqrt{n}\left(\hat{\beta}_{2 s l s}-\beta^{o}\right) \stackrel{d}{\rightarrow} N(0, \Omega) $ as $ n \rightarrow \infty, $ where
$$
\Omega=Q_{\tilde{x} \tilde{x}}^{-1} \tilde{V} Q_{\tilde{x} \tilde{x}}^{-1}=\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\right)^{-1} Q_{x z} Q_{z z}^{-1} V Q_{z z}^{-1} Q_{z x}\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\right)^{-1}
$$
with
$$
\begin{aligned}
\tilde{V} &=\sum_{j=-\infty}^{\infty} \tilde{\Gamma}(j), \quad \tilde{\Gamma}(j)=\operatorname{cov}\left(\tilde{X}_{t} \varepsilon_{t}, \tilde{X}_{t-j} \varepsilon_{t-j}\right) \\
V &=\sum_{j=-\infty}^{\infty} \Gamma(j), \quad \Gamma(j)=\operatorname{cov}\left(Z_{t} \varepsilon_{t}, Z_{t-j} \varepsilon_{t-j}\right)
\end{aligned}
$$

On the other hand, we have
$$
\begin{aligned}
\operatorname{avar}(\sqrt{n} \tilde{\beta}) &=Q_{\tilde{x} \tilde{x}}^{-1} V_{\tilde{x} \tilde{x}} Q_{\tilde{x} \tilde{x}}^{-1}=\left(\gamma^{\prime} Q_{x x} \gamma\right)^{-1} \gamma^{\prime} V \gamma\left(\gamma^{\prime} Q_{x x} \gamma\right)^{-1} \\
&=\Omega \equiv \operatorname{avar}\left(\sqrt{n} \hat{\beta}_{2 s l s}\right)
\end{aligned}
$$
Thus, the asymptotic variance of $ \sqrt{n} \hat{\beta}_{2 s l s} $ is the same as the asymptotic variance of $ \tilde{\beta} $ under this general case.

**Question:** How to estimate $ \Omega ? $

We directly assume that we have a consistent estimator $ \hat{V} $ for $ V $

【Assumption 7.9】$ \hat{V} \stackrel{P}{\rightarrow} V \equiv \Sigma_{j=-\infty}^{\infty} \Gamma(j) $, $ \text { where } \Gamma(j)=\operatorname{cov}\left(Z_{t} \varepsilon_{t}, Z_{t-j} \varepsilon_{t-j}\right) $.

【Theorem 7.7】**Consistency of $ \hat{\Omega} $ under Non-MDS**: Under Assumptions 7.1-7.4,  and 7.9, we have as $ n \rightarrow \infty $
$$
\begin{aligned}
\hat{\Omega} &=\hat{Q}_{\hat{x} \hat{x}}^{-1} \hat{V}_{\hat{x} \hat{x}} \hat{Q}_{\hat{x} \hat{x}}^{-1} \\
&=\left(\hat{Q}_{x z} \hat{Q}_{z z}^{-1} \hat{Q}_{z x}\right)^{-1} \hat{Q}_{x z} \hat{Q}_{z z}^{-1} \hat{V} \hat{Q}_{z z}^{-1} \hat{Q}_{z x}\left(\hat{Q}_{x z} \hat{Q}_{z z}^{-1} \hat{Q}_{z x}\right)^{-1} \\
\stackrel{P}{\rightarrow} \Omega &=Q_{\tilde{x} \tilde{x}}^{-1} \tilde{V} Q_{\tilde{x} \tilde{x}}^{-1}
\end{aligned}
$$
where $ \hat{V}_{\hat{x} \hat{x}}=\hat{\gamma} \hat{V} \hat{\gamma}^{\prime} $ and
$$
\Omega=\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\right)^{-1} Q_{x z} Q_{z z}^{-1} V Q_{z z}^{-1} Q_{z x}\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\right)^{-1}
$$

## 6.7 Hypothesis Testing

**Case I**: $ \left\{Z_{t} \varepsilon_{t}\right\} $ is an MDS with Conditional Homoskedasticity

We have
$$
\sqrt{n}\left(\hat{\beta}_{2 s l s}-\beta^{o}\right) \stackrel{d}{\rightarrow} N(0, \sigma^{2}\left[Q_{x z} Q_{z z}^{-1} Q_{z x}\right]^{-1})
$$
【Theorem 7.8】**Hypothesis Testing**: Put $ \hat{e} \equiv Y-\mathbf{X} \hat{\beta}_{2 s l s} . $ Then under Assumptions 7.1-7.4, 7.6 and 7.7, the Wald test statistic
$$
\hat{W}=\frac{n\left(R \hat{\beta}_{2 s l s}-r\right)^{\prime}\left[R\left(\hat{\mathbf{X}}^{\prime} \hat{\mathbf{X}}\right)^{-1} R^{\prime}\right]^{-1}\left(R \hat{\beta}_{2 s l s}-r\right)}{\hat{e}^{\prime} \hat{e} /(n-K)} \stackrel{d}{\rightarrow} \chi_{J}^{2}
$$
as $ n \rightarrow \infty, $ under $ \mathrm{H}_{0} $.

**Question**: Is $ \hat{W} / J $ the $ F $ -statistic from the second stage regression?

No, because $ \hat{e} $ is not the estimated residual from the second stage.

**Case II**: $ \left\{Z_{t} \varepsilon_{t}\right\} $ is a Stationary Ergodic MDS with Conditional Heteroskedasticity

【Theorem 7.9】**Hypothesis Testing**: Under Assumptions 7.1-7.4, 7.6 and 7.8, the Wald test statistic
$$
\hat{W} \equiv n\left(R \hat{\beta}_{2 s l s}-r\right)^{\prime}\left[R \hat{Q}_{\hat{x} \hat{x}}^{-1} \hat{V}_{\hat{x} \hat{x}} \hat{Q}_{\hat{x} \hat{x}}^{-1} R^{\prime}\right]^{-1}\left(R \hat{\beta}_{2 s l s}-r\right) \stackrel{d}{\rightarrow} \chi_{J}^{2}
$$
under $ \mathrm{H}_{0}, $ where $ \hat{V}_{\hat{x} \hat{x}}=n^{-1} \Sigma_{t=1}^{n} \hat{X}_{t} \hat{X}_{t}^{\prime} \hat{e}_{t}^{2} $ and $ \hat{e}_{t}=Y_{t}-X_{t}^{\prime} \hat{\beta}_{2 s l s} $

**Case III**: $ \left\{Z_{t} \varepsilon_{t}\right\} $ is a Stationary ergodic non-MDS

【Theorem 7.10】**Hypothesis Testing**: Under Assumptions 7.1-7.5 and 7.9, the Wald test statistic
$$
\hat{W}=n\left(R \hat{\beta}_{2 s l s}-r\right)^{\prime}\left[R \hat{Q}_{\hat{x} \hat{x}}^{-1} \hat{V}_{\hat{x} \hat{x}} \hat{Q}_{\hat{x} \hat{x}}^{-1} R^{\prime}\right]^{-1}\left(R \hat{\beta}_{2 s l s}-r\right) \stackrel{d}{\rightarrow} \chi_{J}^{2}
$$
under $ \mathrm{H}_{0}, $ where $ \hat{V}_{\hat{x} \hat{x}}=\hat{\gamma}^{\prime} \hat{V} \hat{\gamma}, \hat{\gamma}=\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime} \mathbf{X} $ and $ \hat{V} $ is a long-run variance-covariance estimator for $ V=\Sigma_{j=-\infty}^{\infty} \Gamma(j) $ with $ \Gamma(j)= \operatorname{cov}\left(Z_{t} \varepsilon_{t}, Z_{t-j} \varepsilon_{t-j}\right) $.

## 6.8 Hausman's Test

When there exists endogeneity so that $ E\left(X_{t} \varepsilon_{t}\right) \neq 0, $ the OLS estimator $ \hat{\beta} $ is inconsistent for $ \beta^{\circ} . $ Instead, $ \hat{\beta}_{2 s l s} $ should be used.

In practice, one is not sure whether there exists endogeneity.
The null hypothesis of interest is:
$$
\mathrm{H}_{0}: E\left(\varepsilon_{t} | X_{t}\right)=0
$$
If this null hypothesis is rejected, one has to use the 2SLS estimator $ \hat{\beta}_{2 s l s} $ provided that one can find $ Z_{t} $ that satisfies A . 7.4.

For simplicity, we impose the following conditions.

【Assumption 7.10】a) $ \left\{\left(X_{t}^{\prime}, Z_{t}^{\prime}\right)^{\prime} \varepsilon_{t}\right\} $ is an MDS process; and b) $ E\left(\varepsilon_{t}^{2} | X_{t}, Z_{t}\right)=\sigma^{2} $.

The basic idea of Hausman's test is under $ \mathrm{H}_{0}: E\left(\varepsilon_{t} | X_{t}\right)=0, $ both $ \hat{\beta}=\left(X^{\prime} X\right)^{-1} X^{\prime} Y $ and $ \hat{\beta}_{2 s l s} $ are consistent for $ \beta^{\circ} $.

- They converge to the same limit $ \beta^{\circ} $ but it can be shown that $ \hat{\beta} $ is an asymptotically efficient estimator while $ \hat{\beta}_{2 s l s} $ is not.

- Under the alternatives to $ H_{0}, \hat{\beta}_{2 s l s} $ remains to be consistent for $ \beta^{\circ} $ but $ \hat{\beta} $ is not.

![](https://cdn.jsdelivr.net/gh/Henrry-Wu/FigBed/Figs/20200614094709.png)

Hausman (1978) considers a test for $ H_{0} $ based on the difference between the two estimators:
$$
\hat{\beta}_{2 s l s}-\hat{\beta}
$$
which converges to zero under $ H_{0} $ but generally to a nonzero constant under $ H_{1} $. 

To construct Hausman's (1978) test statistic, we need to derive the asymptotic distribution of $ \hat{\beta}_{2 s l s}-\hat{\beta} $. For this purpose, we first state a lemma.

【Lemma 7.11】Suppose $ \hat{A} \stackrel{p}{\rightarrow} A $ and $ \hat{B}=O_{P}(1) $. Then $ (\hat{A}-A) \hat{B} \stackrel{p}{\rightarrow} 0 $.

We first consider the OLS $ \hat{\beta} $. Note that
$$
\sqrt{n}\left(\hat{\beta}-\beta^{\circ}\right)=\hat{Q}_{x x}^{-1} n^{-1 / 2} \sum_{t=1}^{n} X_{t} \varepsilon_{t}
$$
where $ \hat{Q}_{x x}^{-1} \stackrel{P}{\rightarrow} Q_{x x}^{-1} $ and
$$
n^{-1 / 2} \sum_{t=1}^{n} X_{t} \varepsilon_{t} \stackrel{d}{\rightarrow} N\left(0, \sigma^{2} Q_{x x}\right)
$$
as $ n \rightarrow \infty $. It follows that $ n^{-1 / 2} \sum_{t=1}^{n} X_{t} \varepsilon_{t}=O_{P}(1) $. And by Lemma 7.11, we have
$$
\sqrt{n}\left(\hat{\beta}-\beta^{o}\right)=Q_{x x}^{-1} n^{-1 / 2} \sum_{t=1}^{n} X_{t} \varepsilon_{t}+o_{P}(1)
$$
Similarly, we can obtain
$$
\sqrt{n}\left(\hat{\beta}_{2 s l s}-\beta^{o}\right)=\hat{A} n^{-1 / 2} \sum_{t=1}^{n} Z_{t} \varepsilon_{t}=A n^{-1 / 2} \sum_{t=1}^{n} Z_{t} \varepsilon_{t}+o_{P}(1)
$$
where  
$$
 \hat{A}=\left(\hat{Q}_{x z} \hat{Q}_{z z}^{-1} \hat{Q}_{z x}\right)^{-1} \hat{Q}_{x z} \hat{Q}_{z z} \stackrel{p}{\rightarrow} A=\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\right)^{-1} Q_{x z} Q_{z z}^{-1} 
$$
and
$$
n^{-1 / 2} \sum_{t=1}^{n} Z_{t} \varepsilon_{t} \stackrel{d}{\rightarrow} N\left(0, \sigma^{2} Q_{z z}\right)
$$

It follows that
$$
\begin{aligned}
\sqrt{n}\left(\hat{\beta}_{2 s l s}-\hat{\beta}\right)=& n^{-1 / 2} \sum_{t=1}^{n}\left[\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\right)^{-1} Q_{x z} Q_{z z}^{-1} Z_{t}-Q_{x x}^{-1} X_{t}\right] \varepsilon_{t}+o_{P}(1) \\
& \stackrel{d}{\rightarrow} N\left(0, \sigma^{2}\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\right)^{-1}-\sigma^{2} Q_{x x}^{-1}\right)
\end{aligned}
$$
by the CLT for the stationary ergodic MDS process and Assumption 7.10 Therefore, under the null hypothesis $ \mathrm{H}_{0}, $ the quadratic form
$$
H=\frac{n\left(\hat{\beta}_{2 s l s}-\hat{\beta}\right)^{\prime}\left[\left(\hat{Q}_{x z} \hat{Q}_{z z}^{-1} \hat{Q}_{z x}\right)^{-1}-\hat{Q}_{x x}^{-1}\right]^{-1}\left(\hat{\beta}_{2 s l s}-\hat{\beta}\right)}{s^{2}} \stackrel{d}{\rightarrow} \chi_{k}^{2}
$$
as $ n \rightarrow \infty $ by the Slutsky theorem, where $ s^{2}=e^{\prime} e / n $ is the residual variance estimator based on the OLS residual $ e=Y-X \hat{\beta} $. This is called Hausman's test statistic.

【Theorem 7.12】**Hausman's Test for Endogeneity**: Suppose Assumptions 7.1-7.4, 7.10 and $ \mathrm{H}_{0} $ hold, and $ Q_{x x}-Q_{x z} Q_{z z}^{-1} Q_{z x} $ is strictly positive definite. Then as $ n \rightarrow \infty $
$$
H \stackrel{d}{\rightarrow} \chi_{K}^{2}
$$
We note that in the above Theorem,
$$
\begin{aligned}
\operatorname{avar}\left[\sqrt{n}\left(\hat{\beta}_{2 s l s}-\hat{\beta}\right)\right] &=\sigma^{2}\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\right)^{-1}-\sigma^{2} Q_{x x}^{-1} \\
&=\operatorname{avar}\left(\sqrt{n} \hat{\beta}_{2 s l s}\right)-\operatorname{avar}(\sqrt{n} \hat{\beta})
\end{aligned}
$$
This simple asymptotic variance-covariance structure is made possible under A. 7.10.

If the conditional heteroskedasticity exists, then we no longer have the above simple variance-covariance structure for $ \operatorname{avar}\left[\sqrt{n}\left(\hat{\beta}_{2 s l s}-\hat{\beta}\right)\right] $.

### More feasible way to do Hausman's Test

What if rank $ J<K ? $
To see modified test, consider
$$
\begin{aligned}
\hat{\beta}_{2 s l s} &=\left(\hat{\mathbf{X}}^{\prime} \mathbf{X}\right)^{-1} \hat{\mathbf{X}}^{\prime} Y=\left[(\mathbf{Z} \hat{\gamma})^{\prime}(\mathbf{Z} \hat{\gamma})\right]^{-1}(\mathbf{Z} \hat{\gamma})^{\prime} Y \\
&=\left\{\left[\mathbf{Z}\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime} \mathbf{X}\right]^{\prime}\left[\mathbf{Z}\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime} \mathbf{X}\right]\right\}^{-1}\left[\mathbf{Z}\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime} \mathbf{X}\right]^{\prime} Y \\
&=\left[\mathbf{X}^{\prime} \mathbf{Z}\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime} \mathbf{Z}\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime} \mathbf{X}\right]^{-1} \mathbf{X}^{\prime} \mathbf{Z}\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime} Y \\
&=\left[\mathbf{X}^{\prime} \mathbf{Z}\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime} \mathbf{X}\right]^{-1} \mathbf{X}^{\prime} \mathbf{Z}\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime} Y \\
&=\left[\mathbf{X}^{\prime} \mathbf{P}_{z} \mathbf{X}\right]^{-1} \mathbf{X}^{\prime} \mathbf{P}_{z} Y \quad \text { where } P_{z} \equiv \mathbf{Z}\left(\mathbf{Z}^{\prime} \mathbf{Z}\right)^{-1} \mathbf{Z}^{\prime}
\end{aligned}
$$
Then the Hausman's Test statistic becomes
$$
H=\frac{n\left(\hat{\beta}_{2 s l s}-\hat{\beta}\right)^{\prime}\left[\left(\mathbf{X}^{\prime} \mathbf{P}_{z} \mathbf{X}\right)^{-1}-\mathbf{X}^{\prime} \mathbf{X}\right]^{-1}\left(\hat{\beta}_{2 s l s}-\hat{\beta}\right)}{s^{2}} \stackrel{d}{\rightarrow} \chi_{K}^{2}
$$
Consider
$$
\begin{aligned}
\hat{\beta}_{2 s l s}-\beta^{o} &=\left(\mathbf{X}^{\prime} \mathbf{P}_{z} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{P}_{z} \mathbf{Y}-\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{Y} \\
&=\left(\mathbf{X}^{\prime} \mathbf{P}_{z} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{P}_{z} \mathbf{Y}-\left(\mathbf{X}^{\prime} \mathbf{P}_{z} \mathbf{X}\right)^{-1}\left(\mathbf{X}^{\prime} \mathbf{P}_{z} \mathbf{X}\right)\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{Y} \\
&=\left(\mathbf{X}^{\prime} \mathbf{P}_{z} \mathbf{X}\right)^{-1}\left\{\mathbf{X}^{\prime} \mathbf{P}_{\mathbf{z}}-\left(\mathbf{X}^{\prime} \mathbf{P}_{\mathbf{z}} \mathbf{X}\right)\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime}\right\} \mathbf{Y} \\
&=\left(\mathbf{X}^{\prime} \mathbf{P}_{z} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{P}_{\mathbf{z}}\left\{\mathbf{I}-\mathbf{X}\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime}\right\} \mathbf{Y} \\
&=\left(\mathbf{X}^{\prime} \mathbf{P}_{z} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{P}_{\mathbf{z}} \mathbf{M}_{X} \mathbf{Y}
\end{aligned}
$$
since the term $ \left(X^{\prime} P_{z} X\right)^{-1} $ never goes to 0 , we can just test if
$$
A \equiv \mathrm{X}^{\prime} \mathrm{P}_{\mathrm{z}} \mathrm{M}_{X} \mathrm{Y} \text { goes to } 0
$$
Let $ \mathbf{X}^{\prime} \equiv\left(X_{1}^{\prime}, W^{\prime}\right) $ and $ \mathbf{Z}^{\prime} \equiv\left(Z_{1}^{\prime}, W^{\prime}\right), $ where $ X_{1} $ are endogenous, $ Z_{1} $ are IV and $ W $ are exogenous. Then
$$
\mathbf{X}^{\prime} \mathbf{P}_{\mathbf{z}} \mathbf{M}_{X} \mathbf{Y}=\left[\begin{array}{ll}
\mathbf{X}_{1}^{\prime} \mathbf{P}_{\mathbf{z}} \mathbf{M}_{X} \mathbf{Y} & W^{\prime} \mathbf{P}_{\mathbf{z}} \mathbf{M}_{X} \mathbf{Y}
\end{array}\right]
$$
where $ W^{\prime} \mathrm{P}_{z} \mathrm{M}_{X} \mathrm{Y} $ goes to zero.

Thus, we need to check $ \mathrm{X}_{1}^{\prime} \mathrm{P}_{\mathrm{z}} \mathrm{M}_{X} \mathrm{Y}=0 . $ How?

Consider
$$
Y=X \beta+P_{z} X_{1} \delta+\varepsilon
$$
Apply OLS to this to have
$$
\hat{\delta}=\left(\mathbf{X}_{1}^{\prime} \mathbf{P}_{\mathbf{Z}} \mathbf{M}_{X} \mathbf{P}_{\mathbf{Z}} \mathbf{X}_{\mathbf{1}}\right)^{-1} \mathbf{X}_{1}^{\prime} \mathbf{P}_{\mathbf{z}} \mathbf{M}_{X} \mathbf{Y}
$$
Since $ \left(\mathrm{X}_{1}^{\prime} \mathrm{P}_{\mathrm{z}} \mathrm{M}_{X} \mathrm{P}_{\mathrm{z}} \mathrm{X}_{1}\right) $ does not go to zero, if $ \widehat{\delta} $ is significant, it implies
$$
\hat{\beta}_{2 s l s}-\beta^{o} \neq 0
$$
## 6.9 Examples

### 6.9.1 Hockey Tickets

Suppose that we are interested in the impact of a hockey teams regular season wins on ticket sales in millions. For a sample of 28 teams in 1995, we estimates the following linear model:
$$
Y_{i}=a+b X_{i}+u_{i}
$$
where $ Y_{i} $ is the log of ticket sales in millions, and $ X_{i} $ is the number of regular season wins in 1995

Q1) Do you think we can get an unbiased estimate of b?

omitted variable bias? or simultaneity? or measurement error in explanatory variables?

If there is a ignored variable that might be correlated with both wins and ticket sales (You could make a hypothesis and then discuss the motivation), then OLS is unbiased due to the omitted variable bias.

It's also possible that not only might wins impact ticket sales, but ticket sales could impact wins.

- For example, if the owner relies on ticket sales to finance the acquisition of star players, and the presence of star players contributes to wins, higher ticket sales can lead to increased wins. $ \Longrightarrow $ **Simultaneity**!

Q2) One might think naively whether one can construct a simple test for whether $ X $ and $ u $ are correlated by doing the following: using OLS estimates of $ a $ and $ b $, estimate the
error term:
$$
\widehat{u}_{i}=Y_{i}-\widehat{a}-\widehat{b} X_{i}
$$
and then run $ \widehat{u}_{i}=c+d X_{i}+v_{i} $ to see $ d=0 . $ Is this a good test procedure?

No! If $ X $ and $ u $ are correlated, then $ O L S $ is inconsistent and $ \widehat{u}_{i} $ is so for $ u $. And $ X \widehat{u}=0 $ no matter the assumption is actually true in reality.

### 6.9.2 Demand for Cigarettes

One might think that high tax on cigarette consumption would discourage smoking so that improves people's health condition (and save government health insurance spending).

To investigate this hypothesis, consider the following model to estimate the elasticity of demand for cigarettes using annual data for the 48 contiguous U.S. states for 1985 through 1995
$$
Q_{i}=\beta_{0}+\beta_{1} P_{i}+\epsilon_{i}
$$
where $ P_{i} $ is the price, $ Q_{i} $ is the quantity of cigarettes, and $ \epsilon_{i} $ is the error term.

One may concern **endogeneity due to simultaneity** that is, as $ Q_{i} $ is causing $ P_{i} $ in the model (1), $ P_{i} $ is driving $ Q_{i} $ due to price-quantity equilibrium.
$$
\begin{aligned}
P_{i} &=\gamma_{0}+\gamma_{1} Q_{i}+u_{i} \\
P_{i} &=\gamma_{0}+\gamma_{1}\left(\beta_{0}+\beta_{1} P_{i}+\epsilon_{i}\right)+u_{i} \\
&=\frac{\gamma_{0}+\gamma_{1} \beta_{0}}{1-\gamma_{1} \beta_{1}}+\frac{\gamma_{1} \epsilon_{i}}{1-\gamma_{1} \beta_{1}}+\frac{u_{i}}{1-\gamma_{1} \beta_{1}}
\end{aligned}
$$
Clearly $ P_ i $ must be correlated with $ \epsilon_{i} $ unless $ \gamma_{1}=0 $

We found one instrumental variable, Sales Tax; which is the portion of the tax on cigarettes arising from the general sales tax, measured in dollars per pack (in real dollars, deflated by the Consumer Price Index). We apply 2 SLS and obtain the following estimated equations
$$
\begin{array}{l}
\hat{P}_{i}&= &4.62+&0.031 \text { Sales Tax}_i \\
& &(0.03) &(0.005)\\
\hat{Q}_{i}&= &9.43- &1.14 \hat{P}_{i}\\
& &(1.26) &(0.37)\\
\end{array}
$$
Q1) (i) what are $ \widehat{P}_{i} $ and $ \widehat{Q}_{i} $ ?

(i) in the first stage of 2 SLS, we regression $ P_{i} $ on the sole instrument and obtain the prediction $ \widehat{P}_{i} $. Then, we use $ \hat{P}_{i} $ as the independent variable in the 2nd stage of 2SLS. We regress $ Q_{i} $ on $ \widehat{P}_{i} . \widehat{Q}_{i} $ is the prediction of $ Q_{i} $ from the stage 2 regression.

(ii) what is the IV estimate of b?

The IV estimate of bl is the coefficient from the 2nd stage of 2SLS, which implies $ \widehat{\beta}_{1}=1.14 $.

(iii) is $\text{Sales Tax}_{i} $ a valid instrument?

We need to **check instrument relevance and instrument exogeneity**. **Given the result of the 1st stage, we note that Sales Taxi is quite significant as its associated t-stat is more than $ 6 $.** This assures instrument relevance.

On the other hand, we cannot statistically test instrument exogeneity in this case.

For the sales tax to be exogenous, it must be uncorrelated with the error in the demand equation; that is, the sales tax must affect the demand for cigarettes only indirectly through the price.

**This seems plausible**: General sales tax rates vary from state to state, because different states choose different mixes of sales, income, property, and other taxes to finance public undertakings. Those choices about public finance are driven by political considerations, not by factors related to the demand for cigarettes.

Q2) Suppose that states levy special taxes that apply only to cigarettes and other tobacco products. These cigarette-specific taxes ($ \text { Cig } \text {Tax}_{i} $) constitute a possible second instrumental variable. Describe how the 2SLS process would change.

The 1st stage of 2SLS will change. We now run
$$
P_{i}=\gamma_{0}+\gamma_{1} \text {Sales } \operatorname{Tax}_{i}+\gamma_{2} \text { Cig} \operatorname{Tax}_{i}+u_{i}
$$
Then obtain the prediction $ \widehat{P}_{i} $ from the above equation and use it as the new independent variable in the 2nd stage of 2SLS. The rest are the same.

Q3) Using the instruments, we rerun the IV estimation and obtain the following results
$$
\widehat{Q}_{i}=\underset{(0.96)}{9.89}-\underset{(0.25)}{1.28 P_{i}}
$$
Comparing with the IV results using one IV, comment on the change of standard error of the estimated price elasticity.

The SE clearly gets smaller once an additional instrument is added. Because this estimate uses more information than the former equation. **Using two instruments explains more of the variation in cigarette prices, and this is reflected in smaller standard errors**. 

Q4) Some may argue that $ \text { Cig } \text {Tax}_{i} $; is not a valid instrument. What do you think? If $ \operatorname{Cig} $ Tax $ _{i} $ is indeed problematic, how would you solve this issue?

If tobacco farming and cigarette production are important industries in a state, then these industries could exert influence to keep cigarette-specific taxes low. This suggests that an omitted factor in cigarette demand - whether the state grows tobacco and produces cigarettes - could be correlated with cigarette-specific taxes.

One solution to this would be to include information on the size of the tobacco and cigarette industry in the state. In this case, the error term would exclude information about tobacco and cigarette industry, therefore, become uncorrelated with $ \text { Cig } \text {Tax}_{i} $.

Q5) Some may argue that one's income is an important factor that determines one's cigarettes demand. We follow this argument and modify model by incorporating Income as a control variable:
$$
Q_{i}=\beta_{0}+\beta_{1} P_{i}+\beta_{2} \text { Income}_{i}+\epsilon_{i}
$$
Discuss how would such alternation change the 2SLS process? Despite the validity of the instruments, we apply 2SLS with two instruments described above.

Step 1. Run
$$
P_{i}=\gamma_{0}+\gamma_{1} \text {Sales Tax}_{i}+\gamma_{2} \text { CigTax}_{i}+\gamma_{2} \text { Income}_{i}+u_{i}
$$
and obtain the prediction $ \widehat{P}_{i}, $ and use it as regressor in the next step.

Step 2 . We consider the following 2nd stage artificial regression:
$$
Q_{i}=\beta_{0}+\beta_{1} \widehat{P}_{i}+\beta_{2} \text { Income}_{i}+\epsilon_{i}
$$
We estimate the above coefficients using OLS. These estimates are consistent (converge to the their true values in probability).